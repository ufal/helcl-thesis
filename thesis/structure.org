*NON-AUTOREGRESSIVE NEURAL MACHINE TRANSLATION*
/Structure of the thesis/


* Introduction
  - generic text about NMT, motivation for non-autoregressive methods
  - what else?

* Neural Machine Translation
  - introductory chapter to NMT models, thorough, but not a survey
  - reader should learn how NMT models work, perhaps even be able to implement
    one, or at least train using a toolkit.
  - readers expected to know some algebra, some computer science concepts.
  - mention TOOLKITS, mention NEURAL MONKEY! :-) and Marian.

** Processing Text with Neural Networks
- section describes how text is loaded by the network
- also should mention post/preprocessing (segmentation, tokenization,
  truecasing, various tools, mention non-latin scripts)
- closer look on BPEs/sentencepiece

** Encoder-Decoder Framework (The Model)
- present formulas for the model in general

*** Recurrent Neural Networks
- present formulas + schemas of RNN models
- mention LSTMs, GRUs, and bidirectional RNNs.
- explain vanishing gradient problem

*** Attention Mechanism
- show formulas and schemas for attention in RNNs
- explain associative memory conceptualization from transformer

*** Deep Architectures
- describe deep RNNs, based loosely on our paper
- mention layer normalization + residual connections and explain the related
  problems

*** Transformer Model
- describe the architecture using formulas and schemas
- use widely-used schemas from the transformer paper, rather than trying to
  re-draw them.
- mention base vs big hyperparameters

** Training
- define the objective function
- describe concept of teacher forcing, mention exposure bias
- explain difference between training of transformer and RNNs, hint to
  autoregressivity
  - it's actually more similar than I thought. the RNNs also get all the
    inputs beforehand and the RNN can be evaluated in one go, but it still
    need to go left to right whereas transformer can be parallelized. This
    should be mentioned in the transformer model section rathrer than the
    training section.
- summarize training methodology, including:
  - variable initialization
  - batching, i.e. batch size, maxi batch, maxi batch sorting, length cropping,
    padding
  - optimizers, i.e. sgd vs adam, learning rate, other hyperparameters
  - validation and early stopping
  - data ? (i.e. shuffling, deduplication, *backtranslation, knowledge distillation*)
  - hardware, i.e. computing on GPUs, multi-GPU training, gradient accumulation
    via optimzier delay, synchronous vs asynchronous training

** Inference (Autoregressive Decoding)
- describe standard autoregressive decoding
- explain greedy and beam search, mention sampling, perhaps dig a little into
  search error vs modeling error

** Evaluation
- describe BLEU, mention other metrics exist
- say that human evaluation still main method, but expensive


* NAR NMT
  Main topic of the theis
  Introdcution contains the description of main concepts and problems:
  - neautoregresivita
  - length estimation
  - multimodality problem
  - teacher forcing
  - knowledge distillation

  - survey of methods for non-autoregressive NMT.
  - leave at the beginning, despite forward references

* NAT
   - efficiency of MT is important
   - local MT models have advantages
   - optimizations speed up the models
   - NAR models are an alternative
   - we analyze NAT models with the optimization techniques
   - structure of the chapter is as follows

** Related work (to be renamed)
   - the key concepts are:
   - Conditional independence
     - a defining feature of NAT models (+equations)
     - RNNs can't do that, transformer can
     - even transformer need to be changed
   - Target length estimation
     - eq 3.1 work with length implicitly using a special token
     - we can factorize 3.1 to get the target lenght estimation
     - ?
   - Multimodality problem
     - popisuje to gu
     - more words with high prob - more modes
     - vielen dank danke dank example
   - Fertility model
     - NAR transformer decoder does not have outputs on input
     - gu copies encoder words to decoder using a fertility model




* NAR NMT with CTC
  - based on the end to end paper
  - detailed version of that paper, includes description of the CTC algorithm

** Connectionist Temporal Classification
   - it's for training neural nets on sequences
   - it's good when there is no evident src/tgt alignment
   - it does not matter which architecture you use
   - models assume alignment is unknown
   - some states don't produce output -> multiple correct hypos
   - loss = sum over equivalent hypo losses
   - number of hypotheses grows exponentially with (max-length - src-length).
   - use dynamic programming to sum
   - description in figure
   - the value of the loss is the sum of all the paths, nodes store sum of
     prefixes

** Model Architecture
   - ctc can be applied on any arch -> use the one which everyone uses
   - NAT models need to know the length beforehand
   - set the length to k-times source
   - expand by state splitting (see figure)
   - we experiment with state splitting in two different places

** Baseline (preliminary) Experiments
   Section about the original CTC experiments plus their analysis. This is
   without knowledge distillation and on different datasets than the recent
   experiments. Also, Romanian (and Czech?) is included.

   - we translate en-de and en-ro in both directions
   - data
     - we use wmt data (EP, nc, cc for ende, EP and setimes for enro)
     - data is normalized, tokenized, truecased
   - experiments
     - we use neural monkey toolkit
     - we show the experimental settings in table ...
   -



* Experiments (to be renamed)... (possibly make three chapters according to the three papers)
  This chapter describes the experiments I am doing now.
  Fair comparison.

** Autoregressive Teacher Models
   This section is based on the EN-DE models from UEDIN submission.

** Student Models
   This section is about the student models distilled from the teacher models

   - we train models in five settings
