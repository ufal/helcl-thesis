*NON-AUTOREGRESSIVE NEURAL MACHINE TRANSLATION*
/Structure of the thesis/


* Introduction
  - generic text about NMT, motivation for non-autoregressive methods
  - what else?

* Neural Machine Translation
  - introductory chapter to NMT models, thorough, but not a survey
  - reader should learn how NMT models work, perhaps even be able to implement
    one, or at least train using a toolkit.
  - readers expected to know some algebra, some computer science concepts.
  - mention TOOLKITS, mention NEURAL MONKEY! :-) and Marian.

** Processing Text with Neural Networks
- section describes how text is loaded by the network
- also should mention post/preprocessing (segmentation, tokenization,
  truecasing, various tools, mention non-latin scripts)
- closer look on BPEs/sentencepiece

** Encoder-Decoder Framework (The Model)
- present formulas for the model in general

*** Recurrent Neural Networks
- present formulas + schemas of RNN models
- mention LSTMs, GRUs, and bidirectional RNNs.
- explain vanishing gradient problem

*** Attention Mechanism
- show formulas and schemas for attention in RNNs
- explain associative memory conceptualization from transformer

*** Deep Architectures
- describe deep RNNs, based loosely on our paper
- mention layer normalization + residual connections and explain the related
  problems

*** Transformer Model
- describe the architecture using formulas and schemas
- use widely-used schemas from the transformer paper, rather than trying to
  re-draw them.
- mention base vs big hyperparameters

** Training
- define the objective function
- describe concept of teacher forcing, mention exposure bias
- explain difference between training of transformer and RNNs, hint to
  autoregressivity
  - it's actually more similar than I thought. the RNNs also get all the
    inputs beforehand and the RNN can be evaluated in one go, but it still
    need to go left to right whereas transformer can be parallelized. This
    should be mentioned in the transformer model section rathrer than the
    training section.
- summarize training methodology, including:
  - variable initialization
  - batching, i.e. batch size, maxi batch, maxi batch sorting, length cropping,
    padding
  - optimizers, i.e. sgd vs adam, learning rate, other hyperparameters
  - validation and early stopping
  - data ? (i.e. shuffling, deduplication, *backtranslation, knowledge distillation*)
  - hardware, i.e. computing on GPUs, multi-GPU training, gradient accumulation
    via optimzier delay, synchronous vs asynchronous training

** Inference (Autoregressive Decoding)
- describe standard autoregressive decoding
- explain greedy and beam search, mention sampling, perhaps dig a little into
  search error vs modeling error

** Evaluation
- describe BLEU, mention other metrics exist
- say that human evaluation still main method, but expensive

* NAT
   - efficiency of MT is important in applications
   - local MT models have advantages
   - optimizations speed up the models
   - NAR models are an alternative
   - we analyze NAT models with the optimization techniques
   - structure of the chapter is as follows

** NAR models
   - describes the main concepts and problems
   - conditional independence
     - RNN-based models cannot be NAR easily
     - conditional independence means we cannot put the prefix on the input
     - the decoder cannot get the usual inputs (decoded prefix)
   - multimodality problem
     - a probability distribution can have multiple modes
     - well known example is danke shon vielen dank
   - length estimation
     - AR models use EOS symbol
     - length can be factorized out from the joint distribution
     - it is handy with multimodality problem and with creating inputs for
       decoder
   - NAT with fertility model (proposed by gu)
     - there are modifications
     - gu solves multimodality by latent fetility variables
     - during decoding this is approximated by argmax, average, or NPD
   - knowledge distillation
     - tackles multimodality problem
     - it is crucial for NAT models to work
   - iterative refinement
     - L latent variables
     - initial sequence is stretched or squished
     - trained by minimizing log-likelihood in every step
     - also, use DAE objective
     - train by mixing the two

** Alignment-based methods
   - AR use XENT loss, which is aligned one-to-one
   - alignment maps positions between target and output, a(i)=i is AR-XENT
   - we were the first to propose this, let's describe others
   - AXE
     - xent penalizes for misaligned words
     - axe is an objective function that takes this into account
     - alignments are monotonic and can skip
     - cmlm is base arch, upsample source length
     - we use ctc which is similar but sum
   - OaXE
     - axe can do only monotonic alignments
     - dynamic algorithm here cannot be used
   - Imputer
     - another alignment aproach
     - model architecture is typical for a NAT model
     - training takes into account partial outputs potentially decoded and
       minimizes lower bound

** Auxiliary objective methods
   - aux objectives are regularizations
   - here the methods use auxiliary objectives
   - NAT-REG
     - wang analyze and find two errors - repetitions and incomplete
       translations
     - for repetitions, they increase dissimilarity of neighboring states when
       outputs are different
     - for incompletes, they use reverse translation model
     - objectives in final nat reg model are mixed-in with XENT
   - Hint-NAT
     - KD is used quite often
     - propose two types of hints from the teacher model
   - Bag-of-ngrams Difference
     - address misalingment
     - based on l1 distance
   - Glancing training
     - by qian
     - first generate non-autoregressive output, then mask something and sample
       what to unmask
     - decoding as usual, length predict as cmlm
     - won WMT

** Semi-autoregressive and iterative methods
   - those before do it in one step
   - these are in between
   - many are based on ideas of refinement
   - CMLMs
     - recently, mlms have attracted research
     - lm estimate word prob given preceding words, mlms predict masked tokens
     - cmlms originate in pretrained XLMs but here are differences
     - target length estimation from an artificial token representation
     - decoding by iterative masking and predicting in a constant number of
       steps
     - generalized frameword do different masking strategies
   - SMART
     - follow-up to CMLM for exposure bias
     - recall exposure bias
     - to tackle, they propose to mix-in grond-truth tokens with predictions
   - Disco
     - kasai adds few additions
   - JM-nat
     - guo mask also encoder source words
     - mask also ngrams instead of tokens in decoder, battles repetition
   - Semi-autoregressive
     - attempt to best of both worlds
     - the less AR the worse
   - Blockwise parallel decoding
     - by stern, similar to semi-autoregressive
     - predict block, then verify
   - Insertion transformer
   - Levenshtein transformer
   - Latent Transformer
     - use discreet latent variables
     - how it works..

** Other methods

** Discussion


* NAR NMT with CTC
  - based on the end to end paper
  - detailed version of that paper, includes description of the CTC algorithm

** Connectionist Temporal Classification
   - it's for training neural nets on sequences
   - it's good when there is no evident src/tgt alignment
   - it does not matter which architecture you use
   - models assume alignment is unknown
   - some states don't produce output -> multiple correct hypos
   - loss = sum over equivalent hypo losses
   - number of hypotheses grows exponentially with (max-length - src-length).
   - use dynamic programming to sum
   - description in figure
   - the value of the loss is the sum of all the paths, nodes store sum of
     prefixes

** Model Architecture
   - ctc can be applied on any arch -> use the one which everyone uses
   - NAT models need to know the length beforehand
   - set the length to k-times source
   - expand by state splitting (see figure)
   - we experiment with state splitting in two different places

** Baseline (preliminary) Experiments
   Section about the original CTC experiments plus their analysis. This is
   without knowledge distillation and on different datasets than the recent
   experiments. Also, Romanian is included.

   - we translate en-de and en-ro in both directions
   - data
     - we use wmt data (EP, nc, cc for ende, EP and setimes for enro)
     - data is normalized, tokenized, truecased
   - experiments
     - we use neural monkey toolkit
     - we show the experimental settings in table ...
   -



* Experiments (to be renamed)... (possibly make three chapters according to the three papers)
  This chapter describes the experiments I am doing now.
  Fair comparison.

** Autoregressive Teacher Models
   This section is based on the EN-DE models from UEDIN submission.

   - we use the models from UEDIN submission as strong autoregressive baselines
   - data cleaning
     - dataset consists of clean and noisy parts, as well as monolingual with BT
     - we apply rule based filtering on all gathered data
     - we show data sizes in a table
     - we also clean the noisy data with dual cross-entropy filtering
     - we score the noisy data with models trained on clean data
   - backtranslation
     - we train for big models for backtranslation
     - translated news crawl is added to the training mix
     - we use tagged backtranslation
   - teacher training
     - train on shuffled concatenation
     - after convergence, finetuned on clean parallel data only
   - knowledge distillation
     - teachers created artificial targets

** Student Models
   This section is about the student models distilled from the teacher models

   - models implemented in Marian and warpctc
   - five experimental settings, large to tiny.

   - jaky detaily o trenovani muzu pridat?
     - urcite nejaky learning curves se hodí
     - popsat jak jsem to trenoval.

** Results
   v tyhle sekci budou vysledky.

   vysledkama se mysli:
   - bleu score na nějakým rozumným datasetu (wmt21) pro teachers a pro students.
   - bleu score na WMT14 (pro porovnani s predchozi kapitolou)

   - časy v různých setupech, plus porovnání.
   - s kým porovnávat?
     - předchozí kapitola
     - zjistit přesnej setting (myslim že to byla batch size 1 ale nejsem si jist)


   - výsledky časový v různých settinzích na tom milionu vět
     - CPU batch 1, 36 CPU batch
     - GPU batch 1, GPU batch

   - čas pro různý počty CPU jader a pro různý batch size
   - autoregresivní časy jakbysmet

   - BLEU, chrf a COMET na WMT21 na referencích A, C a D
