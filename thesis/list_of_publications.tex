\chapter*{List of Publications}

\phantom{\nobibliography*{references}}


% citace pocitany 9.11.2021

% -----------------------------------------------------------------------------
\noindent\bibentry{helcl-libovicky-2017-neural}
\begin{itemize}[noitemsep,topsep=0pt]

\item This paper introduces a software tool \emph{Neural Monkey} which was used
  for the experiments in Chapter \ref{chap:nar-nmt-ctc} of this thesis.

\item Citations (without self-citations): 54
  % 61 citations minus 7 self
\end{itemize}\vspace{.5\baselineskip}

% -----------------------------------------------------------------------------
\noindent\bibentry{libovicky-helcl-2017-attention}
\begin{itemize}[noitemsep,topsep=0pt]

\item In this paper we introduce techniques for combining multiple different
  inputs in sequence-to-sequence learning using RNNs.
\item Selected as an Outstanding Paper at ACL~2017.

\item Citations (without self-citations): 140
  % 144 minus 4 self-citations
\end{itemize}\vspace{.5\baselineskip}

% -----------------------------------------------------------------------------
\noindent\bibentry{miceli-barone-etal-2017-deep}
\begin{itemize}[noitemsep,topsep=0pt]

\item This paper explores deep architectures of RNN-based neural networks for
  MT.

\item Citations (without self-citations): 80
  % 80 minus 0 self-citations
\end{itemize}\vspace{.5\baselineskip}

% -----------------------------------------------------------------------------
\noindent\bibentry{helcl-etal-2018-neural}
\begin{itemize}[noitemsep,topsep=0pt]

\item A paper summarizing the Neural Monkey software at the beginning of 2018.

\item Citations (without self-citations): 7
  % 7 total 0 self
\end{itemize}\vspace{.5\baselineskip}

% -----------------------------------------------------------------------------
\noindent\bibentry{libovicky-etal-2018-input}
\begin{itemize}[noitemsep,topsep=0pt]

\item The paper introduces different strategies for input combination in
  sequence-to-sequence models with self-attentive encoder and decoder.

\item Citations (without self-citations): 43
  % 43 total 0 self
\end{itemize}\vspace{.5\baselineskip}

% -----------------------------------------------------------------------------
\noindent\bibentry{libovicky-helcl-2018-end}
\begin{itemize}[noitemsep,topsep=0pt]

\item This paper introduces the novel CTC-based approach for non-autoregressive
  neural machine translation, described in Chapter~\ref{chap:nar-nmt-ctc} and
  extended in Chapter~\ref{chap:experiments}.

\item Citations (without self-citations): 67
  % 67 total 0 self
\end{itemize}\vspace{.5\baselineskip}

% -----------------------------------------------------------------------------
\noindent\bibentry{chen-etal-2021-university}
\begin{itemize}[noitemsep,topsep=0pt]

\item This system description presents the University of Edinburgh's
  submissions to the WMT~21 News Translation Shared Task. The teacher models
  described in this paper were used in Chapter~\ref{chap:experiments}.

\item Citations (without self-citations): 0
  % 0 total 0 self (1 from findings)
\end{itemize}\vspace{.5\baselineskip}
% -----------------------------------------------------------------------------

\vfill

\noindent Only publications relevant to this thesis are included. The number of
citations was computed using Google Scholar. Total number of citations of
publications related to the topic of the thesis (without self-citations):
{\large 391} % (by the thesis submission on November 17, 2021).



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
