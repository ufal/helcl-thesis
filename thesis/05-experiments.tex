% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiments}
\label{chap:experiments}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this chapter, we present further experiments with \ac{nat} models trained
with \ac{ctc}. We focus on fair comparison with other non-autoregressive
approaches as well as state-of-the-art optimized autoregressive methods.

We performed experiments on English-German and English-Czech translation \JH{do
  the en-cs experiments!}. As stated in previous sections, knowledge
distillation \citep{kim-rush-2016-sequence} is a crucial element that brings
the performance of non-autoregressive methods closer to that of autoregressive
methods. We use strong autoregressive teachers as both our baseline models, and
as the source of the artificial target side data for training of the distilled
models.

% ------------------------------------------------------------------------------
\section{Autoregressive Teacher Models}
\label{sec:exp:autoregressive}
% ------------------------------------------------------------------------------

In this section, we describe the experimental settings for training the
autoregressive teacher models, which we used for knowledge distillation of data
for training our non-autoregressive student models.

\subsection{English -- German.}

\paperdisclaim{This section is based on ``The University of Edinburgh's
  English-German and English-Hausa Submissions to the WMT21 News Translation
  Task'', joint work with Pinzen Chen, Ulrich Germann, Laurie Burchell, Nikolay
  Bogoychev, Antonio Valerio Miceli Barone, Jonas Waldendorf, Alexandra Birch,
  and Kenneth Heafield, published at WMT 2021.}

\noindent
In our English-German experiments, we use autoregressive models from our
submission to the \ac{wmt} 2021 News Translation shared task \JH{add bib when
  ready}.

% ------------------------------------------------------------------------------
\paragraph{Data Cleaning.} For our English-German models, we prepare the
training dataset consisting of the following parts. First, we used clean
parallel data from the Europarl corpus \citep{koehn2005europarl}, the Tilde
MODEL -- RAPID corpus \citep{rozis-skadins-2017-tilde}, and News Commentary
corpus from OPUS \citep{tiedemann2012opus}. Next, we included sources of
crawled parallel data from the web, which are considered noisy. These include
Paracrawl \citep{espla-etal-2019-paracrawl}, Common
Crawl\footnote{\url{https://commoncrawl.org/}}, and WikiMatrix
\citep{schwenk2019wikimatrix}, and the Wikipedia Parallel Titles
Corpus\footnote{\url{https://linguatools.org/tools/corpora/wikipedia-parallel-titles-corpora/}}. Finally,
we used backtranslation \citep{sennrich-etal-2016-improving} of monolingual
data obtained from News Crawl. We trained our own models for generating the
backtranslations, trained on cleaned parallel data, as described below.

On the gathered data (both clean and noisy), we applied filtering techniques to
improve the overall quality of the parallel data. First, we applied a
deterministic rule-based filtering%
\footnote{\url{https://github.com/browsermt/students/blob/master/train-student/clean/clean-corpus.sh}}
and deduplication. We removed all sentence pairs containing non-printing
characters, empty sentences, and sentences longer than 120 words; we also
removed all sentence pairs with length ratio of less than 0.6 (0.4 for
Wikititles), sentences in which over 40\% of characters did not constitute
tokens, and sentences in which more than 50\% was non-alphabetic characters. We
ran language identification using fastText
\citep{joulin-etal-2017-bag,joulin2016fasttext} and removed all sentence pairs
classified as not English-German.

The data sizes before and after the rule-based filtering are shown in Table
\ref{tab:ende-data-sizes}. Note that the vast majority of the training corpus
consists of data from noisy sources.

\begin{table}
  \centering
  \begin{tabular}{lrrr}
    \toprule
    Data source & \multicolumn{2}{l}{Raw size} & \multicolumn{1}{l}{Size after cleaning} \\
    \midrule
    Europarl & 1.83 & \multirow{3}{*}{3.86} & \multirow{3}{*}{3.1} \\
    RAPID & 1.63 & &\\
    News Commentary & 0.40 & & \\
    \midrule
    Paracrawl & 82.64 &  \multirow{4}{*}{91.98} & \multirow{4}{*}{84.6} \\
    WikiMatrix & 5.47 & & \\
    Common Crawl & 2.40 & & \\
    Wikititles & 1.47 & & \\
    \midrule
    \multicolumn{1}{r}{total} & & 95.84 & 87.7 \\
    \bottomrule
  \end{tabular}

  \caption{The sizes of the individual raw corpora before and after rule-based
    cleaning, in millions of sentence pairs.}%
  \label{tab:ende-data-sizes}
\end{table}

To further clean the data, we applied dual cross-entropy filtering, as proposed
by \citet{junczys-dowmunt-2018-dual} and described in the data cleaning
paragraph in Section \ref{sec:training:methodology}.

We trained two Transformer base models for dual cross-entropy filtering using
the clean part of the data after the rule-based cleaning step.  \JH{Add more
  details on these models, perhaps include the hyperparameters somewhere in an
  appendix, plus add the dev scores}

We score the crawled part of the parallel data using the trained models, and we
sort the sentence pairs according to the score. To estimate how many of the
sentence pairs in the crawled we can consider clean, we train translation
models in both directions on different amounts of the scored data. We used
25\%, 50\%, 75\%, and 100\% of the crawled data, taking the highest-scoring
sentence pairs for training. Based on the score on the development data (the
\ac{wmt} 2019 test set, \citealp{barrault-etal-2019-findings}), we select the
75\% highest-scoring sentence pairs from the crawled data. \JH{rephrase this a
  bit + add details about the models} The develoment scores achieved by the
trained models are shown in Table \ref{tab:dual-cross-entropy-selection}.
% \JH{Add the data sizes after this cleaning step.}

\begin{table}
  \centering
  \begin{tabular}{lrrrr}
    \toprule
    Percentage & 25\% & 50\% & 75\% & 100\% \\
    No. of sentences & 21.9  & 43.9 & 66.5 & 87.7 \\
    \midrule
    En $\rightarrow$ De &  & 43.68 & 43.40 & 42.70 \\
    De $\rightarrow$ En & 41.47 & 41.64 & 42.15 & 42.02 \\
    \bottomrule
  \end{tabular}
  \caption{Development scores of the Transformer models trained on different
    amounts of scored crawled parallel data in both directions.}%
  \label{tab:dual-cross-entropy-selection}
\end{table}

% \JH{The following is not true / old:}
% We train our autoregressive models on a mix of the clean data and the
% backtranslations. We use tagged-backtranslation and we oversample the clean
% dataset to form roughly 25\% of the mix. We select the top 100 million sentence
% pairs from the backtranslated data according to Moore-Lewis language modeling
% score. \JH{tento odstavec rozvest aspon na stranku a kus}. We also try using
% the top 150 million. \JH{napsat jinak:} As the size of the clean data is 3.6
% million sentence pairs, we oversample the dataset by a factor of 10 for the 100M
% backtranslation version or 14 for the 150M backtranslation version.

% ------------------------------------------------------------------------------
\paragraph{Backtranslation.} We train four translation models on the filtered
parallel dataset for creating backtranslations
\citep{sennrich-etal-2016-improving}. We use identical Transformer big
hyper-parameter settings but we use different random seeds for parameter
initialization. We use the same hyperparameter settings as for the teacher
models. The parameter values are shown in Table \ref{tab:trafo-big-hparams}
% The Transformer big models have a 6-layer encoder and a 6-layer decoder, with
% 16 attention heads in the self-attention and cross-attention, with the model
% dimension of 1,024, and the feed-forward hidden layer dimension of 4,096. We
% use a shared SentencePiece vocabulary of 32,000 tokens.  \JH{Make a table out
% of this.}
We ensemble the four models to translate the monolingual data. \JH{Say more
  about decoding -- beam search, normalization, etc.} \JH{Somewhere we need to
  say that we are using Marian to train all the models in this chapter.}

As the source of the monolingual data, we use the News Crawl datasets from
years 2018, 2019, and 2020, as released by the \acs{wmt} organizers
\citep{bojar-etal-2018-findings,barrault-etal-2019-findings,
  barrault-etal-2020-findings}. In total, we gathered 91 million English
sentences for backtranslation into German and 146 million German sentences for
backtranslation in the other direction. Table \ref{tab:mono-data-sizes} shows
the sizes of the monolingual data before and after applying the rule-based
filtering described in the paragraphs above.

\begin{table}
  \centering
  \begin{tabular}{llrr}
    \toprule
    \multicolumn{2}{l}{Data source}  & Raw size  & Size after cleaning \\
    \midrule
    \multirow{3}{*}{English News Crawl} & 2018 & 18.11 & 17.90 \\
                                     & 2019 & 33.60 & 32.80 \\
                                     & 2020 & 41.43 & 40.33 \\
    \multicolumn{1}{r}{total} & & 93.14 & 91.03 \\
    \midrule
    \multirow{3}{*}{German News Crawl} & 2018 & 38.65 & 37.42  \\
                                     & 2019 & 57.62 & 56.33  \\
                                     & 2020 & 53.67 & 52.46 \\
    \multicolumn{1}{r}{total} & & 149.94 & 146.22 \\
    \bottomrule
  \end{tabular}

  \caption{The sizes of the monolingual data (in millions of sentences) used
    for the training, including rule-based data filtering}%
  \label{tab:mono-data-sizes}
\end{table}

We follow the approach of \citet{caswell-etal-2019-tagged} and we tag the
backtranslated sentences with a special token on a first position, as explained
in Section \ref{sec:training:methodology}.

% ------------------------------------------------------------------------------
\paragraph{Teacher Model Training.}  We train the teacher models on shuffled
concatenation of the authentic parallel and tagged backtranslated data. As with
the models for backtranslation, we train four models with different seeds for
random initialization in each direction. We show the hyper-parameter values in
Table \ref{tab:trafo-big-params}.

After the training on mixed parallel and backtranslated data converged, we
continued the training of the models on parallel data only.
\JH{Early stopping.}

\JH{Add a table with scores of the teacher models.}

\begin{table}
  \centering
  \begin{tabular}{llr}
    \toprule
    Parameter & Marian config variable & Value \\
    \midrule
    No. of encoder layers & \texttt{enc-depth} & 6 \\
    No. of decoder layers & \texttt{dec-depth} & 6 \\
    Model dimension &  \texttt{dim-emb} & 1,024 \\
    Attention heads & \texttt{transformer-heads} & 16 \\
%              & transformer-postprocess & dan \\
%              & transformer-postprocess-emb & d\\
    Vocabulary size & & 32,000 \\
    \midrule
    Optimizer method & \texttt{optimizer} & adam \\
    $\beta_1$ & \multirow{3}{*}{\texttt{optimizer-params}} & 0.9 \\
    $\beta_2$ & & 0.998 \\
    $\epsilon$ & & 10$^{-9}$ \\
    No. of batches per update & \texttt{optimizer-delay} & 2 \\
    Fit batch to available memory & \texttt{mini-batch-fit} & true\\
    \bottomrule
  \end{tabular}

  \caption{The hyper-parameters of the teacher models. The same values were
    used for training the models for backtranslation.}%
  \label{tab:trafo-big-hparams}
\end{table}

% ------------------------------------------------------------------------------
\paragraph{Knowledge Distillation.} The teacher models are used to create
artificial targets for the student models \citep{kim-rush-2016-sequence}. For
each translation direction, we translate the source side of the parallel data
and the authentic monolingual data (in the source language) using ensembled
teacher models. We do not create wholly synthetic datasets by
forward-translating backtranslated data.



% ------------------------------------------------------------------------------
\JH{Here should be a subsection about a different language pair. If it isn't,
  throw away the subsection heading above.}
% ------------------------------------------------------------------------------
\section{Student Models}
\label{sec:students}
% ------------------------------------------------------------------------------


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
