% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiments}
\label{chap:experiments}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this chapter, we describe the performed experiments. \JH{co dodat?}

\section{Autoregressive Baseline Models}
\label{sec:exp:autoregressive}

\paragraph{English -- German.} For our English-to-German models, we take
dataset created and published by \citet{germann2020university}. The dataset
consists of three parts. First, there is clean parallel data from the Europarl
corpus \citep{koehn2005europarl}, the Tilde MODEL -- RAPID corpus
\citep{rozis-skadins-2017-tilde}, and News Commentary corpus from OPUS
\citep{tiedemann2012opus}. Second, the dataset contains crawled parallel data
from the Web, which are considered noisy. These sources include Paracrawl
\citep{espla-etal-2019-paracrawl}, Common
Crawl\footnote{\url{http://commoncrawl.org/}}, and WikiMatrix
\citep{schwenk2019wikimatrix}. Third, the dataset contains backtranslated
sentences of originally monolingual News Crawl datasets from years 2008 to
2017, in both translation directions. The details of the data sizes are given
in Table \ref{tab:ende-data-sizes}.

\begin{table}
  \centering
  \begin{tabular}{cc}
    \toprule
    haha & beta \\
    \midrule
    bravo & delta \\
    \bottomrule
  \end{tabular}

  \caption{The sizes of the individual data sources in the EN-DE training
    data.}%
  \label{tab:ende-data-sizes}
\end{table}

We trained the first models on the clean parallel data alone, for comparison
purposes and for the sake of completeness.


We train our autoregressive models on a mix of the clean data and the
backtranslations. We use tagged-backtranslation and we oversample the clean
dataset to form roughly 25\% of the mix. We select the top 100 million sentence
pairs from the backtranslated data according to Moore-Lewis language modeling
score. \JH{tento odstavec rozvest aspon na stranku a kus}. We also try using
the top 150 million. \JH{napsat jinak:} As the size of the clean data is 3.6
million sentence pairs, we oversample the dataset by a factor of 10 for the 100M
backtranslation version or 14 for the 150M backtranslation version.


%\input{end-end.tex}

%\input{improving.tex}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
