% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiments}%
\label{chap:experiments}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


As we discussed in Section \ref{sec:nat:discussion}, the flaws in the
evaluation methodology adopted by current research of \ac{nat} models make it
difficult to identify approaches suitable in production-ready conditions.  In
this chapter, we present further experiments with \ac{nat} models trained with
\ac{ctc}. We focus on a fair comparison with other non-autoregressive
approaches as well as state-of-the-art optimized autoregressive methods.  In
line with our previous experiments (see Chapter \ref{chap:nar-nmt-ctc}) and
also with the rest of the related work, we conduct experiments on
English-German translation.

As previously stated (see Section \ref{sec:nat:principles}), knowledge
distillation \citep{kim-rush-2016-sequence} is a crucial element that brings
the performance of \ac{nar} methods closer to that of \ac{ar} methods. We use
strong \ac{ar} teachers as both our baseline models and as the source of the
artificial target side data for training of the distilled models. We describe
our teacher models in detail in Section \ref{sec:exp:teachers}

In Section \ref{sec:exp:students}, we describe the training of the \ac{nar}
student models. We follow the approach described in Chapter
\ref{chap:nar-nmt-ctc}. Key changes to the \acs{ctc}-based models include
training the student models on distilled data and using a faster implementation
of the translation system.

In the analysis of the results presented in Section \ref{sec:exp:results}, we
try to connect the separate worlds of \ac{nat} research and the bids to improve
the efficiency of translation models. Our student models outperform all of the
\ac{nar} models in terms of \acs{bleu} on the standard \acs{wmt}~14 translation
benchmark. However, at the same time, the models score relatively poorly in
comparison to the current state-of-the-art \ac{ar} translation models, which
suggests that the gap in translation quality between \ac{ar} and \ac{nar}
models is still a problem, contrary to some of the claims in the literature
\citep{gu-kong-2021-fully,saharia-etal-2020-non}. We also present an extensive
analysis of the decoding speed under different conditions and conclude that
without future advancements in this field, optimized \ac{ar} models will
continue to achieve superior results over \ac{nar} models.


% ------------------------------------------------------------------------------
\section{Autoregressive Teacher Models}%
\label{sec:exp:teachers}
% ------------------------------------------------------------------------------

\paperdisclaim{This section is based on the paper \emph{``The University of
    Edinburgh's English-German and English-Hausa Submissions to the WMT21 News
    Translation Task''}, joint work with Pinzen Chen, Ulrich Germann, Laurie
  Burchell, Nikolay Bogoychev, Antonio Valerio Miceli Barone, Jonas Waldendorf,
  Alexandra Birch, and Kenneth Heafield, published at WMT 2021.}

\noindent
In this section, we describe the experimental settings for training the \ac{ar}
teacher models, which we use for knowledge distillation to prepare training
data for our \ac{nar} student models.

In our English-German experiments, we use autoregressive models from our
submission to the \ac{wmt} 2021 News Translation Shared Task
\citep{chen-etal-2021-university}. We use these models both as strong \ac{ar}
baselines and as teacher models for generating distilled data for the \ac{nar}
student models, described in Section \ref{sec:exp:students}. The models were
trained on cleaned parallel data augmented with backtranslated monolingual
data, using Marian, a C++ toolkit for training and decoding from \ac{nmt}
models \citep{junczys-dowmunt-etal-2018-marian}.

% ------------------------------------------------------------------------------
\paragraph{Data Cleaning.} We prepare the training dataset consisting of the
following parts: First, we use clean parallel data from the Europarl corpus
\citep{koehn-2005-europarl}, the Tilde MODEL -- RAPID corpus
\citep{rozis-skadins-2017-tilde}, and the News Commentary corpus from OPUS
\citep{tiedemann-2012-parallel}. Next, we include sources of crawled parallel
data from the web, which are considered noisy. These include Paracrawl
\citep{espla-etal-2019-paracrawl}, Common
Crawl\footnote{\url{https://commoncrawl.org/}}, WikiMatrix
\citep{schwenk2019wikimatrix}, and the Wikipedia Parallel Titles
Corpus\footnote{\url{https://linguatools.org/tools/corpora/wikipedia-parallel-titles-corpora/}}. Finally,
we use backtranslation \citep{sennrich-etal-2016-improving} of monolingual data
obtained from News Crawl.  We train our own models to generate the
backtranslations on cleaned parallel data, as described below.

We perform the following filtering techniques on the gathered data (both clean
and noisy) to improve the overall quality of the parallel data.

We start with deterministic rule-based filtering%
\footnote{\url{https://github.com/browsermt/students/blob/master/train-student/clean/}}
and deduplication. We remove all sentence pairs containing non-printing
characters, empty sentences, and sentences longer than 120 words; we also
remove all sentence pairs with length ratio of less than 0.6 (0.4 for
Wikititles), sentences in which over 40\% of characters do not constitute
tokens, and sentences in which more than 50\% were non-alphabetic characters. We
run language identification using fastText
\citep{joulin-etal-2017-bag,joulin2016fasttext} and remove all sentence pairs
classified as not English-German.

The data sizes before and after the rule-based filtering are shown in Table
\ref{tab:ende-data-sizes}. Note that the vast majority of the training corpus
consists of data from noisy sources.

\begin{table}
  \centering
  \begin{tabular}{lrrr}
    \toprule
    Data source & \multicolumn{2}{c}{Raw size} & \mc{Size after cleaning} \\
    \midrule
    Europarl & 1.83 & \multirow{3}{*}{3.86} & \multirow{3}{*}{3.1} \\
    RAPID & 1.63 & &\\
    News Commentary & 0.40 & & \\
    \midrule
    Paracrawl & 82.64 &  \multirow{4}{*}{91.98} & \multirow{4}{*}{84.6} \\
    WikiMatrix & 5.47 & & \\
    Common Crawl & 2.40 & & \\
    Wikititles & 1.47 & & \\
    \midrule
    \multicolumn{1}{r}{total} & & 95.84 & 87.7 \\
    \bottomrule
  \end{tabular}

  \caption{The sizes of the individual raw corpora before and after rule-based
    cleaning, in millions of sentence pairs.}%
  \label{tab:ende-data-sizes}
\end{table}

To further clean the data, we apply dual cross-entropy filtering, as proposed
by \citet{junczys-dowmunt-2018-dual} and described in the data cleaning
paragraph in Section \ref{sec:training:methodology}.  We train two Transformer
base models (one for each translation direction) for dual cross-entropy
filtering using the clean part of the data after the rule-based cleaning
step.
% \JH{Add more details on these models, perhaps include the hyperparameters
% somewhere in an appendix, plus add the dev scores}

We score the crawled part of the parallel data using the trained models and we
sort the sentence pairs according to the score. To estimate how many of the
crawled sentence pairs can we consider clean, we train the big variant of the
Transformer translation models in both directions on different amounts of the
scored data (following the teacher model hyperparameter settings in Table
\ref{tab:trafo-big-hparams}). We use 25\%, 50\%, 75\% and 100\% of the data,
taking the highest-scoring sentence pairs for training. Based on the \acs{bleu}
scores achieved by the Transformer big models on the development data (the
\ac{wmt} 2019 test set, \citealp{barrault-etal-2019-findings}), we declare the
75\% as the ``clean'' portion of the crawled data. The develoment scores
achieved by the trained models are shown in Table
\ref{tab:dual-cross-entropy-selection}.
% \JH{Add the data sizes after this cleaning step.}

\begin{table}
  \centering
  \begin{tabular}{lrrrr}
    \toprule
    Percentage & 25\% & 50\% & 75\% & 100\% \\
    No. of sentences (M) & 21.9  & 43.9 & 66.5 & 87.7 \\
    \midrule
    En $\rightarrow$ De &  & 43.68 & 43.40 & 42.70 \\
    De $\rightarrow$ En & 41.47 & 41.64 & 42.15 & 42.02 \\
    \bottomrule
  \end{tabular}
  \caption{Development \acs{bleu} scores of the Transformer models trained on
    different amounts of scored crawled parallel data in both directions.}%
  \label{tab:dual-cross-entropy-selection}
\end{table}

% \JH{The following is not true / old:}
% We train our autoregressive models on a mix of the clean data and the
% backtranslations. We use tagged-backtranslation and we oversample the clean
% dataset to form roughly 25\% of the mix. We select the top 100 million sentence
% pairs from the backtranslated data according to Moore-Lewis language modeling
% score. \JH{tento odstavec rozvest aspon na stranku a kus}. We also try using
% the top 150 million. \JH{napsat jinak:} As the size of the clean data is 3.6
% million sentence pairs, we oversample the dataset by a factor of 10 for the 100M
% backtranslation version or 14 for the 150M backtranslation version.

% ------------------------------------------------------------------------------
\paragraph{Backtranslation.} We train three additional translation models (four
in total with the one from the data cleaning step) on the filtered parallel
dataset to create backtranslations \citep{sennrich-etal-2016-improving}. We use
the same hyperparameter settings as for the teacher models (see Table
\ref{tab:trafo-big-hparams}), except for different random seeds for parameter
initialization.

We translate the monolingual data using the four models in an ensemble. We
limit the target sentence length to 150 tokens and use beam search decoding
with 6 hypotheses in the beam, with the length normalization parameter set to
1.

As the source of the monolingual data, we use the News Crawl datasets from
years 2018, 2019, and 2020, as released by the \acs{wmt} organizers
\citep{bojar-etal-2018-findings,barrault-etal-2019-findings,
  barrault-etal-2020-findings}. In total, we gathered 91 million English
sentences for backtranslation into German and 146 million German sentences for
backtranslation into English. Table \ref{tab:mono-data-sizes} shows the sizes
of the monolingual data before and after applying the rule-based filtering
described in the paragraphs above.

\begin{table}
  \centering
  \begin{tabular}{llrr}
    \toprule
    \multicolumn{2}{l}{Data source}  & \mcl{Raw size}  & \mcl{Size after cleaning} \\
    \midrule
    \multirow{3}{*}{English News Crawl} & 2018 & 18.11 & 17.90 \\
                                     & 2019 & 33.60 & 32.80 \\
                                     & 2020 & 41.43 & 40.33 \\
    \multicolumn{1}{r}{total} & & 93.14 & 91.03 \\
    \midrule
    \multirow{3}{*}{German News Crawl} & 2018 & 38.65 & 37.42  \\
                                     & 2019 & 57.62 & 56.33  \\
                                     & 2020 & 53.67 & 52.46 \\
    \multicolumn{1}{r}{total} & & 149.94 & 146.22 \\
    \bottomrule
  \end{tabular}

  \caption{The sizes of the monolingual data (in millions of sentences) used
    for the training, including rule-based data filtering}%
  \label{tab:mono-data-sizes}
\end{table}

We follow the approach of \citet{caswell-etal-2019-tagged} and we tag the
backtranslated sentences with a special token on a first position, as explained
in Section \ref{sec:training:methodology}.

% ------------------------------------------------------------------------------
\paragraph{Teacher Model Training.}  We train the teacher models on shuffled
concatenation of the authentic parallel and tagged backtranslated data. As with
the backtranslation models, we train four models with different seeds for
random initialization in each direction. We show the hyperparameter values in
Table \ref{tab:trafo-big-hparams}.

After the training on mixed parallel and backtranslated data converged, we
continued the training of the models on parallel data only.
% \JH{Early stopping.}
% \JH{Add a table with scores of the teacher models.}

\begin{table}
  \centering
  \begin{tabular}{llr}
    \toprule
    Parameter & Marian config variable & \mcl{Value} \\
    \midrule
    No. of encoder layers & \texttt{enc-depth} & 6 \\
    No. of decoder layers & \texttt{dec-depth} & 6 \\
    Model dimension &  \texttt{dim-emb} & 1,024 \\
    Feed-forward state dimension & \texttt{transformer-dim-ffn} & 4,096 \\
    Attention heads & \texttt{transformer-heads} & 16 \\
    % & transformer-postprocess & dan \\
    % & transformer-postprocess-emb & d\\
    Vocabulary size & & 32,000 \\
    \midrule
    Optimizer method & \texttt{optimizer} & adam \\
    $\beta_1$ & \multirow{3}{*}{\texttt{optimizer-params}} & 0.9 \\
    $\beta_2$ & & 0.998 \\
    $\epsilon$ & & 10\textsuperscript{-9} \\
    No. of batches per update & \texttt{optimizer-delay} & 2 \\
    Fit batch to available memory & \texttt{mini-batch-fit} & true \\
    Learning rate & \texttt{learn-rate}  & 10\textsuperscript{-4} \\
    Learning rate warmup & \texttt{lr-warmup} & 8,000 \\
    Learning rate decay & \texttt{lr-decay-inv-sqrt} & 8,000 \\
    Gradient clipping\footnotemark & \texttt{clip-norm} & 0 \\
    \bottomrule
  \end{tabular}

  \caption{The hyperparameters of the teacher models. The same values were
    used for training the models for backtranslation.}%
  \label{tab:trafo-big-hparams}

\end{table}
% ------------------------------------------------------------------------------
\footnotetext{We did not use gradient clipping because of issues in the Marian
  toolkit implementation.}
  %\JH{is there an issue? Also, put this on the right
%    page.}}  % JH: Perhaps switch to the footnote package, which can handle
             % that better
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
\paragraph{Knowledge Distillation.} The teacher models are used to create
artificial targets for the student models \citep{kim-rush-2016-sequence}. For
each translation direction, we translate the source side of the parallel data
and the authentic monolingual data (in the source language) using the ensemble
of the teacher models. We do not create wholly synthetic datasets by
forward-translating backtranslated data.

For generating the knowledge-distilled dataset, we use beam search with the
beam size of 12 and the normalization parameter set to 0.8. Similarly to
generating the backtranslated data, we limit the target sentence length to 150
tokens.


% ------------------------------------------------------------------------------
\section{Student Models}%
\label{sec:exp:students}
% ------------------------------------------------------------------------------

In this section, we describe our non-autoregressive student models. We
implemented the \acs{ctc}-based Transformer architecture in the Marian toolkit
\citep{junczys-dowmunt-etal-2018-marian}. For the computation of the \ac{ctc}
loss, we use the warp-ctc
library,\footnote{\url{https://github.com/baidu-research/warp-ctc/}} an
efficient parallelized implementation for GPUs \citep{amodei-etal-2016-deep}.

We experiment with different hyperparameter settings that control the size of
the student model. Our baseline non-autoregressive model is a big Transformer
model with 6 encoder layers, followed by the state-splitting layer, and another
6 decoder layers. Apart from the \ac{ctc}-specific configuration, we use the
same hyperparameters as in the teacher models, shown in Table
\ref{tab:trafo-big-hparams}. In addition, we train four smaller models based on
the Transformer base hyperparameters -- using model dimension of 512,
feed-forward state dimension of 2,048, and 8 attention heads. Each smaller
model uses a different number of encoder and decoder layers. We show the number
of encoder and decoder layers in Table \ref{tab:student-model-hparams}. In all
settings, we use the splitting factor of 3 in the state splitting layer.

\begin{table}
  \centering

  \begin{tabular}{llrr}
    \toprule
    Model & Base architecture & \mcl{Encoder layers} & \mcl{Decoder layers} \\
    \midrule
    Teacher & Transformer big & 6 & 6 \\
    \midrule
    Large & Transformer big & 6 & 6 \\
    \addlinespace
    Base  & \multirow{4}{*}{Transformer base} & 6 & 6 \\
    Small & & 3 & 3\\
    Micro & & 2 & 2 \\
    Tiny  & & 1 & 1 \\
    \bottomrule
  \end{tabular}

  \caption{The hyperparameters of the student models.}%
  \label{tab:student-model-hparams}
\end{table}

All models were trained for approximately three weeks on four Nvidia Tesla P100
GPUs. Every 8,000 updates of the training, a model checkpoint was saved and
validated. As the validation dataset, we used the concatenation of three
\ac{wmt} test sets, from years 2018, 2019, and 2020
\citep{bojar-etal-2018-findings, barrault-etal-2019-findings,
  barrault-etal-2020-findings}.  Figures \ref{fig:ende-learning-curves} and
\ref{fig:deen-learning-curves} show the training progress of the student
models. Each plot shows the validation \acs{bleu} scores after a given number
of parameter updates.

\begin{figure}
  \centering
  \input{./img/learning-curves/en-de.curves.tex}

  \caption{The learning curves of the English $\rightarrow$ German \ac{nat}
    models. The \acs{bleu} scores are reported on the concatenation of the test
    sets from \acs{wmt} 18, 19, and 20.}%
  \label{fig:ende-learning-curves}
\end{figure}

\begin{figure}
  \centering
  \input{./img/learning-curves/de-en.curves.tex}

  \caption{The learning curves of the German $\rightarrow$ English \ac{nat}
    models. The \acs{bleu} scores are reported on the concatenation of the test
    sets from \acs{wmt} 18, 19, and 20.}%
  \label{fig:deen-learning-curves}
\end{figure}

% ------------------------------------------------------------------------------
\subsection{Lexical Shortlist}
\label{subsec:exp:shortlist}
% ------------------------------------------------------------------------------

Shortlisting is a method used for speeding up the decoding out of a translation
model, originally proposed by \citet{jean-etal-2015-using} as means to tackle
the open vocabulary problem on the generation side (see Section
\ref{subsec:openvoc}). Since then, it has been used as an optimization method
for creating efficient translation systems \citep{kim-etal-2019-research,
  bogoychev-etal-2020-edinburghs}.  In our experiments, we employ shortlisting
as one of the standard optimization techniques to find out whether it has a
positive effect on the decoding speed not only in \ac{ar} models, but also in
\ac{nar} models.

A lexical shortlist limits the number of possible output tokens, reducing the
dimension of the output projection matrix (recall the dimension of the output
matrix is otherwise $d \times |V|$) and thus saving time on matrix
multiplication operation. Output probabilities of the tokens which are not in
the shortlist are not computed (and these tokens are never generated).

To use lexical shortlists during decoding, we need to create a bilingual
lexicon first. Using a parallel corpus, we compute the word alignment. Based on
the alignment, the bilingual lexicon is the set of target tokens aligned with
each source token, along with the occurrence frequencies.

During decoding, a new lexical shortlist is generated for every new batch. To
create a shortlist for a given batch, the potential translations of every
source token are added to the shortlist. Specifically in our settings, we
include 100 most frequent and 100 most probable translations of each source
token.

% ------------------------------------------------------------------------------
\section{Results}%
\label{sec:exp:results}
% ------------------------------------------------------------------------------

In this section we analyze the results both in terms of translation quality
(Subsection \ref{subsec:results:quality}) and decoding speed (Subsection
\ref{subsec:results:time}). \JH{we present the discussion and comparison to
  other methods in \ref{subsec:results:discussion}.}


% ------------------------------------------------------------------------------
\subsection{Translation Quality}%
\label{subsec:results:quality}
% ------------------------------------------------------------------------------

We compare the results of our models to both related work on \ac{nat} and to
the results of the \ac{wmt}~21 Efficiency Shared Task which features highly
optimized \ac{ar} translation models \citep{heafield-etal-2021-findings}. In
the \ac{nar} literature, the \acs{wmt}~14 test set
\citep{bojar-etal-2014-findings} is used as a standard benchmark. On the other
hand, the efficiency task uses the recent test set from \acs{wmt}~21
\citep{akhbardeh-etal-2021-findings}. In this section, we present the results
on both datasets.

\paragraph{Results on the \acs{wmt}~21 test set.}
As argued in Section \ref{sec:evaluation}, \acs{bleu} alone is not necessarily
a reliable automatic metric. In line with the evaluation methodology of the
efficiency task, we also measure \acs{comet} \citep{rei-etal-2020-comet} and
\acs{chrf} score \citep{popovic-2015-chrf}. We do not perform human evaluation.

Table \ref{tab:wmt21-scores} shows the results of automatic evaluation
% with \acs{bleu}, \acs{chrf}, and \acs{comet}% scores
on the \acs{wmt}~21 news translation test set. In the English $\rightarrow$
German direction, the test set consists of 1,002 sentences along with three
different reference translations. In the German $\rightarrow$ English
direction, there are 1,000 sentences with two reference translations each. We
measure multi-reference \acs{bleu} score using Sacrebleu \citep{post-2018-call}
and we report confidence intervals computed with bootstrap resampling. We
report the Sacrebleu signatures for English $\rightarrow$ German%
\footnote{En$\rightarrow$De: {\scriptsize
    \texttt{nrefs:3|bs:1000|seed:12345|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0}}}
and German $\rightarrow$ English%
\footnote{De$\rightarrow$En: {\scriptsize
    \texttt{nrefs:2|bs:1000|seed:12345|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0}}}
directions. We compute the \acs{chrf} score separately on each reference
translation set using Sacrebleu, and we report the
average.\footnote{\acs{chrf}; En$\leftrightarrow$De: {\scriptsize
    \texttt{nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.0.0}}}
Finally, we compute the \acs{comet} scores with the \texttt{wmt20-comet-da}
model of the \acs{comet} version \texttt{dd2298} (1.0.0rc9).

We observe that in both translation directions, the \ac{ar} models outperform
the \ac{nar} models. The performance gap between the models grows further with
beam search and ensembling. We can also see that knowledge distillation has a
positive effect on both \ac{ar} and \ac{nar} models, with the student \ac{ar}
model matching the performance of the ensemble of four large teacher models.
We also note that the difference in the \acs{comet} score is bigger than in
\acs{bleu}, which might suggest that \acs{nar} models would rank poorly in
human evaluation, despite achieving reasonable \acs{bleu} scores.

% \JH{We observe... gaps everywhere, AR clearly better in translation. relatively
%   small gaps in ChrF scores, moderate gaps in bleu, large gaps in COMET which
%   could suggest that NAR systems will rank poorly in human evaluation even
%   though they achieve high \acs{bleu} scores.}

The results on the test set confirm the ranking of the \acs{nar} models as seen
during training (see Figures \ref{fig:ende-learning-curves} and
\ref{fig:deen-learning-curves}), including the interesting exception of the
English $\rightarrow$ German Micro model. Otherwise, the larger the student
model is, the better scores it achieves.
% \JH{we have experiments with tied embeddings but I would keep them out
% because the results were the same.}

Finally, we see that using a lexical shortlist does not have an effect on the
translation quality in all model variants. However, we see that shortlisting
impairs the performance of the Transformer base model when decoding with beam
search.

\begin{table}
  \centering

  \begin{tabular}{lrrr@{}>{\small \enspace \textpm}lrrr@{}>{\small \enspace
        \textpm}l}
    \toprule
    \multirow{2}{*}{\bf En $\rightarrow$ De} %
    & \multicolumn{4}{c}{Full output projection} & \multicolumn{4}{c}{Shortlist} \\
    & \acs{chrf} & \acs{comet} & \multicolumn{2}{c}{\acs{bleu}}
    & \acs{chrf} & \acs{comet} & \multicolumn{2}{c}{\acs{bleu}} \\

    \midrule
    Single greedy \acs{ar} \\
    Large & 59.2 & 0.4110 & 50.5 & 1.3 & 59.2 & 0.4124 & 50.6 & 1.3\\
    Base  & 58.1 & 0.3881 & 47.9 & 1.3 & 58.1 & 0.3875 & 47.9 & 1.2\\

    \addlinespace
    Single beam \acs{ar} \\
    Large & 58.8 & 0.4053 & 50.8 & 1.3 & 58.8 & 0.4144 & 47.9 & 1.2\\
    Base  & 57.9 & 0.3873 & 48.0 & 1.3 & 55.1 & 0.2666 & 39.3 & 1.1\\

    \addlinespace
    Ensemble beam \acs{ar} \\
    Large & 59.5 & 0.4332 & 52.2 & 1.3 & 59.4 & 0.4303 & 52.2 & 1.3\\

    \midrule
    Student \acs{ar} \\
    Base  & 59.5 &  0.4550 & 51.6 & 1.2 & 59.6 &  0.4564 & 51.6 & 1.2 \\

    \addlinespace
    \Acs{nar} models \\
    Large & 58.6 &  0.1485 & 47.8 & 1.2 & 58.7 &  0.1442 & 47.7 & 1.2 \\
    Base  & 56.3 & -0.0521 & 41.8 & 1.1 & 56.3 & -0.0545 & 41.8 & 1.1 \\
    Small & 56.2 & -0.0752 & 41.9 & 1.1 & 56.2 & -0.0773 & 41.9 & 1.2 \\
    Micro & 57.3 & -0.0083 & 43.5 & 1.1 & 57.4 & -0.0085 & 43.6 & 1.1 \\
    Tiny  & 53.6 & -0.3333 & 34.7 & 1.0 & 53.8 & -0.3346 & 34.8 & 1.0 \\

    \bottomrule
  \end{tabular}

%   \caption{Quantitative results of the English $\rightarrow$ German translation
%     models on the \acs{wmt}~21 test set using three different automatic
%     evalutation metrics.}%
%   \label{tab:wmt21-scores-ende}
% \end{table}

% \begin{table}
%   \centering

  \vspace{1\baselineskip}

  \begin{tabular}{lrrr@{}>{\small \enspace \textpm}lrrr@{}>{\small \enspace
        \textpm}l}
    \toprule
    \multirow{2}{*}{\bf De $\rightarrow$ En} %
    & \multicolumn{4}{c}{Full output projection} & \multicolumn{4}{c}{Shortlist} \\
    & \acs{chrf} & \acs{comet} & \multicolumn{2}{c}{\acs{bleu}}
                 & \acs{chrf} & \acs{comet} & \multicolumn{2}{c}{\acs{bleu}} \\

    \midrule
    Single greedy \acs{ar} \\
    Large & 61.9 & 0.5868 & 48.4 & 1.3 & 61.9 & 0.5866 & 48.5 & 1.3\\
    Base  & 61.0 & 0.5532 & 47.0 & 1.3 & 60.5 & 0.5534 & 47.1 & 1.3\\

    \addlinespace
    Single beam \acs{ar} \\
    Large & 61.5 & 0.5885 & 49.2 & 1.2 & 61.2 & 0.5659 & 43.9 & 1.1\\
    Base  & 60.7 & 0.5534 & 47.4 & 1.3 & 58.0 & 0.4591 & 38.5 & 1.2\\

    \addlinespace
    Ensemble beam \acs{ar} \\
    Large & 62.0 & 0.5954 & 50.6 & 1.3 & 62.3 & 0.5970 & 50.8 & 1.3\\

    \midrule
    Student \acs{ar} \\
    Base & 63.3 & 0.6115 & 51.1 & 1.3 & 63.3 & 0.6112 & 51.1 & 1.3\\

    \addlinespace
    \Acs{nar} models \\
    Large & 61.6 &  0.3296 & 46.4 & 1.4 & 61.6 &  0.3288 & 46.4 & 1.4\\
    Base  & 61.4 &  0.2957 & 45.8 & 1.3 & 61.4 &  0.2663 & 45.7 & 1.3\\
    Small & 61.0 &  0.2462 & 44.6 & 1.3 & 61.0 &  0.2454 & 44.6 & 1.3\\
    Micro & 59.6 &  0.1475 & 42.3 & 1.4 & 59.6 &  0.1468 & 42.3 & 1.4\\
    Tiny  & 55.9 & -0.1558 & 34.4 & 1.3 & 55.9 & -0.1560 & 34.4 & 1.3\\

    \bottomrule
  \end{tabular}

  \caption{Quantitative results of the German $\leftrightarrow$ English
    translation models on the \acs{wmt}~21 test set using \acs{chrf},
    \acs{comet}, and \acs{bleu}.}%
  \label{tab:wmt21-scores}
\end{table}


\paragraph{Results on the \acs{wmt}~14 test set.}
We present automatic evaluation results measured on the \acs{wmt}~14 test set
to provide a comparison to the related work on \acl{nar} models (see Table
\ref{tab:related:wmt14} in Section \ref{sec:nat:discussion} for reference).  In
Table \ref{tab:wmt14-bleu-scores} we show the \acs{bleu} scores achieved by our
\ac{nar} student models on this test set.

Since many of the related approaches stop the training early after 300 thousand
updates \citep{gu2017nonautoregressive, gu-kong-2021-fully}, we report the
scores of our models both at this point in training, and after the training
reached convergence.

Table \ref{tab:wmt14-weight-avg} shows the \acs{wmt}~14 \acs{bleu} scores with
checkpoint averaging. In each variant, we take the average parameters of the
five best scoring models as measured on the validation set (either before the
300,000th update or overall). In contrast to our previous experiments in
Chapter \ref{chap:nar-nmt-ctc} (see Table \ref{tab:end-to-end:bleu}), we found
that checkpoint averaging only has a small effect on the translation quality in
terms of \acs{bleu}.

\begin{table}
  \centering
  \begin{tabular}{lrrrr}
    \toprule
    & \multicolumn{2}{c}{En $\rightarrow$ De}
    & \multicolumn{2}{c}{De $\rightarrow$ En} \\
    Model
    & 300k & Final & 300k &  Final \\
    \midrule

    Large & 27.6 & 28.1 & 29.5 & 30.9 \\
    Base  & 22.4 & 23.7 & 27.7 & 29.8 \\
    Small & 22.4 & 23.7 & 26.5 & 28.7 \\
    Micro & 23.6 & 25.1 & 25.5 & 27.2 \\
    Tiny  & 19.0 & 20.3 & 19.5 & 21.5 \\

    \bottomrule
  \end{tabular}

  \caption{The \acs{bleu} scores of the \emph{single best} models on the
    \acs{wmt}~14 test set after 300k updates and at the end of the training.}%
  \label{tab:wmt14-bleu-scores}
\end{table}

\begin{table}
  \centering

  \begin{tabular}{lrrrr}
    \toprule
    & \multicolumn{2}{c}{En $\rightarrow$ De}
    & \multicolumn{2}{c}{De $\rightarrow$ En} \\
    Model
    & 300k & Final & 300k &  Final \\
    \midrule

    Large & 27.7 & 28.4 & 30.0 & 31.3 \\
    Base  & 22.4 & 23.7 & 28.1 & 30.3 \\
    Small & 22.5 & 23.6 & 26.7 & 29.1 \\
    Micro & 23.7 & 25.0 & 25.1 & 27.5 \\
    Tiny  & 19.0 & 20.3 & 19.6 & 21.7 \\

    \bottomrule
  \end{tabular}

  \caption{The \acs{bleu} scores of the \emph{averaged} models on the
    \acs{wmt}~14 test set after 300k updates and at the end of the training.}%
  \label{tab:wmt14-weight-avg}

\end{table}


% ------------------------------------------------------------------------------
\subsection{Decoding Time}%
\label{subsec:results:time}
% ------------------------------------------------------------------------------

In this section, we analyze the decoding speed of the English $\rightarrow$
German \ac{nar} models. We aim at recreating the evaluation conditions
following the \acs{wmt}~21 Efficiency Shared Task
\citep{heafield-etal-2021-findings}. We measure the decoding latency and
throughput in different hardware environments.

The decoding time is measured on a dataset containing one million sentences to
minimize the effect of the model loading overhead.

For measuring the CPU times, we use the same environment as the \acs{wmt}~21
shared task organizers, which is a dual-socket Intel Xeon Gold 6354 from Oracle
Cloud, a 36-core CPU server. For GPU efficiency, we use an Nvidia A100 GPU.
% albeit on a slightly different server architecture. \JH{what arch exactly?}
We also include our results on an Nvidia P100 GPU which has been used often in
the literature (see Table \ref{tab:related:hardware} in Section
\ref{sec:nat:discussion}).

\paragraph{GPU Latency and Throughput.}
Figures \ref{fig:throughput:p100} and \ref{fig:throughput:a100} plot the times
to translate the test set on P100 and A100 GPUs, respectively. Each figure
shows the relationship between the batch size and the decoding time in seconds
and includes a table with the measured data. The runs corresponding to the
missing entries in the table did not finish in 24 hours.

Setting aside the absolute values (as expected, an A100 is faster than a P100),
both GPU settings yield similar results. For \ac{ar} models, increasing the
batch size heavily reduces the overall decoding speed and eventually surpasses
the large \ac{nar} models. Increasing the batch size speeds up the decoding in
\ac{nar} models as well, but the effect is diminished for larger batch sizes.

From this point of view, the optimal scenario for \ac{nar} models is a
situation in which the system runs in online mode, i.e. with a batch size of
1, or with a batch size of a few sentences. Using a faster GPU with more
threads increases the batch size when \ac{ar} models meet the speed of \ac{nar}
models.

We measured the GPU decoding time both with and without shortlisting. However,
the differences were minimal. Therefore, we only report decoding times without
using lexical shortlists.

\begin{figure}
  \centering

  \input{./img/throughputs/p100.tex}
  \vspace{1\baselineskip}

  \begin{tabular}{lrrrrrrrr}
    \toprule
    Batch size & 1 & 2 & 4 & 8 & 16 & 32 & 64 & 128\\
    \midrule
    \acs{ar} -- Large &&        & 44,838 & 26,231 & 16,741 & 11,266 & 8,926 & 7,449 \\
    \acs{ar} -- Base  && 61,965 & 33,957 & 18,503 & 11,087 &  6,677 & 4,503 & 3,409 \\
    \midrule
    Large & 14,157 & 10,285 & 8,079 & 7,003 & 6,452 & 6,056 & 5,767 & 5,581 \\
    Base  &  8,720 &  5,532 & 3,875 & 3,146 & 2,769 & 2,535 & 2,394 & 2,303 \\
    Small &  5,017 &  3,270 & 2,376 & 1,977 & 1,748 & 1,635 & 1,564 & 1,512 \\
    Micro &  3,781 &  2,462 & 1,871 & 1,579 & 1,418 & 1,343 & 1,282 & 1,248 \\
    Tiny  &  1,916 &  1,295 &   960 &   803 &   730 &   687 &   666 &   652 \\
    \bottomrule
  \end{tabular}

  \caption{Wall times to translate one million sentences (in seconds) on a
    single Nvidia \emph{Pascal} P100 GPU with different batch size settings.}%
  \label{fig:throughput:p100}
\end{figure}

\begin{figure}
  \centering

  \input{./img/throughputs/a100.tex}
  \vspace{1\baselineskip}

  \begin{tabular}{lrrrrrrrr}
    \toprule
    Batch size & 1 & 2 & 4 & 8 & 16 & 32 & 64 & 128 \\
    \midrule
    \acs{ar} -- Large && 53,902 & 29,369 & 15,351 & 8,907 & 5,216 & 3,090 & 1,918 \\
    \acs{ar} -- Base  && 47,145 & 25,745 & 13,836 & 7,498 & 3,997 & 2,371 & 1,465 \\
    \midrule
    Large & 7,020 & 3,874 & 2,292 & 1,547 & 1,179 & 973 & 850 & 782 \\
    Base  & 6,289 & 3,400 & 1,854 & 1,166 &   816 & 635 & 542 & 485 \\
    Small & 3,300 & 1,860 & 1,051 &   717 &   526 & 434 & 380 & 357 \\
    Micro & 2,322 & 1,345 &   833 &   544 &   433 & 367 & 332 & 311 \\
    Tiny  & 1,360 &   780 &   503 &   367 &   301 & 273 & 252 & 243 \\
    \bottomrule
  \end{tabular}

  \caption{Wall times to translate one million sentences (in seconds) on a
    single Nvidia \emph{Ampere} A100 GPU with different batch size settings.}%
  \label{fig:throughput:a100}
\end{figure}


% -----------------------------------------------------------------------------
\paragraph{CPU Latency and Throughput.}
In Figure \ref{fig:throughput:cpu36}, we show the CPU decoding times of the
trained models using 36 CPU cores with different batching settings.  We see
similar trends to GPU decoding -- the \ac{nar} models are faster with smaller
batch sizes. When the batch size is increasing, the \ac{ar} models eventually
match the decoding speed of the \ac{nar} models. Unlike GPU decoding, there is
a considerable difference between the large and base models in both \ac{ar} and
\ac{nar} variants.

We also notice that increasing the batch size can even slow down the decoding
speed. There may be several reasons for this behavior. First, the size of the
shortlist grows proportionally with the size of the batch, as there are more
possible target words. Second, when the batch size is too large, much of the
computation is wasted on the padded positions in shorter sentences (as
explained in Section \ref{sec:training:methodology}). These issues are not
evident in GPU decoding due to a much higher level of parallelism.

It is apparent from the data table in Figure \ref{fig:throughput:cpu36} that
using lexical shortlist improves the decoding speed of both \ac{ar} and
\ac{nar} models. For clarity, we only plot the measured decoding times with
shortlisting. We see that changing the batch size has similar effect in both
cases, perhaps with the exception of the micro and tiny models which benefit
greatly from the shortlist in combination with a small batch size.

\JH{a limitation of the implementation: missing fields mean out-of-memory,
  comment on how marian behaves on multiple CPUs, ugh.}

\JH{times on a single CPU core are all rubbish so we do not even report them in
  a table}



\begin{figure}
  \centering

  \input{./img/throughputs/cpu-shortlist.tex}
  \vspace{1\baselineskip}

  \begin{tabular}{lrrrrrrrr}
    \toprule
    Batch size & 1 & 2 & 4 & 8 & 16 & 32 & 64 & 128 \\


    \midrule
    Full output \\
    \acs{ar} -- Large & 52,087 &  &  &  & 12,166 & 10,151 & 9,370 & 9,244 \\
    \acs{ar} -- Base  & 16,293 & 9,727 & 7,191 & 5,420 & 3,635 & 2,925 & 2,707 & 2,664 \\
    \addlinespace
    Large & 14,542 &       &       &       & 8,357 & 8,247 & 8,238 & \\
    Base  &  3,508 &       &       &       & 2,921 & 2,920 & 2,934 & 2,948 \\
    Small &  2,346 &       &       & 1,933 & 1,921 & 1,921 & 1,936 & 1,950 \\
    Micro &  1,952 &       &       &       & 1,588 & 1,587 & 1,607 & 1,627 \\
    Tiny  &    784 &       &       &   687 &   685 &   694 &   701 &   719 \\


    \midrule
    Shortlist \\
    \acs{ar} -- Large & 41,168 & 27,977 & 18,914 & 13,597 & 10,946 & 9,643 & 9,154 & 9,090 \\
    \acs{ar} -- Base  & 10,555 &  6,776 &  4,660 &  3,664 &  2,957 & 2,643 & 2,564 & 2,587 \\
    \addlinespace
    Large & 12,799 & 9,870 & 8,245 & 7,545 & 7,434 & 7,511 & 7,639 & \\
    Base  &  2,549 & 2,298 & 2,263 & 2,306 & 2,399 & 2,503 & 2,609 & 2,707 \\
    Small &  1,346 & 1,246 & 1,250 & 1,306 & 1,399 & 1,497 & 1,606 & 1,714 \\
    Micro &    958 &   897 &   913 &   974 & 1,062 & 1,163 & 1,271 & 1,380 \\
    Tiny  &    244 &   246 &   273 &   316 &   373 &   437 &   506 &   582 \\


    \bottomrule
  \end{tabular}

  \caption{Wall times to translate one million sentences (in seconds) on 36 CPU
    cores with different batch size settings. The graph shows the decoding
    times with shortlisting.}%
  \label{fig:throughput:cpu36}
\end{figure}


% ------------------------------------------------------------------------------
\subsection{Discussion}%
\label{subsec:results:discussion}
% ------------------------------------------------------------------------------

\JH{V diskusi musim rict: First, we achieved the best scores on wmt 14, but
  those exact models score very poorly on wmt21, especially with the COMET
  score which should simulate human evaluation \citep{rei-etal-2020-comet,
    popovic-2015-chrf}}

\JH{dale musim zminit comparison s edinburkem \citep{Behnke-wmt21-speed} v
  tabulce \ref{tab:efficiency:comparison}.}

\JH{from the comparison we see that the autoregressive edinburgh base model is
  superior in most regards, except for the GPU decoding latency. As we discuss
  in Section \ref{sec:nat:discussion}, these conditions are the only considered
  scenario in most of the related studies. }


\begin{table}
  \centering

  \begin{tabular}{lrrrrrr}
    \toprule
    & \multicolumn{3}{c}{Translation quality} & \multicolumn{3}{c}{Decoding time (seconds)} \\
    & {\small \acs{chrf}} & {\small \acs{comet}} & {\small \acs{bleu}} & {\small GPU, b>1} & {\small GPU, b=1} & {\small CPU, b>1} \\
    \midrule
    Edinburgh base & 61.5 & 0.527 & 55.3 & 140 & 16,851 & 500 \\
    \midrule
    \acs{ar} -- Large (teacher) & 59.2 & 0.411 & 50.5 & 1,918 & {\it > 24h} & 9,090 \\
    \acs{ar} -- Base (student) & 59.5 & 0.455 & 51.6 & 1,465 & {\it > 24h} & 2,587 \\
    \addlinespace
    \acs{nar} -- Large & 58.6 & 0.149 & 47.8 & 782 & 7,020 & 7,434 \\
    \acs{nar} -- Micro & 57.3 & -0.008 & 43.5 & 311 & 2,322 & 897 \\
    \bottomrule
  \end{tabular}

  \caption{Comparison of our models with the Edinburgh ``base'' model submitted
    to the \acs{wmt} Efficient Translation Shared Task
    \citep{Behnke-wmt21-speed}. Columns denoted \emph{b>1} show the best result
    using batching, \emph{b=1} is measured with a single sentence in the
    batch. The CPU times were measured using 36 CPU cores.} %
  \label{tab:efficiency:comparison}
\end{table}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
