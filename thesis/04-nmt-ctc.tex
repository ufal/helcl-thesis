% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Non-Autoregressive NMT with Connectionist Temporal Classification}
\label{chap:nar-nmt-ctc}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paperdisclaim{This chapter is based on paper ``End-to-End Non-Autoregressive
  Neural Machine Translation with Connectionist Temporal Classification'',
  joint work with Jindřich Libovický, published at EMNLP 2018.}

\noindent
In this chapter, we lay grounds for the \ac{nar} approaches studied in this
thesis. We describe our experiments with an architecture based on the \ac{ctc}
loss \citep{libovicky-helcl-2018-end}. \JH{ok to self-cite when sections will
  be based on this?} \JH{use \ac{nat} here somewhere}


We begin with the description of the \ac{ctc} algorithm for compute the
cross-entropy loss over all possible alignments between the reference sentence
and the sequence of output states in Section \ref{sec:ctc}. Section
\ref{sec:ctc:arch} introduces the proposed \ac{nat} model architecture based on
the Transformer model \citep{vaswani2017attention}. The results of the
preliminary experiments and the limitations are discussed in Section
\ref{sec:ctc:experiments}. In Section \ref{sec:ctc:fluency}, we present our
attempt to address the most severe issue non-autoregressive models have, which
is the reduced fluency compared to the autoregressive models
\citep{kasner2020improving,kasner2020incorporating}.

% -----------------------------------------------------------------------------
\section{Connectionist Temporal Classification}
\label{sec:ctc}
% -----------------------------------------------------------------------------

\Ac{ctc} \citep{graves2006connectionist} is a method for training neural
networks on sequential data. Originally applied to the phonetic labelling task,
but later successfully adapted in related areas, including \ac{asr} or
handwriting recognition \citep{liwicki2007novel, eyben2009speech,
  graves2014towards}.

The main strength of \ac{ctc} becomes evident in tasks where the input and
output labels are weakly or not at all aligned, for example in situations where
the observed input sequence is considerably longer than the target output
sequence -- hence the application to \ac{asr}, where the number of extracted
features per second is higher than the number of phonemes uttered per second.
\JH{Confirm this.}

Training neural networks with \ac{ctc} is independent on the actual neural
network architecture. The \ac{ctc} loss function can be applied on any network
with sequential outputs. Thus, this method is applicable to both \acp{rnn} and
the Transformer model.

Models trained with the \ac{ctc} assume that the alignment between the input
(e.g. a group of frames in an audio signal) and the output (e.g. a phoneme)
states is unknown. A variable number of frames in a row can encode a single
phoneme. Similarly, in translation, multiple words in the source language may
correspond to any number of (even non-consecutive) words in the target
language.

The idea behind \ac{ctc} is to allow some states to produce no output. This is
realized by introducing a special blank token in the target vocabulary.
Optionally, identical outputs produced by multiple consecutive states may be
merged and considered a single output. Because of these properties, there are
groups of equivalent output sequences, which all represent the same target, as
illustrated in Figure~\ref{fig:ctc-equivalent-sequences}.

\begin{figure}
  \centering
  \begin{minipage}{\textwidth}
    \begin{equation*}
        \text{a cat sat on a mat} =
        \begin{cases}
          & \text{a <blank> cat sat on a <blank> mat} \\
          & \text{a a cat cat sat on a mat} \\
          & \text{a <blank> cat cat sat on a mat} \\
        \end{cases}
    \end{equation*}
  \end{minipage}
  \caption{A group of output sequences of equal length which all represent the
    same target in CTC.} %
  \label{fig:ctc-equivalent-sequences}
\end{figure}

In the standard sequence-to-sequence architectures, the value of the loss
function is defined as the sum of the cross entropies of each output state with
respect to the target sequence (see Equation \ref{eq:loss}). In \ac{ctc}, the
loss is defined as the sum of cross-entropy losses of all of the output
sequences equivalent to the given target sequence:
%
\begin{equation}
  J_{\theta}^{\text{CTC}} = - \sum_{(x, y) \in D} \sum_{y' \sim y}  \log p(y' | x, \theta)
  \label{eq:ctc-loss}
\end{equation}
%
where $\sim$ denotes the equivalence relation.  \JH{$J_{\theta}$ should perhaps
  be $J(\theta)$. Also, consider the $\sim$ sign.}

The inner summation in Equation \ref{eq:ctc-loss} is computed over all possible
sequences equivalent to the label sequence. For technical purposes, the label
sequences are limited to a fixed length, which greatly reduces the number of
acceptable hypotheses. However, the number of equivalent hypotheses of a given
length still grows exponentially with the sequence length -- in \ac{ctc}, the
fixed length is always set to be longer than the label sequence. \JH{confirm
  the exponential claim}

The summation over the large set of equivalent sequences can be
implemented using dynamic programming. When both the length of the output and
the length of the target sequences are known, there is a constant number of the
blank tokens to be generated. The process of computing the loss of the whole
output sequence is divided into computing the partial losses with respect to
the possible label prefixes.

\begin{figure}
  \centering

  \includegraphics[width=13cm]{img/ctc_schema.png}

  \caption{An illustration of the algorihm for the CTC loss computation. Each
    node denotes producing either a token from the label sequence, or the blank
    token. Each path from one of the two top-left nodes to one of the two
    bottom-right nodes corresponds to one of the equivalent sequences.  }
  \label{fig:ctc-dynamic-programming}
\end{figure}

The \ac{ctc} loss computation is illustrated in Figure
\ref{fig:ctc-dynamic-programming}. The rows represent tokens from the label
sequence plus the optional blank tokens. The columns represent the output state
sequence.  Each node in the graph denotes generating a label from an output
state. The arrows show valid transitions between the generation steps. An arrow
can only go down one or two rows, or horizontally.  The horizontal arrows
denote repeated generation of the same label. These labels are later merged to
form a single output. An arrow can only go two rows down when the skipped row
corresponds to the blank token, so no target tokens are left out. Each path in
the diagram therefore shows one of the equivalent sequences that lead to
generating the given label sequence.

Using the idea that many of the paths from left to right in the diagram share
segments, we can apply dynamic programming to compute the sum of losses across
all paths without the need to enumerate each of them. A node on coordinates
$(i,j)$ stores the accumulated losses for the all path prefixes that lead to
the node, added with the negative log-likelihood of the label on the $i$-th row
being generated by the $j$-th output state. The two bottom-right nodes then
store the sum of losses of all the paths.


\JH{doplnit} The training of the network with \ac{ctc} is done by minimizing
the \ac{ctc} loss function, which is defined as follows.

% -----------------------------------------------------------------------------
\section{Model Architecture}
\label{sec:ctc:arch}
% -----------------------------------------------------------------------------

As said in the previous chapter, training models with \ac{ctc} does not impose
any requirements on the model architecture. In our experiments, we aim for a
reasonable comparison between our proposed approach and the state-of-the-art
autoregressive models. We adapt the Transformer model and use similar
hyperparameters where applicable.

Non-autoregressive models generate the outputs in parallel, which requires that
the output length is known beforehand. In autoregressive models, the end of
sequence is indicated by a special end symbol, and the constraint on maximum
length is merely a technical aspect.

To leverage the ability to output empty tokens to the full extent, the output
length should be set to a higher number than the length of the target sequence.
Since the length estimation does not need to be accurate, we select a number
$k$ and we set the target sequence length to be $k$ times longer than the
source length. Note that in case the selected length is shorter than the label
sequence, the model will not be able to generate the whole target sequence.

\begin{figure}
  \centering
  \input{./img/non-autoregressive.tex}

  \caption{The scheme of the non-autoregressive architecture with
    state-splitting and CTC. The image is taken from
    \citet{libovicky-helcl-2018-end}, Figure 1.}%
  \label{fig:state-splitting}
\end{figure}


We implement the source-to-target length expansion by linear projections and
state splitting. This mechanism is illustrated in Figure
\ref{fig:state-splitting}. After a given Transformer layer completes its
computation, we linearly project the states
$h_1, \ldots, h_{T_x} \in \mathbb{R}^d$ into $\mathbb{R}^{kd}$. Then, we split
each of these projections into $k$ parts, which results to a $k$-times longer
sequence of states $s_1, \ldots, s_{k \cdot T_x}$ of the original dimension
$d$:
%
\begin{equation}
  s_{ck+b} = \left( W_{\text{spl}} h_c + b_{\text{spl}} \right)_{bd:(b+1)d}
\end{equation}
%
for $b=0 \ldots k-1$ and $c=1 \ldots T_x$ where
$W_{\text{spl}} \in \mathbb{R}^{d \times kd}$ and
$b_{\text{spl}} \in \mathbb{R}^{kd}$ are the trainable projection parameters.

We experiment with two placement options of the state splitting layer. First,
we try placing the state splitting at the end of the Transformer layer
stack. In this scenario, there are 12 Transformer encoder layers, followed by
the state splitting layer, whose outputs are used in the output
projection. Second, we place the state splitting layer in the middle of the
Transformer layer stack, mimicking the 6-layer encoder-decoder architecture of
the autoregressive Transformer model. In the second variant, cross-attention
can be included in the second half of the layers, which attends to the states
right after state splitting.

% -----------------------------------------------------------------------------
\section{Preliminary Experiments}%
\label{sec:ctc:experiments}
% -----------------------------------------------------------------------------

\JH{Section with experiments from the 2018 paper. Should show that it works,
  but it's still far from perfect.}

We conduct experiments with \ac{ctc}-based \ac{nat} models trained on
English--German and English--Romanian translation in both directions.

\paragraph{Data.}
In our experiments, we use the parallel data provided by the \acs{wmt}
organizers. For English--German, the training data consist of the Europarl
corpus \citep{koehn-2005-europarl}, News commentary
\citep{tiedemann-2012-parallel}, and Common
Crawl.\footnote{\url{https://commoncrawl.org/}} For validation, we use the
WMT~13 test set \citep{bojar-etal-2013-findings}, and we evaluate the
translation quality on the WMT~14 test set
\citep{bojar-etal-2014-findings}. For English--Romanian, the data consist of
the Europarl corpus, and the SETIMES corpus distributed by OPUS
\citep{tiedemann-2012-parallel}. We use the development and test set from
WMT~16 \citep{bojar-etal-2016-findings}. The data sizes are shown in
Table~\ref{tab:end-to-end:data}.

\begin{table}
  \centering
  \begin{tabular}{lrr}
    \toprule
     & Sentence pairs & Vocabulary size \\
    \midrule
    En -- De & 4.58 M & 41,488 \\
    En -- Ro & 613 k & 38,440 \\
    \bottomrule
  \end{tabular}

  \caption{The training data sizes. \JH{more}}%
  \label{tab:end-to-end:data}
\end{table}

We preprocess the data with scripts from the \texttt{mosesdecoder}
repository,\footnote{\url{https://github.com/moses-smt/mosesdecoder/}} a part
of the Moses translation toolkit \citep{koehn-etal-2007-moses}. Namely, we
normalize the punctuation in the data, then we use the tokenizer and a
truecaser. We segment the data using the wordpiece algorithm, creating a
vocabulary of approximately 41k wordpieces for English--German, and 38k
wordpieces fro English--Romanian \citep{wu2016google}.

\paragraph{Models and Training.}
We implement and train our models in the Neural Monkey toolkit
\citep{helcl-libovicky-2017-neural,helcl-etal-2018-neural}. Neural Monkey is a
higher-level deep learning toolkit implemented in TensorFlow
\citep{tensorflow2015-whitepaper}, aiming for fast prototyping using
simple-format configuration files. We train all models for 10 epochs and we
select the best-scoring model based on the validation BLEU score. The training
of the En--De models on a single Nvidia GeForce GTX 1080 GPU took approximately
4 weeks. Since the En--Ro parallel data was much smaller, the training of the
En--Ro models took 4 days.

We summarize the model and training settings in Table
\ref{tab:end-to-end:hparams}. Note that the architecture resembles the
Transformer big settigns, but uses a smaller model dimension. We use the same
settings for training models in both directions in both language pairs.

\begin{table}
  \centering
  \begin{tabular}{lr}
    \toprule
    Parameter & Value \\
    \midrule
    No. of encoder layers & 6 \\
    No. of decoder layers & 6 \\
    Model dimension & 512 \\
    Attention heads & 16 \\
    Dropout probability & 0.1 \\
    Feed-forward hidden size & 4,096 \\
    State splitting factor & 3 \\
    State splitting projection size & 1,536 \\
    Vocabulary size & See Table \ref{tab:end-to-end:data} \\
    \midrule
    Optimizer method & adam \\
    $\beta_1$ & 0.9 \\
    $\beta_2$ & 0.997 \\
    $\epsilon$ & 10\textsuperscript{-9} \\
    Learning rate & 10\textsuperscript{-4} \\
    Fixed batch size & 20 \\
    Gradient clipping & 1 \\
    \bottomrule
  \end{tabular}

  \caption{Experimental settings for the Neural Monkey experiments.}%
  \label{tab:end-to-end:hparams}
\end{table}

\paragraph{Results.} Table \ref{tab:end-to-end:bleu} compares the quantitative
results of our non-autoregressive models with methods proposed by
\citet{gu2017nonautoregressive} and \citet{lee-etal-2018-deterministic}. We use
the SacreBLEU \citep{post-2018-call} to compute the BLEU scores of the model
outputs.

\begin{table}
  \centering
  \begin{tabular}{lcccc}
    \toprule
     & \multicolumn{2}{c}{WMT~16} & \multicolumn{2}{c}{WMT~14} \\
     & En $\rightarrow$ Ro & Ro $\rightarrow$ En & En $\rightarrow$ De & De $\rightarrow$ En \\
    \midrule
    \citet{gu2017nonautoregressive} & & & & \\
    Autoregressive baseline & 31.35 & 31.03 & 22.71 & 26.39 \\
    \addlinespace
    NAT + FT & 27.29 & 29.06 & 17.69 & 21.47 \\
    NAT + FT + NPD (100 s) & 29.79 & 31.44 & 19.17 & 23.20 \\
    \midrule
    \citet{lee-etal-2018-deterministic} & & & & \\
    Autoregressive baseline & 31.93 & 31.55  & 23.77 & 28.15 \\
    \addlinespace
    1 iteration & 24.45 & 25.73 & 13.91 & 16.77 \\
    10 iterations & 29.32 & 30.19 & 21.61 & 25.48 \\
    \midrule
    \citet{libovicky-helcl-2018-end} & & & & \\
    Autoregressive baseline & 21.19 & 29.64 & 22.94 & 28.58 \\
    \addlinespace
    Deep encoder & 17.33 & 22.85 & 12.21 & 12.53 \\
    \quad + weight averaging & 18.47 & 24.68 & 14.65 & 16.72 \\
    \quad + beam search & 18.70 & 25.28 & 15.19 & 17.58 \\
    \addlinespace
    Encoder-decoder  & 18.51 & 22.37 & 13.29 & 17.98 \\
    \quad + weight averaging & 19.54 & 24.67 & 16.56 & 18.64 \\
    \quad + beam search & 19.81 & 25.21 & 17.09 & 18.80  \\
    \addlinespace
    Encoder-decoder with pos. enc. & 18.13 & 22.75 & 12.51 & 11.35 \\
    \quad + weight averaging & 19.31 & 24.21 & 17.37 & 18.07 \\
    \quad + beam search & 19.93 & 24.71 & 17.68 & 19.80 \\
    \bottomrule
  \end{tabular}

  \caption{Automatic evaluation of our \acs{ctc}-based approach, compared to
    the two of the first non-autoregressive methods, along with autoregressive
    greedy-decoding baseline scores. }%
  \label{tab:end-to-end:bleu}
\end{table}

The first model of \citet{gu2017nonautoregressive} represents the \ac{nat}
model with fine-tuning, which minimizes the KL divergence between the output
distributions of the teacher and student models. In the second row, \ac{npd}
with 100 samples is used. \JH{This should be described in Related work.} Using
\ac{npd} can slow down the decoding, because an additional scoring step by an
autoregressive model is needed. In theory, if enough parallelism is available,
this doubles the latency.

The approach of \citet{lee-etal-2018-deterministic} is iterative. We show the
performance of this method with 1 iteration and with 10 iterations. Note that
using more iterations slows down the decoding speed of the iterative model.

When compared to the single-pass models, the performance of the English--German
\acs{ctc}-based models is similar. However, the performance of the
English--Romanian models is poor. This is likely because of issues with
Romanian diacritics in the data, as suggested also by the poorer performance of
our autoregressive baseline. Also, the enhanced techniques (\ac{npd} and
iterative refiniment) outperform our method in all scenarios, but at a
computational cost.

\JH{Description of weight averaging and the beam search.}

The decoding speed for \ac{ar} and {nar} models under different conditions is
shown in Table~\ref{tab:end-to-end:speed}. We report the average time to decode
a single sentence on the \ac{wmt}~15 test set \citep{bojar-etal-2015-findings},
which consists of 2,169 sentence pairs. In the CPU setting, we are using a CPU
machine with the TensorFlow session configured to use 12 CPU threads. The GPU
results are measured on a single Nvidia GeForce GTX 1080 GPU. We show the
average latency as well as the average time to decode a sentence when using
batch decoding.
\JH{Compare speed results? Or leave it for the next chapter.}

\begin{table}
  \centering

  \begin{tabular}{lrr}
    \toprule
     & \mc{CPU} & \mc{GPU} \\
    \midrule
    \acs{ar}, batch=1 & 2,247 ms & 1,129 ms \\
    \acs{ar}, batch=100 & & 127 ms\\
    % AR, beam=?
    \addlinespace
    \acs{nar}, batch=1 & 397 ms & 353 ms  \\
    \acs{nar}, batch=100 &  & 41 ms \\
    \bottomrule
  \end{tabular}

  \caption{The average times to decode one sentence under different conditions.}%
  \label{tab:end-to-end:speed}

\end{table}

Figure \ref{fig:end-to-end:speed} shows the decoding latencies on sentences
from the \ac{wmt}~15 test set \citep{bojar-etal-2015-findings}. As we can see
from the relationship between the source sentence length and the decoding time,
the non-autoregressive model exhibits lower time complexity than the
autoregressive model. The latencies in the plot were measured on a CPU server
using 12 threads. We used greedy decoding with a single sentence in a batch.

\begin{figure}
  \centering
  \input{img/end-to-end-decoding-speed/speed.tex}

  \caption{The relationship between the latency and the number of source tokens
    using an \acs{ar} and an \acs{nar} model}%
  \label{fig:end-to-end:speed}
\end{figure}


\paragraph{Error Analysis.}
We performed manual error analysis of the model outputs. We sampled 100 random
sentences from the English--German test set for each translation direction.
For each sentence in the test set, we first classified whether the sentence was
correctly translated, or whether it was at least comprehensible.

We then identified four types of errors which were prevalent the most in the
outputs: 1) the output sentence was too short, 2) a verb was missing in the
output, 3) a named entity that consists of mutliple subwords was not properly
constructed, and 4) another multi-subword word was corrupted.

The prevalence of the different types of errors we found is shown in Table
\ref{tab:end-to-end:error-analysis}. We observed that when the outputs of the
\ac{ar} model were incorrect, this was further amplified in the \ac{nar} model,
rendering many sentences incomprehensible. Overall, we can see that the number
of completely correct translations in the sample was low for the \ac{nar}
model, less than a quarter of the sentences in both directions. One of the most
common errors in English $\rightarrow$ German direction was the ommission of a
verb at the end of the sentence. This is likely a manifest of the multimodality
problem -- in German, there are two ways of composing the past tense, one of
which inflects the verb in place, and the other puts the verb to the end of the
sentence. In both directions, we noticed many errors connected to words which
are represented with multiple wordpieces, such as rare or infrequent words or
named entities.

Many of the errors are related to the fluency of the output, which might be
caused by the weakened language model in the \ac{nar} models. We address this
issue by incorporating an external language model into the decoding process,
and we describe these experiments in the following section.
% In Section \ref{sec:ctc:fluency}

\begin{table}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    & \multicolumn{2}{c}{En $\rightarrow$ De} & \multicolumn{2}{c}{De $\rightarrow$ En} \\
    & \acs{ar} & \acs{nar} & \acs{ar} & \acs{nar} \\
    \midrule
    Correct        & 65 & 23 & 67 & 13 \\
    Comprehensible & 93 & 71 & 92 & 51 \\
    \midrule
    Too short      & 1 & 16 & 0 & 36 \\
    Missing verb   & 4 & 35 & 0 & 8 \\
    Corrupt. named entity   & 1 & 27 & 8 & 21 \\
    Corrupt. other words & 1 & 20 & 0 & 46 \\
    \bottomrule
  \end{tabular}

  \caption{An error analysis. \JH{todo}}%
  \label{tab:end-to-end:error-analysis}

\end{table}


\JH{is there something to add?}

% -----------------------------------------------------------------------------
\section{Improving Fluency with n-gram Language Models}%
\label{sec:ctc:fluency}
% -----------------------------------------------------------------------------

\paperdisclaim{This section is based on paper \emph{``Improving Fluency of
    Non-Autoregressive Neural Machine Translation''}, joint work with Zdeněk
  Kasner and Jindřich Libovický, published online at \texttt{arXiv.org}.}

\noindent
In this section, we describe our effort to improve the language modeling
capabilities of the \acs{ctc}-based \ac{nat} model. As described in the
previous sections, frequent errors \acl{nar} models make include missing verbs
or bad composition of words from multiple subwords. The literature on \ac{nar}
models also points out that token repetition is also a frequent issue. \JH{but
  not so much in alignment based models which often collapse the repetition
  while still allowing to generate repeated words intentionally (separating
  with blank token)}

\Acp{lm} have been a very successful component in the classical phrase-based
statistical \acs{mt} models \citep{koehn2009statistical}. However in neural
\acs{mt}, language modeling is handeled by the decoder part, which can be
viewed as an \acs{lm} conditioned on the encoder hidden representation of the
source sentence.

By imposing the conditional independence assumption in non-autoregressive
models (Equation \ref{eq:nat-output-distribution}) we impair the implicit
language model, which leads to the lower fluency of the translations.

In this study, we proposed to enhance the beam search algorithm described in
the previous sections \JH{(check ref)} by a number of fluency-oriented
features, which we include in the hypothesis scoring model:
%
\begin{equation}
  \text{score}(y|x) = \log p(y|x) + \sum{i=1}^n w_i \cdot \Phi_i(y)
\end{equation}
%
where $p(y|x)$ is the probability given by the \acs{ctc} model, $\Phi_i(y)$ are
the scoring model features, and $w_i$ are their respective weights. This
rescoring approach is somewhat similar to the \ac{npd} proposed by
\citet{gu2017nonautoregressive}, in which they create more hypotheses by
sampling from the fertility model and rescore them using an \ac{ar} Transformer
model as a scorer.

We experiment with the following three features in the scoring model. First, we
use the score of $y$ from an n-gram language model. We use KenLM, an efficient
implementation of n-gram \acp{lm} in C++
\citep{heafield-2011-kenlm}. Second,
we use the ratio of the blank tokens in the output alignment $y$:
%
\begin{equation}
  \Phi_{\text{blank-ratio}}(y) = \max(0, \frac{B}{T_y - B} - 4 )
\end{equation}
%
where $B$ is the number of blank tokens in the output, $T_y - B$ is the number
of non-blank tokens. Third, we count the number of trailing blank symbols in
the hypothesis:
%
\begin{equation}
  \Phi_{\text{trailing-blanks}}(y) = \max(0, B^T - T_x)
\end{equation}
%
where $B^T$ is the number of blank tokens at the end of the sequence, and $T_x$
is the length of the source sentence.

\subsection{Experiments and Results}


\begin{table}
  \centering
  \begin{tabular}{lr}
    \toprule
    & Data size \\
    \midrule
    Parallel & \\
    En -- De & 4.58 M \\
    En -- Ro & 613 k \\
    \addlinespace
    Monolingual & \\
    En & 20 M \\
    De & 20 M \\
    Ro & 2.2 M \\
    \bottomrule
  \end{tabular}

  \caption{The sizes of the parallel and monolingual data used for training.}%
  \label{tab:ngrams:data}
\end{table}

We follow a similar experimental setup as in the previous section with a few
differences. In addition to the parallel data, we use backtranslated
monolingual data (dataset sizes are shown in Table~\ref{tab:ngrams:data}),
which we mix with upsampled parallel data so that the ratio of the synthetic
and parallel data in the training dataset is approximately 1:1. We train the
\ac{ar} baselines and the \ac{nat} model on this dataset, so that the models
with \ac{lm} scorer have access to the same data.  Instead of the wordpiece
vocabulary, we use Sentencepiece \citep{kudo-richardson-2018-sentencepiece}
with 50k tokens in the vocabulary.  We train a 5-gram language model on the
monolingual data using KenLM.\footnote{\url{https://github.com/kpu/kenlm/}}
We use structured perceptron \citep{huang-etal-2012-structured} to learn the
feature weights in the rescoring model. We train the perceptron on the
development set, using half of the devset for training and the other half for
testing.

Table \ref{tab:ngrams:bleu} shows the \acs{bleu} scores on the test data and
the latencies of the \ac{nar} models when decoding with various beam sizes. We
can see that even for lower beam size settings, the beam rescoring model
achieves considerable improvements over the \ac{nar} baseline. The rescoring
model does affect the decoding speed as the beam size grows, but the
translation improvement gained by beam sizes up to 10 does not cost too much in
terms of speed.

\begin{table}
  \centering

  \begin{tabular}{lccccr}
    \toprule
    & \multicolumn{2}{c}{WMT~16}
    & \multicolumn{2}{c}{WMT~14}
    & \multirow{2}{*}{Latency (ms)} \\
    & En $\rightarrow$ Ro & Ro $\rightarrow$ En
    & En $\rightarrow$ De & De $\rightarrow$ En  & \\
    \midrule
    Baseline & & & & & \\
    \acs{ar}, greedy & 25.89 & 33.54 & 27.29 &  31.06 & 1,664 \\
    \acs{ar}, beam 5 & 26.46 & 34.06 & 27.71 & 31.85 & 3,848 \\
    \acs{nar} without \acs{lm} & 19.88 & 28.99 & 19.55 & 23.04 & 233 \\
    \addlinespace
    \acs{lm} scoring & & & & & \\
    Beam 1 & 19.74 & 29.65 & 20.59 & 24.11 &  337 \\
    Beam 5 & 22.46 & 33.01 & 23.61 & 27.19 & 408  \\
    Beam 10 & 23.33 & 33.29 & 24.27 & 27.83 & 526 \\
    Beam 20 & 24.11 & 33.51 & 24.41 & 28.14 & 1,097 \\
    \bottomrule
  \end{tabular}

  \caption{The model performance (in terms of \acs{bleu} score and latency) for
    the English--Romanian and English--German translation models with \acs{lm}
    scoring with various beam sizes.}%
  \label{tab:ngrams:bleu}
\end{table}

We conduct an ablation study of the features in the rescoring model. The
\acs{bleu} scores of the rescoring model with different sets of rescoring
features (\ac{lm}, blank vs. non-blank symbol ratio, and the number of trailing
blank tokens) for various beam sizes are shown in Table
~\ref{tab:ngrams:features}. In the first row, only the \ac{ctc} score is used,
therefore no rescoring is performed. In the second row, the \ac{lm} score is
added to the model. We see that adding the \ac{lm} alone accounts for most of
the performance gain. In the third row, we add the blank vs. non-blank symbol
ratio to the \ac{ctc} and \ac{lm} scores. The fourth row shows results of the
scoring model with the full set of features.\footnote{Note that we use a
  different test set (\ac{wmt}~15 news test set). For comparison with the other
  models on this test set, we refer the reader to \citet{kasner2020improving}
  or \citet{kasner2020incorporating}.}

\begin{table}
  \centering

  \begin{tabular}{lcccc}
    \toprule
    Beam Size &  1 &  5 &  10 &  20 \\
    \midrule
    \acs{ctc} score & 21.67 & 22.06 & 22.13 & 22.17 \\
    \quad + \acs{lm} & 22.05 & 24.64 & 24.77 & 25.12 \\
    \quad + blank symbol ratio & 22.21 & 24.92 & 25.12 & 25.35 \\
    \quad + trailing blank symbols  & 22.68 & 25.50 & 25.93 & 26.03 \\
    \bottomrule
  \end{tabular}

  \caption{The translation score of the English--German \ac{nat} model when
    using various combinations of features and beam sizes measured on the
    \acs{wmt}~15 test set.}%
  \label{tab:ngrams:features}
\end{table}

Similarly to Figure \ref{fig:end-to-end:speed} in the previous section, we
analyze the relationship between the latencies in models with and without
\ac{lm} rescoring in Figure \ref{fig:ngrams:speed}. We can see that even
though the latency of the \acs{nar}+\acs{lm} system grows with the length of
the source sentence slightly faster than the latency of the pure \acs{nar}
system, it is still much faster than the \ac{ar} counterpart.

\begin{figure}
  \centering
  \input{img/ngrams-decoding-speed/speed.tex}

  \caption{The relationship between the latency and the number of source tokens
    in \acs{ar}, \acs{nar}, and \acs{nar} models with \acs{lm}.}%
  \label{fig:ngrams:speed}
\end{figure}

To illustrate how can the beam rescoring improve the fluency of the outputs of
the \ac{nar} model, we hand-pick a few examples from the German--English test
set and we show them in Table \ref{tab:ngrams:examples}.

\begin{table}
  \centering
  \input{img/ngrams_examples.tex}

  \caption{Hand-picked examples of the model outputs for German--English
    translation.}%
  \label{tab:ngrams:examples}
\end{table}



\section{Limitations}

The proposed approaches and the experimental evaluation have their
limitations. \JH{List all the limitations here: namely, no knowledge
  distillation, the autoregressive baselines are not comparable, etc.}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
