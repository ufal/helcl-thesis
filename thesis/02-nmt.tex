% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Neural Machine Translation}
\label{chap:nmt}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Summary: Jak se procesuje text neuronkama, jak vypadají základní architektury
% pro překlad - RNNs, Transformer, jak se to trénuje, jak se generuje a jak se
% to vyhodnocuje

% zpracování: slovník, segmentace, batche
% RNN
% Trafo

\gls{nmt} is the current state-of-the-art approach to \gls{mt}. As the name
suggests, the underlying machine learning concept in \gls{nmt} systems are
neural networks. The specific types, or \emph{architectures}, of neural
networks used in an \gls{nmt} system vary across history, applications, or
domains, but all share a few common traits.

Neural networks are a mathematical model that works with real numbers: The
inputs and the outputs to a neural network are real-valued vectors, the
training objective is a differentiable function, and the space of the
parameters needs to have a continuous gradient with respect to the loss
function. Specific to neural networks is the computation of the gradient by the
\emph{backpropagation} algorithm \JH{ocitovat}, and the large portfolio of
optimization methods.

In Section \ref{sec:text-processing}, we explain how to adapt neural networks
to the discrete nature of the textual data. Perhaps the most common way is to
assign a real-valued vector to each word in the vocabulary. These
representations are usually trained along with the whole network, end to end.

Textual data are sequential and the lengths of the sequences are
variable. Using feed-forward neural networks is therefore not
suitable. Sequences can be processed by a number of network architectures,
namely \glspl{cnn}, \glspl{rnn}, or \glspl{san}. All of those can be adapted
for text processing, and we will describe the latter two in more detail in
Sections \ref{sec:encdec:rnn} and \ref{sec:transformer}, respectively.

In \acrshort{nlp} tasks like sentence classification or language modeling,
there is only a single sequence on the input. In \acrshort{mt}, we have a
source sentence on the input, and a target sentence on the output. To reflect
this property, most of the \gls{nmt} architectures have two components: an
\emph{encoder} and a \emph{decoder}. The encoder processes the input sentence
and creates a hidden representation. The decoder then accesses this hidden
representation and generates (or scores) the output sentence. When both
components of the model are trained end to end, this framework is sometimes
referred to as \emph{sequence-to-sequence learning}. \JH{We explore the
  encoder-decoder architectures more closely in Sections \ref{sec:encdec},
  \ref{sec:attention}, and \ref{sec:deep-arch}.}


%Section about training: batching, optimization, multiple GPUs, ...
A \gls{nmt} architecture is usually trained with \emph{teacher forcing} -- for
a given source sentence and its translation, the neural network is trained to
maximize the likelihood of $i$-th target word on $i$-th position, conditioned
on the $i-1$ previous words of the reference sentence. During decoding, when
this information is not available, the model is conditioned on previously
decoded outputs instead. More details on this phenomenon and other practical
considerations for training a model are presented in Section
\ref{sec:training}. \JH{maybe this is already decoding?}

% alternative paragraph for Training section overview. Focus on batching and
% optimization instead of teacher forcing.





\Gls{mt} is evaluated by automatic metrics such as \acrshort{bleu} and by human
evaluation. This chapter concludes with a brief overview of the evaluation of
\gls{nmt} systems (Section \ref{sec:evaluation}).



% \JH{přepsat aby to odpovídalo} This chapter provides theoretical details about
% \gls{nmt} systems in general, as well as particular features of the systems
% that are relevant to experiments described in this thesis later on. The
% contents of this chapter are structured as follows. Section
% \ref{sec:text-processing} analyzes the approaches to processing (sequential)
% textual data by neural networks. Further on, Sections \ref{sec:rnn} and
% \ref{sec:transformer} describe two of the most popular neural network
% architectures used for \gls{nmt}, \glspl{rnn} and the Transformer model.  In
% Section \ref{sec:training-vs-inference}, we explain the different modes of
% operation of \gls{nmt} models for training and inference. This chapter
% concludes with the overview of the \gls{nmt} evaluation methods (Section
% \ref{sec:evaluation}).

% ------------------------------------------------------------------------------

\section{Processing Text with Neural Networks}
\label{sec:text-processing}

% Text are not numbers, NNs work with numbers
There are a number of problems that arise when we want to use neural networks
for text processing. Foremost, neural networks are a mathematical model that
deals with real numbers. In contrast, language is discreet and has a complex
structure. Thus, we need to find ways of expressing textual data
numerically. Second, since language units (such as words, sentences, or
documents) come in various lengths, we need to use neural network models
designed for processing sequential data.

% One-hot representation are straightforward.
In this section, we begin with the representation of words. The most
straightforward, and also the most common way to represent words in neural
networks is by having a list (a \emph{vocabulary}) of words $V$, where each
word has a corresponding integer index $j$ pointing at it. Each word $w$ at
index $j$ in this list can be represented by a vector
$x \in \{0,1\}^N$ where
%
\begin{equation} x_i =
\begin{cases} 1, & \text{if } i = j \\ 0, & \text{otherwise.}
\end{cases}
\end{equation}
%
We call this vector the \emph{one-hot representation} of $w$.

% One-hot representation do not capture word roles in language.
Using one-hot representation help us convert discreet words to numbers, but it
does not capture anything about the underlying linguistic structure.  For this
purpose, \emph{learned distributed feature vectors}, or \emph{word embeddings}
are used \citep{bengio2003neural} \JH{pridat vic citaček}. The idea is to
associate a real-valued vector with each word, so that representations of words
with similar roles in language are closer together in the assigned vector
space. \citet{mikolov2013distributed} train these word embeddings using the
\emph{skip-gram} and \emph{continuous bag of words} objectives and find that
the resulting embeddings have interesting properties. For example, arithmetic
operations on embeddings can express word analogy.  In some \gls{nlp} tasks,
pre-trained embeddings can be used to greatly improve the model performance on
the end task. In contemporary \gls{nmt} research, the most prevalent approach
is to train the word embeddings end-to-end along with the translation model.

% Word embedding layer is inserted to help.
We provide the model with the distributed representation of words using an
embedding layer as follows. For a given one-hot vector $x$, and a
parameter matrix $E$ called the \emph{embedding matrix}, we retrieve a single
row that corresponds to the vocabulary index of $x$ by the
multiplication $E x$.
% %
% \begin{equation}
%   e(\mathbf{w}) = E\mathbf{w}.
% \end{equation}
% %
The embedding dimension is usually much lower than the size of the vocabulary
(in \gls{nmt}, the embedding dimension in state-of-the-art models is around
1,024).

% There are out-of-vocabulary tokens
A major drawback of having a fixed-size vocabulary is the fact that there will
be unseen words in the data. The following section lists methods that solve
this problem.


% výstup
% Once the input text is converted into real-valued numeric data and the neural
% network computation is executed, we need to interpret the results

% \JH{přidat text o output distributions:}
% %
% \begin{equation}
%   p(y|x) = \prod_{t=1}^{T_y}p(y_t|y_{<t},x,\theta)
%   \label{eq:output-distribution}
% \end{equation}
% %
% where $T_y \in \mathbb{R}$ is the length of the target sentence, $y_{<t}$ are
% the previously decoded words, and $\theta$ is a set of the model parameters.

\subsection{Open Vocabulary Problem}

% Zipf nature of language
A well known characteristic of language is that it follows the Zipf's law
\citep{zipf1949human}. As a result, in a large enough sample of text, there is
a huge amount of unique words or words occurring with low frequency. Such words
constitute a substantial part of the data and cannot be ignored.  Another issue
is that our training datasets do not contain all words that can occur in the
language. These \gls{oov} words usually also make up significant portions of
the validation and/or test data.

% Open vocabulary problem
The problem of rare and unseen words is also referred to as the \emph{open
vocabulary} problem. Although some work in this direction exist
\citep{jean2015using}, in neural networks for \gls{nlp}, it is difficult to use
very large vocabularies. The main reason is that the embedding layer and the
output projection layer would become too large for the computation to be
efficient.

% Most common solution
The most common solution of the open vocabulary problem is to use smaller units
than words \citep{sennrich2016neural}. These \emph{subword} units can be
constructed in a smart way, such that frequent words are left intact, whereas
rare words are composed of more common strings of letters. Below, we describe
two of the methods for subword segmentation. It is worth to mention that
research on character-level methods is still ongoing, but as of the writing of
this thesis, this approach did not yet outperform the subword state-of-the-art
\citep{chung-etal-2016-character,lee-etal-2017-fully,gao-etal-2020-character}.
%
%
%\begin{algorithm} %\centering
%
%\begin{algorithmic}
%
%\Function{InitializeVocab}{}
%
%\State $V \gets \emptyset$ %\ForAll{$x \in $\text{ training data tokens}}
%\State $w \gets $ \text{ characters in} $x$ \text{plus special end-of-word
%symbol} $\cdot$
%
%%\If{$w \in V$} %\State $V[w] \gets V[w] + 1$ %%\Else %%\State $V[w] \gets 0$
%%\EndIf %\EndFor %\State \Return{$V$} %\EndFunction
%
%\\
%
%\Function{GetCounts}{$V$}
%
%\State $P \gets \emptyset$
%
%\ForAll{$(w, c) \in V$} %\For{$i = 1 \ldots |w|$}
%
%\State $P[w_i, w_{i+1}] \gets P[w_i, w_{i+1}] + c$
%
%\EndFor %\EndFor
%
%
%\State \Return{$P$} %\EndFunction
%
%\\
%
%\Function{MergeVocab}{$V, s_1, s_2$}
%
%\State \Return{$V$} %\EndFunction
%
%\\
%
%\State $V \gets $ \Call{InitializeVocab}{}
%
%\For{$i = 1 \ldots M$} %\State $P \gets $ \Call{GetCounts}{$V$} %\State $(s_1,
%s_2) \gets \argmax_{(s_i, s_{i+1} \in P)} P[s_i, s_{i+1}]$ %\State $V \gets $
%\Call{MergeVocab}{$V, s_1, s_2$}
%
%\EndFor
%
%
%\end{algorithmic}
%
%\caption{Learning BPE merges.}
%\label{alg:learn-bpe}
%\end{algorithm}


\paragraph{\acrshort{bpe}.}  \Acrlong{bpe} (\acrshort{bpe};
\citealp{sennrich2016neural}) is an approach which tackles the open vocabulary
problem by splitting words to subword units.  The idea is to devise a
vocabulary of a predefined size, such that nearly every word can be composed
using the units from the vocabulary. An additional requirement is for the
vocabulary to contain as many frequent words as possible, so only rare words
need to be split to more subwords.

The \acrshort{bpe} algorithm %(Algorithm ~\ref{alg:learn-bpe})
proceeds as follows. First, the vocabulary is initialized with all characters
as symbols. Second, all tokens in the data are segmented using the symbols in
the vocabulary (in the first step, the tokens are split to characters).  Third,
the algorithm computes the counts of pairs of consecutive symbols in the
training data, and selects the most frequent pair. Next, the most frequent pair
of symbols is merged into a new symbol which is added to the vocabulary. Then
the algorithm iterates the segmentation, counting and merging, until a
predefined number of merges is done. In practice, instead of working with the
training corpus, the algorithm runs on the frequency list of tokens, without
loss of generality.

\paragraph{SentencePiece.}  \JH{copied from JNLE:} SentencePiece
\citep{kudo2018sentencepiece} is a toolkit that implements BPE and a unigram
language model for subword segmentation \citep{kudo-2018-subword}. It supports a
number of features, such as sampling and regularization by introducing noise on
the source side. As opposed to BPEs and wordpieces, Sentencepiece does not
require prior tokenization of the input text, and unlike other methods its
pre-tokenization allows to fully reconstruct the original string.

% ------------------------------------------------------------------------------
\section{Encoder-Decoder Framework}
\label{sec:encdec}
% ------------------------------------------------------------------------------

The contemporary \gls{nmt} models share a common framework where each model is
composed of two parts -- an \emph{encoder}, and a \emph{decoder}. The encoder
reads in the input sentence and processes it in order to generate an
intermediate hidden representation.  The decoder then uses this intermediate
hidden representation to produce the probability distributions over the output
tokens.

The early \gls{nmt} models based on \glspl{rnn} use the final encoder state as
the intermediate representation \citep{sutskever2014sequence}:
%
\begin{eqnarray} h_j & = & \mathrm{RNN}_{\text{enc}}(x_j, h_{j-1}), \quad j \in
\{0, 1, \ldots, T_x \} \\ s_0 & = & h_{T_x}
\end{eqnarray}
%
where $\mathbf{x}$ is the input sentence and $h_0$ is the initial hidden state,
usually set to $\mathbf{0}$. $T_x$ denotes the length of the input sentence.
Note that the input words are represented as their embeddings.
\citet{sutskever2014sequence} do not use subword segmentation and instead
reserve a special OOV token for unseen words.

The decoder is initialized with the state $s_0$ and runs the second \gls{rnn}:
\begin{equation} s_i = \mathrm{RNN}_{\text{dec}}(y_{i-1}, s_{i-1})
\end{equation}
%
where $y_{i-1}$ is the preceding word in the reference output sentence (during
training), with $s_0$ being a special symbol which expresses the start of a
sequence (denoted \texttt{<s>}).

While recurrent neural networks are not used as the underlying model in the most
of the current research anymore, the encoder-decoder framework is independent of
the actual neural network structure and the concept is still used as a main
approach for designing sequence-to-sequence models.

In this section, we introduce the two most notable encoder-decoder
architectures. The first is based on \glspl{rnn} and became the first neural
architecture to outperform statistical \gls{mt} models. The second
architecture, called \emph{Transformer}, is based on self-attentive networks
and is the best-performing architecture for many \gls{nlp} tasks today.


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Recurrent Neural Networks}
\label{sec:encdec:rnn}
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

The invention of \glspl{rnn} \citep{elman1990finding} allowed processing of
sequential data by neural networks. \Glspl{rnn} process the sequence one item at
a time, chaining consecutive steps with recurrence connections.

In the early stages of the \gls{rnn} development, a single hidden layer of the
network was altered to take into account the output of itself from the previous
time step:

%\begin{equation} % h_t = f(x_t, h_{t-1})
%\end{equation}
%
%\noindent %where $f$ is usually a non-linear projection:

\begin{equation} h_t = \tanh ( W x_t + U h_{t-1} + b_h ) \label{eq:vanilla-rnn}
\end{equation}

\noindent where $W \in \mathbb{R}^{m \times n}$, $U \in \mathbb{R}^{n \times
n}$, $b_h \in \mathbb{R}^{n}$ are trainable parameters, $x_t \in \mathbb{R}^{m}$
is the \gls{rnn} input, and $h_{t-1} \in \mathbb{R}^{n}$ is the previous hidden
state.

This version of \glspl{rnn} however suffers from the \emph{vanishing gradient
problem}. During backpropagation, the gradients are multiplied with the
derivative of the hyperbolic tangent, which is always less or equal to one. In
long sequences, the learning signal over distant parts of the sequence has
therefore almost no effect on the training. Due to this fact, the network
manifests poor performance in handling long-range dependencies.

There has been several approaches to combat the vanishing gradient problem.  The
most prevailing types of \gls{rnn} architectures in \gls{nmt} are \gls{lstm}
networks and \gls{gru} networks.

\paragraph{\acrshort{lstm}.} \acrlong{lstm} networks \citep{hochreiter1997long}
introduce gating mechanisms and a concept of \emph{information highway}, which
ensures that only linear operations are applied on the states in the recurrent
chain. A gating mechanism is an operation which computes a number between 0 and
1, which we refer to as \emph{gate value}.  The output of the operation is the
input multiplied by the gate value.

Given a current time step $t$, input $x_t \in \mathbb{R}^m$, and previous hidden
states $h_{t-1}, C_{t-1} \in \mathbb{R}^n$, \gls{lstm} networks proceed as
follows:
%
\begin{eqnarray} f_t & = & \sigma\left(W_f x_t + U_f h_{t-1} + b_f\right)
\label{eq:lstm-forget-gate}\\ i_t & = & \sigma\left(W_i x_t + U_i h_{t-1} +
b_i\right)
\label{eq:lstm-input-gate}\\ o_t & = & \sigma\left(W_o x_t + U_o h_{t-1} +
b_o\right)
\label{eq:lstm-output-gate}\\ \tilde{C}_t & = & \tanh \left( W_c x_t + U_c
h_{t-1} + b_c \right)\label{eq:lstm-candidate} \\ C_t & = & f_t \odot C_{t-1} +
i_t \odot \tilde{C}_t
\label{eq:lstm-information-highway}\\ h_t & = & o_t \odot \tanh
C_t \label{eq:lstm-hidden-state}
\end{eqnarray}
%
where $W_f, W_i, W_o, W_c \in \mathbb{R}^{m \times n}, U_f, U_i, U_o, U_c \in
\mathbb{R}^{n \times n}, b_f, b_i, b_o, b_c \in \mathbb{R}^n$ are trainable
parameters, $\sigma$ is the logistic function, and $\odot$ represents
element-wise multiplication. The states $h_t$ and $C_t$ are also called public
and private hidden states respectively. The intermediate value $\tilde{C}_t$ is
called the candidate state.

The three gates in the \gls{lstm} network are called \emph{forget gate},
\emph{input gate}, and \emph{output gate}. The forget gate (Eq.
\ref{eq:lstm-forget-gate}) controls how much of the information from the
previous private hidden state $C_{t-1}$ to the current private state (Eq.
\ref{eq:lstm-information-highway}). The input gate (Eq.
\ref{eq:lstm-input-gate}) controls the amount of information received from the
candidate state. Finally, the output gate (Eq. \ref{eq:lstm-output-gate})
decides which portion of the currently computed private hidden state is
transferred to the current public hidden state
(Eq. \ref{eq:lstm-output-gate}). Note that the values of the gates are computed
in the same way, but with different parameters.

The original recurrence relation from Equation \ref{eq:vanilla-rnn} is expressed
by Equation \ref{eq:lstm-candidate}, where the new candidate state is
computed. The transfer of information from the previous private state and the
current candidate state is done in Equation
\ref{eq:lstm-information-highway}. Note that with respect to the states from the
preceding steps, the previous state is only multiplied by a constant. This
constitutes the information highway that allows propagating the gradients over
long distances in the sequence.




\paragraph{\acrshort{gru}.} \acrlong{gru} networks \citep{cho2014gru} are an
alternative to \glspl{lstm}. Instead of four sets of parameter matrices,
\glspl{gru} need only three, while maintaining the theoretical strength. Unlike
\glspl{lstm}, \glspl{gru} use only a single hidden state in the recurrence
relations.

Given the time step $t$, input $x_t \in \mathbb{R}^m$, and previous hidden state
$h_{t-1}$, one \gls{gru} step is defined as follows:
%
\begin{eqnarray} r_t & = & \sigma\left(W_r x_t + U_r h_{t-1} + b_r\right)
\label{eq:gru-reset-gate} \\ z_t & = & \sigma\left(W_z x_t + U_z h_{t-1} +
b_z\right)
\label{eq:gru-update-gate} \\ \tilde{h}_t & = & \tanh \left(W x_t + U \left( r_t
\odot h_{t-1} \right) + b \right) \label{eq:gru-candidate} \\ h_t & = & (1 -
z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
\label{eq:gru-hidden-state}
\end{eqnarray}
%
where $W, W_z, W_r \in \mathbb{R}^{m\times n}, U, U_z, U_r \in \mathbb{R}^{n
\times n}, b, b_z, b_r \in \mathbb{R}^n$ are trainable parameters.

The two gates in \gls{gru} networks are called \emph{reset gate} and
\emph{update gate}.  The reset gate (Eq. \ref{eq:gru-reset-gate}) determines how
much of the information from the previous state is preserved and is applied in
the recurrence relation for computing the candidate
(Eq. \ref{eq:gru-candidate}). The update gate (Eq.  \ref{eq:gru-update-gate})
controls the merging of the previous state with the candidate state in Equation
\ref{eq:gru-hidden-state}. Note that the information highway concept is
expressed by this equation.

\paragraph{Bidirectional RNNs.} There is a bidirectional variant of \glspl{rnn}
which can be applied to any of the flavors of \glspl{rnn} discussed above. When
the whole input sequence is known, we can apply a \gls{rnn} in both directions
separately and then concatenate the states from the corresponding positions:
%
\begin{eqnarray} \overrightarrow{h}_t & = &
\overrightarrow{\mathrm{RNN}}(x_{t-1}, \overrightarrow{h}_{t-1}) \\
\overleftarrow{h}_t & = & \overleftarrow{\mathrm{RNN}}(x_{t+1},
\overleftarrow{h}_{t+1}) \\ h_t & = & \left[ \begin{matrix} \overrightarrow{h}_t
\\ \overleftarrow{h}_t
\end{matrix} \right]
\end{eqnarray}


\paragraph{Deep RNNs.} Some NMT architectures based on RNNs use multiple
recurrent layers \citep{barone2017deep,wu2016google}. In deep RNNs, each layer
is a recurrent network. The inputs of the second and each layer onwards are the
outputs of the preceding layer. The output of the whole deep RNN is the output
of the last layer.

In bidirectional RNNs, there are a few settings to consider. \JH{tell me more}

To improve the convergence speed of the deep models, \emph{layer normalization}
\citep{ba2016layer} is often employed between layers. For layer $l$ and its
output states
%$h_1^l, \ldots h_{T_x}^l$, $h_i^l \in \mathbb{R}^n$,
$\mathbf{h}^l \in \mathbb{R}^{T_x \times n}$, we compute:
%
\begin{eqnarray}
  \mu^l & = & \frac{1}{nT_x} \sum_{i=1}^{T_x}\sum_{j=1}^n h^l_{ij} \\
  \sigma^l & = & \sqrt{\frac{1}{nT_x} \sum_{i=1}^{T_x}\sum_{j=1}^n (h^l_{ij} - \mu^l)^2} \\
  \bar{\mathbf{h}}^l & = & \frac{\mathbf{h}^l - \mu^l}{\sigma^l} \odot \gamma^l + \beta^l
\end{eqnarray}
%
where $\mu^l$ and $\sigma^l$ are the mean and standard deviation of the output
state values, $\gamma^l, \beta^l \in \mathbb{R}^{T_x \times n}$ are learnable
\emph{gain} and \emph{bias} parameters, and $\odot$ denotes element-wise
multiplication. The normalized states $\bar{\mathbf{h}}^l$ are then used as the
input to the $l+1$-th layer of the network instead.


% ------------------------------------------------------------------------------
\subsection{Attention Mechanism}
\label{sec:attention}
% ------------------------------------------------------------------------------
\JH{demote this to paragraph, move above deep rnns.}

Another important concept introduced as a part of \gls{rnn}-based \gls{nmt}
models is the \emph{attention mechanism}
\citep{bahdanau2014neural,luong2015effective}.

The problem with the encoder-decoder framework as described in the previous
section is that there is an information bottleneck between the encoder and the
decoder. All the information from the source sentence needs to be compressed
into a single hidden state vector $s_0$.

The attention mechanism enables the decoder to access the information stored in
the encoder hidden states rather than relying only on the value of the initial
state.  Formally, based on a current decoder step $s_i$, the attention mechanism
computes a local \emph{context vector} as a weighted average of the encoder
states:
%
\begin{eqnarray}
  % attn energies
  e_{ij} & = & v_a^\top \tanh (W_a s_{i-1} + U_a h_j + b_a) + b', \label{eq:attn-energies} \\
  % attn distro
  \alpha_{ij} & = & \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x}\exp(e_{ik})}, \\
  %
  % \softmax_{j \in 1, 2, \ldots, T_x } & = & e_{ij} \\
  % context vector
  c_i & = & \sum_{j=1}^{T_x} \alpha_{ij} h_j.
\end{eqnarray}
%
In the first step, the \emph{attention energies} are computed for every encoder
hidden state using a single-hidden-layer feed-forward network parameterized by
$W_a \in \mathbb{R}^{n' \times n}$, $U_a \in \mathbb{R}^{n' \times 2n}$,
$b_a \in \mathbb{R}^{n'}$, $v_a \in \mathbb{R}^{n'}$, and $b' \in \mathbb{R}$
where $n'$ is the dimension of the hidden layer. Next, the attention energies
are normalized using the softmax function into the \emph{attention
  distribution}.  Finally, the context vector is computed as a weighted sum of
the encoder states $h_0,\ldots, h_{T_x}$, using the attention distribution
values as weights.

The concept of attention can be viewed as a soft-lookup function over an
associative memory. Given a \emph{query}, which is the decoder state in a given
time step, we use a similarity metric over a set of \emph{keys}. We then
normalize the similarities and use them as weights in the weighted sum of
\emph{values} associated with the corresponding keys. In this case, the sets of
attention keys and values are equal -- the encoder hidden states $h_j$. The
similarity metric is defined in Equation \ref{eq:attn-energies}.


\subsection{Deep Architectures}
\label{sec:deep-arch}
\JH{deep architectures, cite sebe a GNMT}
\JH{make this a paragraph}


\subsection{Transformer Model}
\label{sec:encdec:transformer}

One of the disadvantages of the RNN-based models is the linear time nature of
the recurrence relation. Few models have been proposed to remove the recurrence
and thus allowing for simultaneous computation across time steps, at least at
the training time. Most notably, these models include convolutional
architectures \citep{gehring2017convolutional}, and the current state-of-the-art
architecture, the \emph{Transformer} model \citep{vaswani2017attention}.


\section{Training}
\label{sec:training}



%
\begin{equation}
  J_{\theta} = - \sum_{(x, y) \in D} \log p(y | x, \theta)
\end{equation}
%

Since the probability distributions are conditioned on the previously decoded
tokens, the likelihood of the target sentence is reformulated using the chain
rule:
%
\begin{equation}
  J_{\theta} = - \sum_{(x, y) \in D} \sum_{i = 1}^{T_y} \log p(y_i | x, y_{<i}, \theta)
\end{equation}
%
where $T_y$ is the length of the target sentence.

Note that in the equation above, the probability distributions are conditioned
on the reference target prefix, rather than the model outputs. This technique
is known as \emph{teacher forcing}, and is essential for the training
convergence. When the model is exposed to its own outputs from the start of the
training, it will very likely fail to converge.

Teacher forcing, however, brings along a problem called \emph{exposure bias} --
the model is never exposed to its own errors, which makes it less robust
against them.

Methods have been proposed to address this issue, including a curriculum
learning approach which gradually replaces teacher forcing with the model
predictions \citep{scheduled}. Other methods focus on sequence-level training
or beam search optimization methods, mostly based on reinforcement learning
\citep{williams, rush-bso, mixer}. However, none of these methods was widely
adopted, and current models are believed to be capable of recover from their
errors despite being explicitly trained to do so.





\section{Autoregressive Decoding}
\label{sec:training-vs-inference}

The models we described in the sections above are \emph{autoregressive} -- the
output tokens are predicted left-to-right, while every decision is conditioned
on the previously generated outputs. With this property, there comes an
important distinction in behavior between training and decoding. Whereas during
training, the ground-truth data are used to simulate the previous decisions,
during decoding, the ground truth is unknown and therefore the model need to
rely solely on its own decisions. This constitutes a theoretical problem,
called \emph{exposure bias} -- the model is never exposed to its own errors
during training.

In the RNN-based models, the difference between training and decoding is
minimal. The model execution is done the same way, with the one exception of
providing ground-truth data during training. Another exception is that the final
softmax does not need to be computed during greedy decoding (there is no need
for normalized distribution if we are interested only in the token with maximum
probability), but is still needed for beam search.

The Transformer models are quite different in this aspect. Since there is no
recurrence operation which requires accumulation of information in a hidden
state, the network can be trained on a whole ground-truth sentence in one step.
The only requirement is to prevent the decoder self-attention from attending to
the future positions. However, during training, the model is still conditioned
on its own decisions.




\section{Evaluation}
\label{sec:evaluation}

The problem of \gls{mt} evaluation is almost as challenging as \gls{mt} itself.
The most reliable method for assessing the quality of \gls{mt} systems remains
human evaluation.  Since the adoption of statistical approach for \gls{mt}
\citep{brown-etal-1993-mathematics,koehn-etal-2003-statistical}, the demand for
automatic translation quality metrics grew larger, as the models often need to
be validated several times during training.

Perhaps the best-known automatic \gls{mt} metric is BLEU
\citep{papineni2002bleu}.  Despite a long term effort led by the organizers of
the WMT Metrics shared task to create an automatic evaluation metric that would
be better correlated with scores assigned by humans, BLEU continues to be the
most widely used metric in the contemporary literature. However, over the nearly
two decades of using BLEU, it has been criticized for being prone to errors due
to outliers or being too inaccurate when the score itself is low.
\citep{callison-burch-etal-2006-evaluating,bojar-etal-2010-tackling,reiter2018structured,mathur-etal-2020-tangled}

\JH{evaluation of decoding speed -- latency, throughput}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
