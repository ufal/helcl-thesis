% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Non-Autoregressive NMT}
\label{chap:nat}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The efficiency of \gls{mt} models is often crucial in real-world applications.
Most commercial \gls{nmt} models are available through a cloud-based service,
such as Microsoft Translator\footnote{\url{https://microsoft.com/translator}}
or Google Translate\footnote{\url{https://translate.google.com}}. Scaling
cloud-based solutions for large user bases is simple but costly. But even with
a large pool of computational resources, it is worthwhile to implement some
optimizations which decrease the model latency and improve the user experience.

% lokální modely maj tu výhodu, že to běží u mně, takže to jde pouyžít offline
% a taky se nemusím bát o svoje soukromý data. ale jsou pomalý.

Locally-deployed \gls{nmt} models provide a number of advantages over
cloud-based solutions. First, the models can be used in places without internet
connection. Second, the data are not being sent to a 3rd party server, and
therefore it is suitable for translating private or confidential data.
However, without optimization, running state-of-the-art translation models
locally often requires specialized hardware, such as one or more GPU
cards. Otherwise, the time for translating a single sentence can easily reach
more than a second on a standard CPU.

Higher decoding speeds can be achieved by model optimization. In their 2019
submission to the \gls{wngt} shared task on efficiency,
\citet{kim-etal-2019-research} successfully employed knowledge distillation,
quantization, shortlisting \citep{jean2015using} and a simpler recurrent unit
design to bring the throughput of the translation model up to 3,600 words per
second on a CPU, for a modest drop in the BLEU score. Following this
submission, \citet{bogoychev-etal-2020-edinburghs} reported further
improvements with attention head pruning. The work has been a part of the
Bergamot research project, which aims to bring offline translation models into
a browser (\JH{cite}).

% vedle toho tu jsou neautoregresivní modely který se na to snaží jít jinak

An alternative approach to model optimization is using a different (and faster)
model and its decoding algorithm.



\Gls{nat} is a recent subtask of \gls{nmt} in which the decoding process cannot
access the previously decoded outputs, imposing conditional independence on the
output token probability distributions. This assumption allows parallelization
of the decoding which can significantly reduce the latency of the translation
system. On the other hand, it also presents a challenge to the language model,
which usually leads to poorer translation quality.

This chapter begins with an overview of the non-autoregressive methods (Section
\ref{sec:nat-methods}). In Section \ref{sec:nat-ctc}, we introduce
non-autoregressive \gls{nmt} with \gls{ctc}. \JH{rewrite following commented:}
% We present our experiments with \gls{ctc} in Section
% \ref{sec:ctc-experiments}. We summarize our contributions and outline the
% possible future efforts in Section \ref{sec:nat-future}.


\section{Related Work}
\label{sec:nat-methods}

This section lays grounds for understanding the key concepts in
non-autoregressive \gls{nmt}. We first describe the principles that most of the
related literature has in common. Then, we provide a survey of notable
approaches to \gls{nat}.

\paragraph{Conditional Independence.} The defining feature of a
non-autoregressive model is the conditional independence of the output
distributions across time steps. Recall Equation \ref{eq:output-distribution}
which defines the output distribution for autoregressive models:
%
\begin{equation}
  p(y|x) = \prod_{t=1}^{T_y}p(y_t|y_{<t},x,\theta).
  \tag{\ref{eq:output-distribution}}
\end{equation}
%
Unlike Equation \ref{eq:output-distribution}, \Gls{nat} models do not condition
the output token probabilities on the previously decoded outputs $y_{<t}$.  The
probability of an output sentence $y$ given an input sequence $x$ can then be
modeled as:
%
\begin{equation}
  p(y|x) = \prod_{t=1}^{T_y}p(y_t|x,\theta).
  \label{eq:nat-output-distribution}
\end{equation}
%
Note that without any constraints, $p(y|x)$ might not be a probability
distribution. In the equations above, $y_{T_y}$ must represent the
end-of-sentence (\texttt{</s>}) token, with none of the preceding tokens
$y_t, t < T_y$ being \texttt{</s>}, too. This requirement can be expressed as an
explicit estimation of the target sentence length, leaving out the technical
\texttt{</s>} symbol from the mathematical model:
\begin{equation}
  p(y|x) = p_L(T_y|x, \theta) \cdot \prod_{t=1}^{T_y}p(y_t|x,\theta).
  \label{eq:explicit-length}
\end{equation}

\paragraph{Target Length Estimation.} As mentioned in the above paragraph, in a
standard NMT model such as the Transformer or an \acrshort{rnn}-based model with
attention, the length of the output sentence is modeled implicitly by using the
special end-of-sentence (\texttt{</s>}) token. \JH{Not sure if this fits the
  narrative.}

When using Equation \ref{eq:output-distribution} to model probability
distribution over the set of all possible sentences, the probabilities assigned
to longer sentences are considerably lower than the probabilities of sentences
which are short. This follows from the multiplication of probabilities, which
are numbers between zero and one. \JH{další věc je že je ten search space
  exponenciálně roste -- This follows from the fact that the search space
  (i.e. the number of all possible sentences of length $T_y$) grows
  exponentially with $T_y$.}  \JH{odkazat spis na argmax nez na output
  distribution.} To diminish the negative influence of this property,
\citet{wu2016google} introduce length normalization to the beam search algorithm
(\JH{odkazat na eq. v intro do MT}). The normalization acts as a prior imposed
on the target sentence length distribution.

It is, however, possible to estimate the target length explicitly.
\JH{...}

\citep{ghazvininejad2019mask} \citep{mansimov2019generalized}


% ------------------------------------------------------------------------------

\paragraph{Non-Autoregressive Neural Machine Translation
  \citep{gu2017nonautoregressive}.} The first to propose non-autoregressive
models applied to \gls{nmt}.

Work with Equation \ref{eq:explicit-length} which brings the conditional
independence and the explicit length estimation. Describe the
\emph{multimodality problem}, which comes from poor capturing of multimodal
distributions of the translations.

Compared to the autoregressive Transformer model, the model has the following
modifications. First, the inputs to the decoder are made up of the sequence of
encoder inputs, either uniformly stretched to the predicted target sentence
length, or copyied using a fertility model. Second, the decoder self-attention
does not use the causal mask since all states can now attend to all other states
in both directions. Third, \emph{positional attention} is added to every decoder
layer where the positional encoding is used as queries and keys, and the decoder
states as values.

The multimodal problem is addressed by introducing a latent variable $z$ sampled
from a prior distribution, and modeling the probability conditionally on $z$:
\begin{equation}
  p(Y|X) = p_L(T_Y|X, \theta) \cdot \prod_{t=1}^{T_Y}p(y_t|X, z, \theta).
\end{equation}
\JH{Sjednotit maly a velky X a Y.}

The explicit length estimation model is replaced with fertility model,
fertilities modeled with the latent variables.

\textbf{Decoding} -- marginalizing over all possible fertilities is intractable,
there are three methods to consider: Argmax decoding, average decoding, or
\gls{npd}.

\textbf{Training} -- Done with translation loss and fertility loss:
\begin{align}
  \mathcal{L}(\theta)  & = \log p(Y|X, \theta) \\
                       & = log \sum_{F \in \mathcal{F}} p(F| X, \theta ) p(Y | X, F, \theta) \\
                       & \geq \mathbb{E}_{\bar{F} \sim q}
                         \left(
                         \sum_{t=1}^{T_Y} \log p(y_t | X, \bar{F}, \theta)
                         + \sum_{t=1}^{T_X} \log p(f_t | X, \theta)
                         \right)
                         + \mathcal{H}(q)
\end{align}
\JH{what is $\mathcal{H}(q)$?}

The fertility model depends on an external module and is not trained end to
end. The authors fine-tune the trained model using reinforcement learning.

% ------------------------------------------------------------------------------

\paragraph{Iterative refinement \citep{lee2018deterministic}.} One of the first
papers on \gls{nat}. Introduce $L$ discrete sequential latent variables
interpreted as stages of refinement, which are marginalized out.
\begin{align*}
  p(Y|X) & = \sum_{Y^L}
           \left( \prod_{t=1}^{T_Y} p(y_t|Y^L, X) \right) p(Y^L|X) \\
  p(Y^L|X) & = \sum_{Y^{L-1}}
             \left( \prod_{t=1}^{T_Y} p(y_t^L | Y^{L-1}, X) \right)
             p(Y^{L-1}|X) \\
  \vdots \nonumber \\
  p(Y^0|X) & = \prod_{t=1}^{T_Y} p(y_t^0|X)
\end{align*}
Each summation in the equations above is computed over the whole space of
possible sequences, and thus, it is intractable for the probablility $p(Y|X)$ to
be caluculated exactly. To overcome this problem, the authors approximate the
sums with only the element corresponding to the sequence with the largest
probability, $\hat{y}_t = \argmax_{y_t}p(y_t|X)$.\footnote{Note that in
  non-autoregressive model where the probability does not depend on previously
  decoded outputs $y_{<t}$, getting the most probable tokens will also yield the
  most probable sequence.}  Putting it together with the equations above and
moving to logarithmic domain, we get:
\begin{align}
  \log p(Y|X) \geq
    & \sum_{t=1}^{T_Y} \log p(y_t| \hat{Y}^L, X) + \\
    & + \sum_{t=1}^{T_Y} \log p(\hat{y}_t^{L}| \hat{Y}^{L-1}, X) + \ldots \\
    & + \sum_{t=1}^{T_Y} \log p(\hat{y}_t^0 | X) \label{eq:refinement-lowerbound}
% = & \sum_{l=0}^{L} \sum_{t=1}^{T_Y} \log p(\hat{y}_t^{l+1} | \hat{Y}^l, X)
\end{align}
%where $Y^0 = X$ and $\hat{y}_t^{L+1} = y_t$.

The initial sequence $\hat{y}_t^0$ is set to $x_{t'}$ where
$t' = (T_X / T_Y) \cdot t$, i.e. the source sentence is either squished or
stretched by copying or omitting some words in order to fit the target sentence
length.

All probability distributions in the above equations are modeled with neural
networks with shared parameters. In this way, the number of intermediate
refinement steps remains flexible during decoding.

\textbf{Training} -- the latent variable model loss is trained by minimizing the
negative log-likelihood of the target sequence $Y^*$ in each of the refinement
steps:
\begin{equation}
  \mathcal{L}_{\text{LVM}}(\theta) = - \sum_{l=1}^{L+1} \left(
    \sum_{t=1}^{T_Y} \log p(y_t^* | \hat{Y}^{l-1}, X, \theta)
  \right) \label{eq:refinement-lvm-loss}
\end{equation}


\textbf{Denoising perspective} -- The authors introduce an additional denoising
autoencoder loss to the model:
\begin{equation}
  \mathcal{L}-{\text{DAE}}(\theta) = - \sum_{t=1}^{T_Y} \log p(y_t^* | \bar{Y}, X, \theta)
\end{equation}
where $\bar{Y}$ is a corrupted version of a reference translation $Y^*$. The
corruption operations are applied with probability $\beta$ on the tokens --
either a token is replaced with a random word from the vocabulary, or a token is
replaced (copied) with its neighbor, or the neighbors are swapped.

During training, the two loss functions are stochastically mixed using a
hyper-parameter $\alpha$, sampled from a Bernoulli distribution.

They also use knowledge distillation.

% ------------------------------------------------------------------------------

\paragraph{Blockwise Parallel Decoding for Deep Autoregressive Models.}
\citet{stern2018blockwise} propose a semi-autoregressive approach where chunks
of the target sentence are generated in parallel.

They start with a greedy decoding from an autoregressive model, $p_1$, and
introduce additional ``look-ahead'' models $p_2, \ldots p_k$. In time step $t$,
each model $p_i$ predicts the $(t + i)$-th word in the target sequence given the
same prefix of $t$ previously decoded words.

The decoding process has three stages. First, the block of predictions using the
models $p_1, \ldots, p_k$ is computed. Second, model $p_1$ is used to verify the
$(k-1)$ candidates (which is done in parallel in the Transformer model) and
finds the largest $\hat{k}$ such that decoded words from models $p_i$,
$1 \leq i \leq \hat{k}$ are all considered best by $p_1$. Third, the accepted
$\hat{k}$ words are generated and the decoding process jumps to time step
$t + \hat{k}$.






% ------------------------------------------------------------------------------

\JH{add semiautroegressive paper from emnlp 2018}

\begin{itemize}
\item iNat \citep{lee2018deterministic} -- iterative refinement
\item Blockwise \citep{stern2018blockwise}
\item InsT \citep{stern2019insertion} -- insertion transformer
\item CMLM \citep{ghazvininejad2019mask} -- conditional masked language models
\item LevT \citep{gu2019levenshtein} -- Levenshtein transformer
\item KERMIT \citep{chan2019kermit} -- Kermit (arxiv)
\item LaNMT \citep{shu2020latent} -- latent-variable NAR NMT with deterministic inference using a delta posterior
\item SMART \citep{ghazvininejad2020semiautoregressive} -- semi-autoregressive training improves mask-predict decoding
\item DisCO \citep{kasai2020nonautoregressive} -- NAR NMT with disentangled context transformer
\item Imputer \citep{saharia2020nonautoregressive} -- NAR NMT with latent alignments
\end{itemize}

Fully NAT:


% ------------------------------------------------------------------------------

\paragraph{Latent Transformer \citep{kaiser2018fast}.} Latent transformer (fast
decoding in seq. models using discrete latent vars) Use latent variables to make
decoding parallelizable. Auto-encode targets to latent variables which are
predicted autoregressively, then decode from these latent variables in
parallel. Mostly use one eigth of the original sentence length.

Unlike perhaps more common autoencoders, this autoencoder uses discrete latent
variables.  The discretization is studied and decomposed vector quantization
(DVQ) or latent semantic hashing is proposed as best-performing. Generally,
$enc(y) \in \mathbb{R}^D$, where $D$ is latent space dimension. $[K]$ is the
discrete latent space alphabet. $enc(y)$ is discretized to two latent variables
$z_d(y) \in K$ as the latent representation and $z_q(y)$ as input to the
decoder. $\tau_m(i)$ is binary representation of $i$ using $m$ bits.

\textbf{Gumbel softmax} -- produce logits $l = W \text{enc}(y)$,
$W \in \mathbb{R}^{K \times D}$, then take argmax to get $z_d(y)$. Decoder input
$z_q(y)$ is embedding of $z_d(y)$. To make this argmax differentiable, during
training, Gumbel-Softmax trick is used: $g_1, \ldots, g_K$ are sampled from
Gumbel distribution, $g_i \sim -\log(-\log u)$, where $u \sim U(0,1)$, and:
\begin{equation}
  w_i= \frac{\exp((l_i + g_i) / \tau) }{\sum_i exp((l_i + g_i)/\tau)}
\end{equation}
to get $w \in \mathbb{R}^K$, and use $z_q(y) = w \times E$. Gumbel-softmax is
simply used as differentiable sampling from intermediate logits.

\textbf{Improved semantic hashing} -- use rounding bottleneck after squashing
encoder state $z_e(y)$ using saturating sigmoid:
\begin{equation}
  \sigma'(x) = \max(0, \min(1, 1.2 \sigma(x) - 0.1))
\end{equation}
During training, gaussian noise is added before the saturating sigmoid to
encoder states $z_e(y)$: $f_e(y) = \sigma'(z_e(y) + \eta ~ N(0,1)^D)$. Discrete
latent representation, binary vector $g_e(y)$ is constructed by rounding
$f_e(y)$ to zeros and ones. $z_d(y)$ is computed as $\tau^{-1}_{log_K}(g(y))$
(that is, by conversion of $g_e(y)$ from binary to decimal with base $K$). The
input $z_q(y) = e^1_{h_e(y)} + e^2_{1-h_e(y)}$ where $h_e(y)$ is either $f_e$ or
$g_e$ during training, but only $g_e$ during inference.

\textbf{Vector quantizated variational autoencoder (VQ-VAE)} --
$enc(y)\in\mathbb{R}$ mapped on embeddeding $e\in\mathbb{R}^{K\times D}$ as
nearest-neighbor lookup. $z_q(y) = e_k$, whereoq
$k = argmin_{j\in[K]} || enc(y) - e_j ||_2$, latent $z_d(y)$ being $k$. This is
trained with reconstruction loss, and $enc(y)$ being drawn to fixed values of
$z_q(y)$. They also keep moving averages of the embeddings and the counts,
EM-like.

\textbf{DVQ} -- the former has an issue with a few indices receiving the most signal
during training. Proposed two solutions: sliced VQ and projected VQ. In sliced,
idea is to slice $enc(y)$ to smaller vectors and do the nearest neighbor using
the sslices, reconstructing $z_q(y)$ from the results. In projected, instead of
slicing, the vector is projected multiple times to subspace.

The model has three components: autoencoder $ae(y, x)$ which maps $y$ to shorter
latent sequence $l$, latent prediction model $lp(x)$ which produces $l$ out of
$x$, and decoder $ad(l, x)$ which non-autoregressively produces $y$ out of $l$
and $x$. They use two losses with equal weights: recunstruction loss computed
from $ad(ae(y,x),x)$ and latent prediction loss comparing $ae(y,x)$ and $lp(x)$.
The latent predictor is a transformer, the autoencoder is stack of convolutions
with residual connections, and the decoder consists of up-convolutions.

Results measured on WMT'14 EN-DE. Baseline 27.3 BLEU, they achieve 22.5 with
rescoring. p-DVQ and s-DVQ as well as improved semhash are comparable, semhash
seems faster. Latency in the order of 100 ms, 7-8 ms per sentence with batch
size of 64.


% ------------------------------------------------------------------------------
\paragraph{Aligned Cross Entropy for \gls{nat}} The problem with using
cross-entropy objective for training \gls{nat} models is that it heavily
penalizes misaligned target words. If a correct word is generated at an
incorrect position in the target sentence, the loss is the same as if the model
generates a completely unrelated word. In autoregressive models, this problem
is avoided with teacher forcing -- the model is provided with the preceding
words from the reference sentence. Without teacher forcing, the alignment
between the predicted distributions and the reference sentence needs to be
considered as latent.

Aligned cross entropy (AXE; \citealp{ghazvininejad2020aligned}) is an objective
function that takes this consideration into account. It uses a dynamic
programming algorithm to find the alignment with the minimum cross-entropy
loss.

Formally, ... \JH{doplnit}


% ------------------------------------------------------------------------------
\paragraph{\Gls{nat} with auxiliary regularization}
\citet{wang2019nonautoregressive} use regularization terms to penalize
similarity of hidden states corresponding to dissimilar words(which battles
repeated translations), and to express a reconstruction loss using an external
translation model (in order to battle incomplete translation problem).

% ------------------------------------------------------------------------------
\paragraph{\citep{shao2020minimizing}}



% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------


% ------------------------------------------------------------------------------

\begin{itemize}
\item vanilla \citep{gu2017nonautoregressive}
\item CTC (to jsme my)
\item NAT-REG \citep{wang2019nonautoregressive} -- NAR NMT with auxiliary regularization
\item bag-of-ngrams \citep{shao2020minimizing} -- Minimizing the bag-of-ngrams diff for NAR NMT
\item Hint-NAT \citep{li2019hint} -- hint-based training for NAR MT
\item DCRF \citep{sun2019fast} -- fast structured decoding for sequence models
\item Flowseq \citep{ma2019flowseq} -- NAR conditional sequence generator with generative flow
\item ReorderNAT \citep{ran2019guiding} -- guiding NAR NMT decoding with reordering information
\item AXE \citep{ghazvininejad2020aligned} -- aligned xent for NAR MT
\item EM+ODD \citep{sun2020em} -- an em approach to NAR conditional seq generation
\item GLAT \citep{qian2020glancing} -- glancing transformer for NAR NMT
\item Imputer \citep{saharia2020nonautoregressive} -- imputer (again)
\end{itemize}

\section{Non-Autoregressive NMT with Connectionist Temporal Classification}
\label{sec:nat-ctc}

\paperdisclaim{This section is based on paper ``End-to-End Non-Autoregressive
  Neural Machine Translation with Connectionist Temporal Classification'',
  joint work with Jindřich Libovický, published at EMNLP 2018.}

In this section, we describe our approach to \acrlong{nat}.

The idea behind our approach is that we do not explicitly estimate the target
sequence length. Instead, we fix the decoder length on an upper limit of the
target length, given by a hyper-parameter. As the decoder input, we use the
encoder states, split and linearly projected to match the decoder state
dimension and the target length.

As a consequence, we allow the model to generate special ``blank'' tokens in
any given time steps. Since the alignment between the reference tokens and the
corresponding output distribution is unknown, we treat it as a latent variable,
and we use the loss function which marginalizes this latent variable out. In
other words, our loss function is the sum of losses given any possible
alignment. This loss, used mostly in speech recognition domain
\citep{graves2006connectionist}, is known as the \acrfull{ctc}. We describe
this function in more detail in Section \ref{subsec:ctc}.

The \gls{ctc} loss is differentiable with respect to the model parameters,
which means we can back-propagate the gradients returned by the \gls{ctc}
computation. Thus, unlike the originally proposed architectures of
\citet{gu2017nonautoregressive}, our model can be trained end to end.


\subsection{Connectionist Temporal Classification}
\label{subsec:ctc}






\section{Improving Fluency using N-gram Language Models}
\label{sec:nat-lm}

\paperdisclaim{This section is based on paper \emph{``Improving Fluency of
  Non-Autoregressive Neural Machine Translation''}, joint work with Zdeněk
  Kasner and Jindřich Libovický, published online at \texttt{arXiv.org}.}




\section{Non-Autoregressive Model Optimizations}
\label{sec:nat-opt}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
