@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}


@article{conneau-lample-2019-cross,
  title={Cross-lingual language model pretraining},
  author={Conneau, Alexis and Lample, Guillaume},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={7059--7069},
  year={2019}
}


@article{joulin2016fasttext,
  title={FastText.zip: Compressing text classification models},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze,
                  Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1612.03651},
  year={2016}
}


@InProceedings{glorot2010understanding,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html}
 }

@inproceedings{eyben2009speech,
  title={From speech to letters-using a novel neural network architecture for grapheme based asr},
  author={Eyben, Florian and W{\"o}llmer, Martin and Schuller, Bj{\"o}rn and Graves, Alex},
  booktitle={2009 IEEE Workshop on Automatic Speech Recognition \& Understanding},
  pages={376--380},
  year={2009},
  organization={IEEE}
}

@inproceedings{graves2014towards,
  title={Towards end-to-end speech recognition with recurrent neural networks},
  author={Graves, Alex and Jaitly, Navdeep},
  booktitle={International conference on machine learning},
  pages={1764--1772},
  year={2014},
  organization={PMLR}
}

@inproceedings{liwicki2007novel,
  title={A novel approach to on-line handwriting recognition based on
                  bidirectional long short-term memory networks},
  author={Liwicki, Marcus and Graves, Alex and Fern{\`a}ndez, Santiago and
                  Bunke, Horst and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 9th International Conference on Document
                  Analysis and Recognition, ICDAR 2007},
  year={2007}
}

@inproceedings{ranzato2016sequence,
  author    = {Marc'Aurelio Ranzato and
               Sumit Chopra and
               Michael Auli and
               Wojciech Zaremba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Sequence Level Training with Recurrent Neural Networks},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.06732},
  timestamp = {Thu, 25 Jul 2019 14:25:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RanzatoCAZ15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{bengio2015scheduled,
  title={Scheduled sampling for sequence prediction with recurrent neural networks},
  author={Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  journal={arXiv preprint arXiv:1506.03099},
  year={2015}
}

@article{schwenk2019wikimatrix,
  title={Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia},
  author={Schwenk, Holger and Chaudhary, Vishrav and Sun, Shuo and Gong, Hongyu and Guzm{\'a}n, Francisco},
  journal={arXiv preprint arXiv:1907.05791},
  year={2019}
}

@book{zipf1949human,
  author = {Zipf, George Kingsley},
  title = {Human Behavior and the Principle of Least Effort},
  publisher = {Addison-Wesley},
  year = 1949
}

@misc{kasner2020improving,
      title={Improving Fluency of Non-Autoregressive Machine Translation},
      author={Zdeněk Kasner and Jindřich Libovický and Jindřich Helcl},
      year={2020},
      eprint={2004.03227},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{germann2020university,
    title = "The {U}niversity of {E}dinburgh{'}s submission to the {G}erman-to-{E}nglish and {E}nglish-to-{G}erman Tracks in the {WMT} 2020 News Translation and Zero-shot Translation Robustness Tasks",
    author = "Germann, Ulrich",
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.wmt-1.18",
    pages = "197--201",
}

@article{qian2020glancing,
  title =	 {Glancing Transformer for Non-Autoregressive Neural Machine
                  Translation},
  author =	 {L. Qian and Hao Zhou and Yu Bao and Mingxuan Wang and Lin Qiu
                  and W. Zhang and Y. Yu and Lei Li},
  journal =	 {ArXiv},
  year =	 {2020},
  volume =	 {abs/2008.07905}
}

@InProceedings{sun2020em,
  title =	 {An {EM} Approach to Non-autoregressive Conditional Sequence
                  Generation},
  author =	 {Sun, Zhiqing and Yang, Yiming},
  booktitle =	 {Proceedings of the 37th International Conference on Machine
                  Learning},
  pages =	 {9249--9258},
  year =	 {2020},
  editor =	 {Hal Daumé III and Aarti Singh},
  volume =	 {119},
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--18 Jul},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v119/sun20c/sun20c.pdf},
  url =		 { http://proceedings.mlr.press/v119/sun20c.html },
  abstract =	 {Autoregressive (AR) models have been the dominating approach
                  to conditional sequence generation, but are suffering from the
                  issue of high inference latency. Non-autoregressive (NAR)
                  models have been recently proposed to reduce the latency by
                  generating all output tokens in parallel but could only
                  achieve inferior accuracy compared to their autoregressive
                  counterparts, primarily due to a difficulty in dealing with
                  the multi-modality in sequence generation. This paper proposes
                  a new approach that jointly optimizes both AR and NAR models
                  in a unified Expectation-Maximization (EM) framework. In the
                  E-step, an AR model learns to approximate the regularized
                  posterior of the NAR model. In the M-step, the NAR model is
                  updated on the new posterior and selects the training examples
                  for the next AR model. This iterative process can effectively
                  guide the system to remove the multi-modality in the output
                  sequences. To our knowledge, this is the first EM approach to
                  NAR sequence generation. We evaluate our method on the task of
                  machine translation. Experimental results on benchmark data
                  sets show that the proposed approach achieves competitive, if
                  not better, performance with existing NAR models and
                  significantly reduces the inference latency.}
}

@InProceedings{ghazvininejad2020aligned,
  title =	 {Aligned Cross Entropy for Non-Autoregressive Machine
                  Translation},
  author =	 {Ghazvininejad, Marjan and Karpukhin, Vladimir and Zettlemoyer,
                  Luke and Levy, Omer},
  booktitle =	 {Proceedings of the 37th International Conference on Machine
                  Learning},
  pages =	 {3515--3523},
  year =	 {2020},
  editor =	 {Hal Daumé III and Aarti Singh},
  volume =	 {119},
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--18 Jul},
  publisher =	 {PMLR},
  pdf =
                  {http://proceedings.mlr.press/v119/ghazvininejad20a/ghazvininejad20a.pdf},
  url =		 { http://proceedings.mlr.press/v119/ghazvininejad20a.html },
  abstract =	 {Non-autoregressive machine translation models significantly
                  speed up decoding by allowing for parallel prediction of the
                  entire target sequence. However, modeling word order is more
                  challenging due to the lack of autoregressive factors in the
                  model. This difficultly is compounded during training with
                  cross entropy loss, which can highly penalize small shifts in
                  word order. In this paper, we propose aligned cross entropy
                  (AXE) as an alternative loss function for training of
                  non-autoregressive models. AXE uses a differentiable dynamic
                  program to assign loss based on the best possible monotonic
                  alignment between target tokens and model
                  predictions. AXE-based training of conditional masked language
                  models (CMLMs) substantially improves performance on major WMT
                  benchmarks, while setting a new state of the art for
                  non-autoregressive models.}
}

@article{ran2019guiding,
  author =	 {Qiu Ran and Yankai Lin and Peng Li and Jie Zhou},
  title =	 {Guiding Non-Autoregressive Neural Machine Translation Decoding
                  with Reordering Information},
  journal =	 {CoRR},
  volume =	 {abs/1911.02215},
  year =	 {2019},
  url =		 {http://arxiv.org/abs/1911.02215},
  archivePrefix ={arXiv},
  eprint =	 {1911.02215},
  timestamp =	 {Sat, 23 Jan 2021 01:17:25 +0100},
  biburl =	 {https://dblp.org/rec/journals/corr/abs-1911-02215.bib},
  bibsource =	 {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ma2019flowseq,
  title =	 "{F}low{S}eq: Non-Autoregressive Conditional Sequence
                  Generation with Generative Flow",
  author =	 "Ma, Xuezhe and Zhou, Chunting and Li, Xian and Neubig, Graham
                  and Hovy, Eduard",
  booktitle =	 "Proceedings of the 2019 Conference on Empirical Methods in
                  Natural Language Processing and the 9th International Joint
                  Conference on Natural Language Processing (EMNLP-IJCNLP)",
  month =	 nov,
  year =	 "2019",
  address =	 "Hong Kong, China",
  publisher =	 "Association for Computational Linguistics",
  url =		 "https://www.aclweb.org/anthology/D19-1437",
  doi =		 "10.18653/v1/D19-1437",
  pages =	 "4282--4292",
  abstract =	 "Most sequence-to-sequence (seq2seq) models are autoregressive;
                  they generate each token by conditioning on previously
                  generated tokens. In contrast, non-autoregressive seq2seq
                  models generate all tokens in one pass, which leads to
                  increased efficiency through parallel processing on hardware
                  such as GPUs. However, directly modeling the joint
                  distribution of all tokens simultaneously is challenging, and
                  even with increasingly complex model structures accuracy lags
                  significantly behind autoregressive models. In this paper, we
                  propose a simple, efficient, and effective model for
                  non-autoregressive sequence generation using latent variable
                  models. Specifically, we turn to generative flow, an elegant
                  technique to model complex distributions using neural
                  networks, and design several layers of flow tailored for
                  modeling the conditional density of sequential latent
                  variables. We evaluate this model on three neural machine
                  translation (NMT) benchmark datasets, achieving comparable
                  performance with state-of-the-art non-autoregressive NMT
                  models and almost constant decoding time w.r.t the sequence
                  length.",
}

@inproceedings{sun2019fast,
  author =	 {Sun, Zhiqing and Li, Zhuohan and Wang, Haoqing and He, Di and
                  Lin, Zi and Deng, Zhihong},
  booktitle =	 {Advances in Neural Information Processing Systems},
  editor =	 {H. Wallach and H. Larochelle and A. Beygelzimer and
                  F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages =	 {3016--3026},
  publisher =	 {Curran Associates, Inc.},
  title =	 {Fast Structured Decoding for Sequence Models},
  url =
                  {https://proceedings.neurips.cc/paper/2019/file/74563ba21a90da13dacf2a73e3ddefa7-Paper.pdf},
  volume =	 {32},
  year =	 {2019}
}

@inproceedings{li2019hint,
  title =	 "Hint-Based Training for Non-Autoregressive Machine
                  Translation",
  author =	 "Li, Zhuohan and Lin, Zi and He, Di and Tian, Fei and Qin, Tao
                  and Wang, Liwei and Liu, Tie-Yan",
  booktitle =	 "Proceedings of the 2019 Conference on Empirical Methods in
                  Natural Language Processing and the 9th International Joint
                  Conference on Natural Language Processing (EMNLP-IJCNLP)",
  month =	 nov,
  year =	 "2019",
  address =	 "Hong Kong, China",
  publisher =	 "Association for Computational Linguistics",
  url =		 "https://www.aclweb.org/anthology/D19-1573",
  doi =		 "10.18653/v1/D19-1573",
  pages =	 "5708--5713",
  abstract =	 "Due to the unparallelizable nature of the autoregressive
                  factorization, AutoRegressive Translation (ART) models have to
                  generate tokens sequentially during decoding and thus suffer
                  from high inference latency. Non-AutoRegressive Translation
                  (NART) models were proposed to reduce the inference time, but
                  could only achieve inferior translation accuracy. In this
                  paper, we proposed a novel approach to leveraging the hints
                  from hidden states and word alignments to help the training of
                  NART models. The results achieve significant improvement over
                  previous NART models for the WMT14 En-De and De-En datasets
                  and are even comparable to a strong LSTM-based ART baseline
                  but one order of magnitude faster in inference.",
}

@article{shao2020minimizing,
  title =	 {Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive
                  Neural Machine Translation},
  volume =	 {34},
  url =		 {https://ojs.aaai.org/index.php/AAAI/article/view/5351},
  DOI =		 {10.1609/aaai.v34i01.5351},
  abstractNote = {&lt;p&gt;Non-Autoregressive Neural Machine Translation (NAT)
                  achieves significant decoding speedup through generating
                  target words independently and simultaneously. However, in the
                  context of non-autoregressive translation, the word-level
                  cross-entropy loss cannot model the target-side sequential
                  dependency properly, leading to its weak correlation with the
                  translation quality. As a result, NAT tends to generate
                  influent translations with over-translation and
                  under-translation errors. In this paper, we propose to train
                  NAT to minimize the Bag-of-Ngrams (BoN) difference between the
                  model output and the reference sentence. The bag-of-ngrams
                  training objective is differentiable and can be efficiently
                  calculated, which encourages NAT to capture the target-side
                  sequential dependency and correlates well with the translation
                  quality. We validate our approach on three translation tasks
                  and show that our approach largely outperforms the NAT
                  baseline by about 5.0 BLEU scores on WMT14 En↔De and about 2.5
                  BLEU scores on WMT16 En↔Ro.&lt;/p&gt;},
  number =	 {01},
  journal =	 {Proceedings of the AAAI Conference on Artificial Intelligence},
  author =	 {Shao, Chenze and Zhang, Jinchao and Feng, Yang and Meng,
                  Fandong and Zhou, Jie},
  year =	 {2020},
  month =	 {Apr.},
  pages =	 {198-205}
}

@article{wang2019nonautoregressive,
  title =	 {Non-Autoregressive Machine Translation with Auxiliary
                  Regularization},
  volume =	 {33},
  url =		 {https://ojs.aaai.org/index.php/AAAI/article/view/4476},
  DOI =		 {10.1609/aaai.v33i01.33015377},
  abstractNote = {&lt;p&gt;As a new neural machine translation approach,
                  NonAutoregressive machine Translation (NAT) has attracted
                  attention recently due to its high efficiency in
                  inference. However, the high efficiency has come at the cost
                  of not capturing the sequential dependency on the target side
                  of translation, which causes NAT to suffer from two kinds of
                  translation errors: 1) repeated translations (due to
                  indistinguishable adjacent decoder hidden states), and 2)
                  incomplete translations (due to incomplete transfer of source
                  side information via the decoder hidden states). In this
                  paper, we propose to address these two problems by improving
                  the quality of decoder hidden representations via two
                  auxiliary regularization terms in the training process of an
                  NAT model. First, to make the hidden states more
                  distinguishable, we regularize the similarity between
                  consecutive hidden states based on the corresponding target
                  tokens. Second, to force the hidden states to contain all the
                  information in the source sentence, we leverage the dual
                  nature of translation tasks (e.g., English to German and
                  German to English) and minimize a backward reconstruction
                  error to ensure that the hidden states of the NAT decoder are
                  able to recover the source side sentence. Extensive
                  experiments conducted on several benchmark datasets show that
                  both regularization strategies are effective and can alleviate
                  the issues of repeated translations and incomplete
                  translations in NAT models. The accuracy of NAT models is
                  therefore improved significantly over the state-of-the-art NAT
                  models with even better efficiency for inference.&lt;/p&gt;},
  number =	 {01},
  journal =	 {Proceedings of the AAAI Conference on Artificial Intelligence},
  author =	 {Wang, Yiren and Tian, Fei and He, Di and Qin, Tao and Zhai,
                  ChengXiang and Liu, Tie-Yan},
  year =	 {2019},
  month =	 {Jul.},
  pages =	 {5377-5384}
}

@InProceedings{kaiser2018fast,
  title =	 {Fast Decoding in Sequence Models Using Discrete Latent
                  Variables},
  author =	 {Kaiser, Lukasz and Bengio, Samy and Roy, Aurko and Vaswani,
                  Ashish and Parmar, Niki and Uszkoreit, Jakob and Shazeer,
                  Noam},
  booktitle =	 {Proceedings of the 35th International Conference on Machine
                  Learning},
  pages =	 {2390--2399},
  year =	 2018,
  editor =	 {Jennifer Dy and Andreas Krause},
  volume =	 80,
  series =	 {Proceedings of Machine Learning Research},
  address =	 {Stockholmsmässan, Stockholm Sweden},
  month =	 {10--15 Jul},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v80/kaiser18a/kaiser18a.pdf},
  url =		 {http://proceedings.mlr.press/v80/kaiser18a.html},
  abstract =	 {Autoregressive sequence models based on deep neural networks,
                  such as RNNs, Wavenet and Transformer are the state-of-the-art
                  on many tasks. However, they lack parallelism and are thus
                  slow for long sequences. RNNs lack parallelism both during
                  training and decoding, while architectures like WaveNet and
                  Transformer are much more parallel during training, but still
                  lack parallelism during decoding. We present a method to
                  extend sequence models using discrete latent variables that
                  makes decoding much more parallel. The main idea behind this
                  approach is to first autoencode the target sequence into a
                  shorter discrete latent sequence, which is generated
                  autoregressively, and finally decode the full sequence from
                  this shorter latent sequence in a parallel manner. To this
                  end, we introduce a new method for constructing discrete
                  latent variables and compare it with previously introduced
                  methods. Finally, we verify that our model works on the task
                  of neural machine translation, where our models are an order
                  of magnitude faster than comparable autoregressive models and,
                  while lower in BLEU than purely autoregressive models, better
                  than previously proposed non-autogregressive translation.}
}

@inproceedings{saharia2020nonautoregressive,
  title =	 "Non-Autoregressive Machine Translation with Latent Alignments",
  author =	 "Saharia, Chitwan and Chan, William and Saxena, Saurabh and
                  Norouzi, Mohammad",
  booktitle =	 "Proceedings of the 2020 Conference on Empirical Methods in
                  Natural Language Processing (EMNLP)",
  month =	 nov,
  year =	 "2020",
  address =	 "Online",
  publisher =	 "Association for Computational Linguistics",
  url =		 "https://www.aclweb.org/anthology/2020.emnlp-main.83",
  doi =		 "10.18653/v1/2020.emnlp-main.83",
  pages =	 "1098--1108",
  abstract =	 "This paper presents two strong methods, CTC and Imputer, for
                  non-autoregressive machine translation that model latent
                  alignments with dynamic programming. We revisit CTC for
                  machine translation and demonstrate that a simple CTC model
                  can achieve state-of-the-art for single-step
                  non-autoregressive machine translation, contrary to what prior
                  work indicates. In addition, we adapt the Imputer model for
                  non-autoregressive machine translation and demonstrate that
                  Imputer with just 4 generation steps can match the performance
                  of an autoregressive Transformer baseline. Our latent
                  alignment models are simpler than many existing
                  non-autoregressive translation baselines; for example, we do
                  not require target length prediction or re-scoring with an
                  autoregressive model. On the competitive WMT{'}14
                  En$\rightarrow$De task, our CTC model achieves 25.7 BLEU with
                  a single generation step, while Imputer achieves 27.5 BLEU
                  with 2 generation steps, and 28.0 BLEU with 4 generation
                  steps. This compares favourably to the autoregressive
                  Transformer baseline at 27.8 BLEU.",
}

@InProceedings{chan2020imputer,
  title =	 {Imputer: Sequence Modelling via Imputation and Dynamic
                  Programming},
  author =	 {Chan, William and Saharia, Chitwan and Hinton, Geoffrey and
                  Norouzi, Mohammad and Jaitly, Navdeep},
  booktitle =	 {Proceedings of the 37th International Conference on Machine
                  Learning},
  pages =	 {1403--1413},
  year =	 {2020},
  editor =	 {Hal Daumé III and Aarti Singh},
  volume =	 {119},
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--18 Jul},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v119/chan20b/chan20b.pdf},
  url =		 { http://proceedings.mlr.press/v119/chan20b.html },
  abstract =	 {This paper presents the Imputer, a neural sequence model that
                  generates output sequences iteratively via imputations. The
                  Imputer is an iterative generation model, requiring only a
                  constant number of generation steps independent of the number
                  of input or output tokens. The Imputer can be trained to
                  approximately marginalize over all possible alignments between
                  the input and output sequences, and all possible generation
                  orders. We present a tractable dynamic programming training
                  algorithm, which yields a lower bound on the log marginal
                  likelihood. When applied to end-to-end speech recognition, the
                  Imputer outperforms prior non-autoregressive models and
                  achieves competitive results to autoregressive models. On
                  LibriSpeech test-other, the Imputer achieves 11.1 WER,
                  outperforming CTC at 13.0 WER and seq2seq at 12.5 WER.}
}

@InProceedings{kasai2020nonautoregressive,
  title =	 {Non-autoregressive Machine Translation with Disentangled
                  Context Transformer},
  author =	 {Kasai, Jungo and Cross, James and Ghazvininejad, Marjan and
                  Gu, Jiatao},
  booktitle =	 {Proceedings of the 37th International Conference on Machine
                  Learning},
  pages =	 {5144--5155},
  year =	 {2020},
  editor =	 {Hal Daumé III and Aarti Singh},
  volume =	 {119},
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--18 Jul},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v119/kasai20a/kasai20a.pdf},
  url =		 { http://proceedings.mlr.press/v119/kasai20a.html },
  abstract =	 {State-of-the-art neural machine translation models generate a
                  translation from left to right and every step is conditioned
                  on the previously generated tokens. The sequential nature of
                  this generation process causes fundamental latency in
                  inference since we cannot generate multiple tokens in each
                  sentence in parallel. We propose an attention-masking based
                  model, called Disentangled Context (DisCo) transformer, that
                  simultaneously generates all tokens given different
                  contexts. The DisCo transformer is trained to predict every
                  output token given an arbitrary subset of the other reference
                  tokens. We also develop the parallel easy-first inference
                  algorithm, which iteratively refines every token in parallel
                  and reduces the number of required iterations. Our extensive
                  experiments on 7 translation directions with varying data
                  sizes demonstrate that our model achieves competitive, if not
                  better, performance compared to the state of the art in
                  non-autoregressive machine translation while significantly
                  reducing decoding time on average.}
}

@article{ghazvininejad2020semiautoregressive,
  title =	 {Semi-Autoregressive Training Improves Mask-Predict Decoding},
  author =	 {Marjan Ghazvininejad and Omer Levy and Luke Zettlemoyer},
  journal =	 {ArXiv},
  year =	 {2020},
  volume =	 {abs/2001.08785}
}

@article{shu2020latent,
  title =	 {Latent-Variable Non-Autoregressive Neural Machine Translation
                  with Deterministic Inference Using a Delta Posterior},
  volume =	 {34},
  url =		 {https://ojs.aaai.org/index.php/AAAI/article/view/6413},
  DOI =		 {10.1609/aaai.v34i05.6413},
  abstractNote = {&lt;p&gt;Although neural machine translation models reached
                  high translation quality, the autoregressive nature makes
                  inference difficult to parallelize and leads to high
                  translation latency. Inspired by recent refinement-based
                  approaches, we propose LaNMT, a latent-variable
                  non-autoregressive model with continuous latent variables and
                  deterministic inference procedure. In contrast to existing
                  approaches, we use a deterministic inference algorithm to find
                  the target sequence that maximizes the lowerbound to the
                  log-probability. During inference, the length of translation
                  automatically adapts itself. Our experiments show that the
                  lowerbound can be greatly increased by running the inference
                  algorithm, resulting in significantly improved translation
                  quality. Our proposed model closes the performance gap between
                  non-autoregressive and autoregressive approaches on ASPEC
                  Ja-En dataset with 8.6x faster decoding. On WMT’14 En-De
                  dataset, our model narrows the gap with autoregressive
                  baseline to 2.0 BLEU points with 12.5x speedup. By decoding
                  multiple initial latent variables in parallel and rescore
                  using a teacher model, the proposed model further brings the
                  gap down to 1.0 BLEU point on WMT’14 En-De task with 6.8x
                  speedup.&lt;/p&gt;},
  number =	 {05},
  journal =	 {Proceedings of the AAAI Conference on Artificial Intelligence},
  author =	 {Shu, Raphael and Lee, Jason and Nakayama, Hideki and Cho,
                  Kyunghyun},
  year =	 {2020},
  month =	 {Apr.},
  pages =	 {8846-8853}
}

@article{chan2019kermit,
  author =	 {William Chan and Nikita Kitaev and Kelvin Guu and Mitchell
                  Stern and Jakob Uszkoreit},
  title =	 {{KERMIT:} Generative Insertion-Based Modeling for Sequences},
  journal =	 {CoRR},
  volume =	 {abs/1906.01604},
  year =	 {2019},
  url =		 {http://arxiv.org/abs/1906.01604},
  archivePrefix ={arXiv},
  eprint =	 {1906.01604},
  timestamp =	 {Thu, 13 Jun 2019 13:36:00 +0200},
  biburl =	 {https://dblp.org/rec/journals/corr/abs-1906-01604.bib},
  bibsource =	 {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gu2019levenshtein,
  author =	 {Gu, Jiatao and Wang, Changhan and Zhao, Junbo},
  booktitle =	 {Advances in Neural Information Processing Systems},
  editor =	 {H. Wallach and H. Larochelle and A. Beygelzimer and
                  F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages =	 {11181--11191},
  publisher =	 {Curran Associates, Inc.},
  title =	 {Levenshtein Transformer},
  url =
                  {https://proceedings.neurips.cc/paper/2019/file/675f9820626f5bc0afb47b57890b466e-Paper.pdf},
  volume =	 {32},
  year =	 {2019}
}



@inproceedings{stern2018blockwise,
  author =	 {Stern, Mitchell and Shazeer, Noam and Uszkoreit, Jakob},
  booktitle =	 {Advances in Neural Information Processing Systems},
  editor =	 {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and
                  N. Cesa-Bianchi and R. Garnett},
  pages =	 {10086--10095},
  publisher =	 {Curran Associates, Inc.},
  title =	 {Blockwise Parallel Decoding for Deep Autoregressive Models},
  url =
                  {https://proceedings.neurips.cc/paper/2018/file/c4127b9194fe8562c64dc0f5bf2c93bc-Paper.pdf},
  volume =	 {31},
  year =	 {2018}
}


@InProceedings{stern2019insertion,
  title =	 {Insertion Transformer: Flexible Sequence Generation via
                  Insertion Operations},
  author =	 {Stern, Mitchell and Chan, William and Kiros, Jamie and
                  Uszkoreit, Jakob},
  booktitle =	 {Proceedings of the 36th International Conference on Machine
                  Learning},
  pages =	 {5976--5985},
  year =	 2019,
  editor =	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume =	 97,
  series =	 {Proceedings of Machine Learning Research},
  month =	 {09--15 Jun},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v97/stern19a/stern19a.pdf},
  url =		 { http://proceedings.mlr.press/v97/stern19a.html },
  abstract =	 {We present the Insertion Transformer, an iterative, partially
                  autoregressive model for sequence generation based on
                  insertion operations. Unlike typical autoregressive models
                  which rely on a fixed, often left-to-right ordering of the
                  output, our approach accommodates arbitrary orderings by
                  allowing for tokens to be inserted anywhere in the sequence
                  during decoding. This flexibility confers a number of
                  advantages: for instance, not only can our model be trained to
                  follow specific orderings such as left-to-right generation or
                  a binary tree traversal, but it can also be trained to
                  maximize entropy over all valid insertions for robustness. In
                  addition, our model seamlessly accommodates both fully
                  autoregressive generation (one insertion at a time) and
                  partially autoregressive generation (simultaneous insertions
                  at multiple locations). We validate our approach by analyzing
                  its performance on the WMT 2014 English-German machine
                  translation task under various settings for training and
                  decoding. We find that the Insertion Transformer outperforms
                  many prior non-autoregressive approaches to translation at
                  comparable or better levels of parallelism, and successfully
                  recovers the performance of the original Transformer while
                  requiring only logarithmically many iterations during
                  decoding.}
}


@inproceedings{barone2017deep,
  title =	 "Deep architectures for Neural Machine Translation",
  author =	 "Miceli Barone, Antonio Valerio and Helcl, Jind{\v{r}}ich and
                  Sennrich, Rico and Haddow, Barry and Birch, Alexandra",
  booktitle =	 "Proceedings of the Second Conference on Machine Translation",
  month =	 sep,
  year =	 2017,
  address =	 "Copenhagen, Denmark",
  publisher =	 "Association for Computational Linguistics",
  url =		 "https://www.aclweb.org/anthology/W17-4710",
  doi =		 "10.18653/v1/W17-4710",
  pages =	 "99--107",
}

@inproceedings{gu2017nonautoregressive,
  author =	 {Jiatao Gu and James Bradbury and Caiming Xiong and Victor
                  O. K. Li and Richard Socher},
  title =	 {Non-Autoregressive Neural Machine Translation},
  booktitle =	 {6th International Conference on Learning Representations,
                  {ICLR} 2018},
  month =	 apr,
  address =	 {Vancouver, BC, Canada},
  year =	 2018,
  url =		 {https://openreview.net/forum?id=B1l8BtlCb},
  timestamp =	 {Thu, 25 Jul 2019 14:25:57 +0200},
}

@inproceedings{lee2018deterministic,
  author =	 {Lee, Jason and Mansimov, Elman and Cho, Kyunghyun},
  title =	 {Deterministic Non-Autoregressive Neural Sequence Modeling by
                  Iterative Refinement},
  booktitle =	 {Proceedings of the 2018 Conference on Empirical Methods in
                  Natural Language Processing},
  address =	 {Brussels, Belgium},
  month =	 {November},
  pages =	 {1173--1182},
  publisher =	 {Association for Computational Linguistics},
  url =		 {http://www.aclweb.org/anthology/D18-1149},
  year =	 2018
}

@inproceedings{ghazvininejad2019mask,
  title =	 "Mask-Predict: Parallel Decoding of Conditional Masked Language
                  Models",
  author =	 "Ghazvininejad, Marjan and Levy, Omer and Liu, Yinhan and
                  Zettlemoyer, Luke",
  booktitle =	 "Proceedings of the 2019 Conference on Empirical Methods in
                  Natural Language Processing and the 9th International Joint
                  Conference on Natural Language Processing (EMNLP-IJCNLP)",
  month =	 nov,
  year =	 2019,
  address =	 "Hong Kong, China",
  publisher =	 "Association for Computational Linguistics",
  url =		 "https://www.aclweb.org/anthology/D19-1633",
  doi =		 "10.18653/v1/D19-1633",
  pages =	 "6111--6120",
}

@article{mansimov2019generalized,
  author =	 {Elman Mansimov and Alex Wang and Kyunghyun Cho},
  title =	 {A Generalized Framework of Sequence Generation with
                  Application to Undirected Sequence Models},
  journal =	 {CoRR},
  volume =	 {abs/1905.12790},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1905.12790},
  archivePrefix ={arXiv},
  eprint =	 {1905.12790},
  timestamp =	 {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl =	 {https://dblp.org/rec/journals/corr/abs-1905-12790.bib},
  bibsource =	 {dblp computer science bibliography, https://dblp.org}
}
@article{reiter2018structured,
  author =	 {Reiter, Ehud},
  title =	 {A Structured Review of the Validity of BLEU},
  journal =	 {Computational Linguistics},
  volume =	 44,
  number =	 3,
  pages =	 {393-401},
  year =	 2018,
  doi =		 {10.1162/coli\_a\_00322},
  URL =		 {https://doi.org/10.1162/coli_a_00322},
  eprint =	 {ttps://doi.org/10.1162/coli_a_00322},
  abstract =	 { The BLEU metric has been widely used in NLP for over 15 years
                  to evaluate NLP systems, especially in machine translation and
                  natural language generation. I present a structured review of
                  the evidence on whether BLEU is a valid evaluation
                  technique—in other words, whether BLEU scores correlate with
                  real-world utility and user-satisfaction of NLP systems; this
                  review covers 284 correlations reported in 34 papers. Overall,
                  the evidence supports using BLEU for diagnostic evaluation of
                  MT systems (which is what it was originally proposed for), but
                  does not support using BLEU outside of MT, for evaluation of
                  individual texts, or for scientific hypothesis testing. }
}

@inproceedings{jean2015using,
  title =	 "On Using Very Large Target Vocabulary for Neural Machine
                  Translation",
  author =	 "Jean, S{\'e}bastien and Cho, Kyunghyun and Memisevic, Roland
                  and Bengio, Yoshua",
  booktitle =	 "Proceedings of the 53rd Annual Meeting of the Association for
                  Computational Linguistics and the 7th International Joint
                  Conference on Natural Language Processing (Volume 1: Long
                  Papers)",
  month =	 jul,
  year =	 2015,
  address =	 "Beijing, China",
  publisher =	 "Association for Computational Linguistics",
  url =		 "https://www.aclweb.org/anthology/P15-1001",
  doi =		 "10.3115/v1/P15-1001",
  pages =	 "1--10",
}

@inproceedings{mikolov2013distributed,
  title =	 {Distributed representations of words and phrases and their
                  compositionality},
  author =	 {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado,
                  Greg S and Dean, Jeff},
  booktitle =	 {Advances in neural information processing systems},
  pages =	 {3111--3119},
  year =	 2013
}

@article{bengio2003neural,
  title =	 {A neural probabilistic language model},
  author =	 {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal
                  and Jauvin, Christian},
  journal =	 {Journal of machine learning research},
  volume =	 3,
  number =	 {Feb},
  pages =	 {1137--1155},
  year =	 2003
}

@inproceedings{kudo2018sentencepiece,
  title =	 "{S}entence{P}iece: A simple and language independent subword
                  tokenizer and detokenizer for Neural Text Processing",
  author =	 "Kudo, Taku and Richardson, John",
  booktitle =	 "Proceedings of the 2018 Conference on Empirical Methods in
                  Natural Language Processing: System Demonstrations",
  month =	 nov,
  year =	 2018,
  address =	 "Brussels, Belgium",
  publisher =	 "Association for Computational Linguistics",
  url =		 "https://www.aclweb.org/anthology/D18-2012",
  doi =		 "10.18653/v1/D18-2012",
  pages =	 "66--71"
}

@article{ba2016layer,
  author =	 {Ba, Lei Jimmy and Kiros, Ryan and Hinton, Geoffrey E},
  title =	 {Layer Normalization},
  journal =	 {CoRR},
  archiveprefix ={arXiv},
  eprint =	 {1607.06450},
  issn =	 {2331-8422},
  volume =	 {abs/1607.06450},
  year =	 2016
}

@article{bahdanau2014neural,
  author =	 {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  title =	 {Neural Machine Translation by Jointly Learning to Align and
                  Translate},
  journal =	 {CoRR},
  issn =	 {2331-8422},
  volume =	 {abs/1409.0473},
  year =	 2014
}

@inproceedings{bengio2007greedy,
  author =	 {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and
                  Larochelle, Hugo},
  title =	 {Greedy layer-wise training of deep networks},
  booktitle =	 {Advances in Neural Information Processing Systems 20},
  address =	 {Vancouver, Canada},
  isbn =	 9780262256919,
  month =	 {December},
  pages =	 {153--160},
  publisher =	 {Curran Associates, Inc.},
  year =	 2007
}

@article{bengio2013representation,
  author =	 {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  title =	 {Representation Learning: A Review and New Perspectives},
  journal =	 {IEEE Trans. Pattern Anal. Mach. Intell.},
  address =	 {Washington, D.C., USA},
  issn =	 {0162-8828},
  month =	 {August},
  number =	 8,
  pages =	 {1798--1828},
  publisher =	 {IEEE Computer Society},
  volume =	 35,
  year =	 2013
}

@inproceedings{cho2014gru,
  author =	 {Cho, Kyunghyun and Merrienboer, Bart van and Bahdanau, Dzmitry
                  and Bengio, Yoshua},
  title =	 {On the Properties of Neural Machine Translation:
                  Encoder--Decoder Approaches},
  booktitle =	 {Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics
                  and Structure in Statistical Translation},
  address =	 {Doha, Qatar},
  month =	 {October},
  pages =	 {103--111},
  publisher =	 {Association for Computational Linguistics},
  year =	 2014
}

@inproceedings{cho2014learning,
  author =	 {Cho, Kyunghyun and Merrienboer, Bart van and Gulcehre, Caglar
                  and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger
                  and Bengio, Yoshua},
  title =	 {Learning Phrase Representations using RNN Encoder--Decoder for
                  Statistical Machine Translation},
  booktitle =	 {Proceedings of the 2014 Conference on Empirical Methods in
                  Natural Language Processing ({EMNLP})},
  address =	 {Doha, Qatar},
  month =	 {October},
  pages =	 {1724--1734},
  publisher =	 {Association for Computational Linguistics},
  year =	 2014
}

@article{conneau2017supervised,
  author =	 {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and
                  Barrault, Lo\"{i}c and Bordes, Antoine},
  title =	 {Supervised learning of universal sentence representations from
                  natural language inference data},
  journal =	 {CoRR},
  issn =	 {2331-8422},
  volume =	 {abs/1705.02364},
  year =	 2017
}

@article{conneau2017word,
  author =	 {Conneau, Alexis and Lample, Guillaume and Ranzato,
                  Marc'Aurelio and Denoyer, Ludovic and J{\'{e}}gou,
                  Herv{\'{e}}},
  title =	 {Word Translation Without Parallel Data},
  journal =	 {CoRR},
  archiveprefix ={arXiv},
  eprint =	 {1710.04087},
  issn =	 {2331-8422},
  volume =	 {abs/1710.04087},
  year =	 2017
}

@inproceedings{conneau2018senteval,
  author =	 {Conneau, Alexis and Kiela, Douwe},
  title =	 {{SentEval}: An Evaluation Toolkit for Universal Sentence
                  Representations},
  booktitle =	 {Proceedings of the Eleventh International Conference on
                  Language Resources and Evaluation (LREC)},
  address =	 {Miyazaki, Japan},
  isbn =	 {979-10-95546-00-9},
  language =	 {english},
  month =	 {May},
  pages =	 {1699--1704},
  publisher =	 {European Language Resources Association (ELRA)},
  year =	 2018
}

@inproceedings{denkowski2011meteor,
  author =	 {Denkowski, Michael and Lavie, Alon},
  title =	 {Meteor 1.3: {A}utomatic Metric for Reliable Optimization and
                  Evaluation of Machine Translation Systems},
  booktitle =	 {Proceedings of the Sixth Workshop on Statistical Machine
                  Translation},
  address =	 {Edinburgh, United Kingdom},
  month =	 {July},
  pages =	 {85--91},
  publisher =	 {Association for Computational Linguistics},
  year =	 2011
}

@article{elman1990finding,
  author =	 {Elman, Jeffrey L},
  title =	 {Finding Structure in Time},
  journal =	 {Cognitive Science},
  address =	 {Victoria, Canada},
  issn =	 {1551-6709},
  number =	 2,
  pages =	 {179--211},
  publisher =	 {Cognitive Science Society},
  volume =	 14,
  year =	 1990
}

@inproceedings{gehring2017convolutional,
  author =	 {Gehring, Jonas and Auli, Michael and Grangier, David and
                  Yarats, Denis and Dauphin, Yann N},
  title =	 {Convolutional Sequence to Sequence Learning},
  booktitle =	 {International Conference on Machine Learning},
  address =	 {Sydney, Australia},
  month =	 {August},
  pages =	 {1243--1252},
  publisher =	 {PMLR},
  year =	 2017
}

@article{graves2005framewise,
  author =	 {Graves, Alex and Schmidhuber, J\"{u}rgen},
  title =	 {Framewise phoneme classification with bidirectional LSTM and
                  other neural network architectures},
  journal =	 {Neural Networks},
  address =	 {Amsterdam, Netherlands},
  issn =	 {0893-6080},
  month =	 {Jul},
  number =	 {5-6},
  pages =	 {602--610},
  publisher =	 {Elsevier},
  volume =	 18,
  year =	 2005
}

@inproceedings{graves2006connectionist,
  author =	 {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino
                  and Schmidhuber, J\"{u}rgen},
  title =	 {Connectionist temporal classification: labelling unsegmented
                  sequence data with recurrent neural networks},
  booktitle =	 {Proceedings of the 23rd International Conference on Machine
                  Learning},
  address =	 {Pittsburgh, PA, USA},
  month =	 {June},
  pages =	 {369--376},
  publisher =	 {JMLR.org},
  year =	 2006
}

@inproceedings{graves2009offline,
  author =	 {Graves, Alex and Schmidhuber, J\"{u}rgen},
  title =	 {Offline Handwriting Recognition with Multidimensional
                  Recurrent Neural Networks},
  booktitle =	 {Advances in Neural Information Processing Systems 21},
  address =	 {Vancouver, Canada},
  month =	 {December},
  pages =	 {545--552},
  publisher =	 {Curran Associates, Inc.},
  year =	 2009
}

@inproceedings{graves2013speech,
  author =	 {Graves, Alex and Mohamed, Abdel{-}rahman and Hinton, Geoffrey
                  E},
  title =	 {Speech recognition with deep recurrent neural networks},
  booktitle =	 {International Conference on Acoustics, Speech and Signal
                  Processing ({ICASSP})},
  address =	 {Vancouver, Canada},
  isbn =	 9781479903566,
  month =	 {May},
  pages =	 {6645--6649},
  publisher =	 {{IEEE} Computer Society},
  year =	 2013
}

@article{helcl-libovicky-2017-neural,
  author =	 {Helcl, Jind\v{r}ich and Libovick\'{y}, Jind\v{r}ich},
  title =	 {Neural Monkey: An Open-source Tool for Sequence Learning},
  journal =	 {The Prague Bulletin of Mathematical Linguistics},
  address =	 {Prague, Czech Republic},
  issn =	 {0032-6585},
  month =	 {Apr},
  number =	 1,
  pages =	 {5--17},
  publisher =	 {Charles University},
  volume =	 107,
  year =	 2017
}

@article{hochreiter1997long,
  author =	 {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
  title =	 {Long short-term memory},
  journal =	 {Neural Computation},
  address =	 {Cambridge, MA, USA},
  issn =	 {0899-7667},
  number =	 8,
  pages =	 {1735--1780},
  publisher =	 {MIT Press},
  volume =	 9,
  year =	 1997
}

@inproceedings{kingma2014semi,
  author =	 {Kingma, Diederik P and Mohamed, Shakir and Rezende, Danilo
                  Jimenez and Welling, Max},
  title =	 {Semi-supervised Learning with Deep Generative Models},
  booktitle =	 {Advances in Neural Information Processing Systems 27},
  address =	 {Montreal, Canada},
  month =	 {December},
  pages =	 {3581--3589},
  publisher =	 {Curran Associates, Inc.},
  year =	 2014
}

@article{kingma2015adam,
  author =	 {Kingma, Diederik P and Ba, Jimmy},
  title =	 {Adam: {A} Method for Stochastic Optimization},
  journal =	 {CoRR},
  issn =	 {2331-8422},
  volume =	 {abs/1412.6980},
  year =	 2014
}

@inproceedings{krizhevsky2012imagenet,
  author =	 {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  title =	 {Imagenet classification with deep convolutional neural
                  networks},
  booktitle =	 {Advances in Neural Information Processing Systems 25},
  address =	 {Red Hook, NY, USA},
  isbn =	 9781627480031,
  issn =	 {0001-0782},
  month =	 {May},
  number =	 6,
  pages =	 {1097--1105},
  publisher =	 {Curran Associates, Inc.},
  volume =	 60,
  year =	 2012
}

@inproceedings{kulkarni2011baby,
  author =	 {Kulkarni, Girish and Premraj, Visruth and Dhar, Sagnik and Li,
                  Siming and Choi, Yejin and Berg, Alexander C and Berg, Tamara
                  L},
  title =	 {Baby talk: Understanding and generating simple image
                  descriptions.},
  booktitle =	 {Conference on Computer Vision and Pattern Recognition
                  ({CVPR})},
  address =	 {Colorado Sprtings, CO, USA},
  isbn =	 {978-1-4577-0394-2},
  month =	 {June},
  pages =	 {1601-1608},
  publisher =	 {{IEEE} Computer Society},
  year =	 2011
}

@inproceedings{lafferty2001conditional,
  author =	 {Lafferty, John and McCallum, Andrew and Pereira, Fernando},
  title =	 {Conditional Random Fields: Probabilistic Models for Segmenting
                  and Labeling Sequence Data},
  booktitle =	 {Proceedings of the 18th International Conference on Machine
                  Learning},
  address =	 {Williamstown, MA, USA},
  isbn =	 {1-55860-778-1},
  month =	 {June},
  pages =	 {282--289},
  publisher =	 {Morgan Kaufmann},
  year =	 2001
}

@article{srivastava2014dropout,
  author =	 {Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex
                  and Sutskever, Ilya and Salakhutdinov, Ruslan},
  title =	 {Dropout: A simple way to prevent neural networks from
                  overfitting},
  journal =	 {The Journal of Machine Learning Research},
  address =	 {Brookline, MA, USA},
  issn =	 {1532-4435},
  number =	 1,
  pages =	 {1929--1958},
  publisher =	 {JMLR. org},
  volume =	 15,
  year =	 2014
}

@inproceedings{sutskever2014sequence,
  author =	 {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  title =	 {Sequence to Sequence Learning with Neural Networks},
  booktitle =	 {Advances in Neural Information Processing Systems 27},
  address =	 {Montreal, Canada},
  month =	 {December},
  pages =	 {3104--3112},
  publisher =	 {Curran Associates, Inc.},
  year =	 2014
}

@inproceedings{szegedy2015going,
  author =	 {Szegedy, Christian and Liu, Wei and Jia, Yangqing and
                  Sermanet, Pierre and Reed, Scott E and Anguelov, Dragomir and
                  Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title =	 {Going deeper with convolutions},
  booktitle =	 {Conference on Computer Vision and Pattern Recognition
                  ({CVPR})},
  address =	 {Boston, MA, USA},
  isbn =	 9781467369640,
  month =	 {June},
  pages =	 {1--9},
  publisher =	 {{IEEE} Computer Society},
  year =	 2015
}


@inproceedings{vaswani2017attention,
  author =	 {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                  Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                  Kaiser, {\L}ukasz and Polosukhin, Illia},
  title =	 {Attention is all you need},
  booktitle =	 {Advances in Neural Information Processing Systems 30},
  address =	 {Long Beach, CA, USA},
  month =	 {December},
  pages =	 {6000--6010},
  publisher =	 {Curran Associates, Inc.},
  year =	 2017
}


@article{wu2016google,
  author =	 {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc
                  V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun,
                  Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and
                  Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu,
                  Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato,
                  Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith
                  and Kurian, George and Patil, Nishant and Wang, Wei and Young,
                  Cliff and Smith, Jason R and Riesa, Jason and Rudnick, Alex
                  and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and
                  Dean, Jeffrey},
  title =	 {Google's Neural Machine Translation System: Bridging the Gap
                  between Human and Machine Translation},
  journal =	 {CoRR},
  archiveprefix ={arXiv},
  eprint =	 {1609.08144},
  issn =	 {2331-8422},
  volume =	 {abs/1609.08144},
  year =	 2016
}
