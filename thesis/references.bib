@book{koehn2009statistical,
    author        = {Koehn, Philipp},
    title         = {Statistical machine translation},
    address       = {Cambridge, UK},
    isbn          = {978-0521874151},
    publisher     = {Cambridge University Press},
    url           = {http://www.statmt.org/book/},
    year          = {2009}
}

@article{popel-etal-2020-transforming,
  journal = {Nature Communications},
  title = {Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals},
  author = {Martin Popel and Marketa Tomkova and Jakub Tomek and {\L}ukasz Kaiser and Jakob Uszkoreit and Ond{\v{r}}ej Bojar and Zden{\v{e}}k {\v{Z}}abokrtsk{\'{y}}},
  year = {2020},
  volume = {11},
  number = {4381},
  pages = {1--15},
  issn = {2041-1723},
  doi={10.1038/s41467-020-18073-9},
  url={https://www.nature.com/articles/s41467-020-18073-9},
}


@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}

@misc{tieleman2012lecture,
  title={{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}

@inproceedings{amodei-etal-2016-deep,
  title={Deep Speech 2: End-to-end Speech Recognition in English and Mandarin},
  author={Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and
                  Bai, Jingliang and Battenberg, Eric and Case, Carl and
                  Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen,
                  Guoliang and others},
  booktitle={International conference on machine learning},
  pages={173--182},
  year={2016},
  organization={PMLR}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}


@article{conneau-lample-2019-cross,
  title={Cross-lingual language model pretraining},
  author={Conneau, Alexis and Lample, Guillaume},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={7059--7069},
  year={2019}
}


@article{joulin2016fasttext,
  title={FastText.zip: Compressing text classification models},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze,
                  Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1612.03651},
  year={2016}
}


@InProceedings{glorot2010understanding,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html}
 }

@inproceedings{eyben2009speech,
  title={From speech to letters-using a novel neural network architecture for grapheme based asr},
  author={Eyben, Florian and W{\"o}llmer, Martin and Schuller, Bj{\"o}rn and Graves, Alex},
  booktitle={2009 IEEE Workshop on Automatic Speech Recognition \& Understanding},
  pages={376--380},
  year={2009},
  organization={IEEE}
}

@inproceedings{graves2014towards,
  title={Towards end-to-end speech recognition with recurrent neural networks},
  author={Graves, Alex and Jaitly, Navdeep},
  booktitle={International conference on machine learning},
  pages={1764--1772},
  year={2014},
  organization={PMLR}
}

@inproceedings{liwicki2007novel,
  title={A novel approach to on-line handwriting recognition based on
                  bidirectional long short-term memory networks},
  author={Liwicki, Marcus and Graves, Alex and Fern{\`a}ndez, Santiago and
                  Bunke, Horst and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 9th International Conference on Document
                  Analysis and Recognition, ICDAR 2007},
  year={2007}
}

@inproceedings{ranzato2016sequence,
  author    = {Marc'Aurelio Ranzato and
               Sumit Chopra and
               Michael Auli and
               Wojciech Zaremba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Sequence Level Training with Recurrent Neural Networks},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.06732},
  timestamp = {Thu, 25 Jul 2019 14:25:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RanzatoCAZ15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{bengio2015scheduled,
  title={Scheduled sampling for sequence prediction with recurrent neural networks},
  author={Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  journal={arXiv preprint arXiv:1506.03099},
  year={2015}
}

@article{schwenk2019wikimatrix,
  title={Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia},
  author={Schwenk, Holger and Chaudhary, Vishrav and Sun, Shuo and Gong, Hongyu and Guzm{\'a}n, Francisco},
  journal={arXiv preprint arXiv:1907.05791},
  year={2019}
}

@book{zipf1949human,
  author = {Zipf, George Kingsley},
  title = {Human Behavior and the Principle of Least Effort},
  publisher = {Addison-Wesley},
  year = 1949
}

@mastersthesis{kasner2020incorporating,
  title={Incorporating Language Models into Non-autoregressive Neural Machine
         Translation},
  author={Zdeněk Kasner},
  year={2020},
  school={Czech Technical University},
}

@misc{kasner2020improving,
      title={Improving Fluency of Non-Autoregressive Machine Translation},
      author={Zdeněk Kasner and Jindřich Libovický and Jindřich Helcl},
      year={2020},
      eprint={2004.03227},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{qian2020glancing,
  title =	 {Glancing Transformer for Non-Autoregressive Neural Machine
                  Translation},
  author =	 {L. Qian and Hao Zhou and Yu Bao and Mingxuan Wang and Lin Qiu
                  and W. Zhang and Y. Yu and Lei Li},
  journal =	 {ArXiv},
  year =	 {2020},
  volume =	 {abs/2008.07905}
}

@InProceedings{sun2020em,
  title =	 {An {EM} Approach to Non-autoregressive Conditional Sequence
                  Generation},
  author =	 {Sun, Zhiqing and Yang, Yiming},
  booktitle =	 {Proceedings of the 37th International Conference on Machine
                  Learning},
  pages =	 {9249--9258},
  year =	 {2020},
  editor =	 {Hal Daumé III and Aarti Singh},
  volume =	 {119},
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--18 Jul},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v119/sun20c/sun20c.pdf},
  url =		 { http://proceedings.mlr.press/v119/sun20c.html },
  abstract =	 {Autoregressive (AR) models have been the dominating approach
                  to conditional sequence generation, but are suffering from the
                  issue of high inference latency. Non-autoregressive (NAR)
                  models have been recently proposed to reduce the latency by
                  generating all output tokens in parallel but could only
                  achieve inferior accuracy compared to their autoregressive
                  counterparts, primarily due to a difficulty in dealing with
                  the multi-modality in sequence generation. This paper proposes
                  a new approach that jointly optimizes both AR and NAR models
                  in a unified Expectation-Maximization (EM) framework. In the
                  E-step, an AR model learns to approximate the regularized
                  posterior of the NAR model. In the M-step, the NAR model is
                  updated on the new posterior and selects the training examples
                  for the next AR model. This iterative process can effectively
                  guide the system to remove the multi-modality in the output
                  sequences. To our knowledge, this is the first EM approach to
                  NAR sequence generation. We evaluate our method on the task of
                  machine translation. Experimental results on benchmark data
                  sets show that the proposed approach achieves competitive, if
                  not better, performance with existing NAR models and
                  significantly reduces the inference latency.}
}

@InProceedings{ghazvininejad2020aligned,
  title =	 {Aligned Cross Entropy for Non-Autoregressive Machine
                  Translation},
  author =	 {Ghazvininejad, Marjan and Karpukhin, Vladimir and Zettlemoyer,
                  Luke and Levy, Omer},
  booktitle =	 {Proceedings of the 37th International Conference on Machine
                  Learning},
  pages =	 {3515--3523},
  year =	 {2020},
  editor =	 {Hal Daumé III and Aarti Singh},
  volume =	 {119},
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--18 Jul},
  publisher =	 {PMLR},
  pdf =
                  {http://proceedings.mlr.press/v119/ghazvininejad20a/ghazvininejad20a.pdf},
  url =		 { http://proceedings.mlr.press/v119/ghazvininejad20a.html },
  abstract =	 {Non-autoregressive machine translation models significantly
                  speed up decoding by allowing for parallel prediction of the
                  entire target sequence. However, modeling word order is more
                  challenging due to the lack of autoregressive factors in the
                  model. This difficultly is compounded during training with
                  cross entropy loss, which can highly penalize small shifts in
                  word order. In this paper, we propose aligned cross entropy
                  (AXE) as an alternative loss function for training of
                  non-autoregressive models. AXE uses a differentiable dynamic
                  program to assign loss based on the best possible monotonic
                  alignment between target tokens and model
                  predictions. AXE-based training of conditional masked language
                  models (CMLMs) substantially improves performance on major WMT
                  benchmarks, while setting a new state of the art for
                  non-autoregressive models.}
}


@article{ran-etal-2021-guiding,
  title={Guiding Non-Autoregressive Neural Machine Translation Decoding with
                  Reordering Information},
  volume={35},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/17618},
  number={15},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Ran, Qiu and Lin, Yankai and Li, Peng and Zhou, Jie},
  year={2021},
  month={May},
  pages={13727-13735}}


@inproceedings{sun2019fast,
  author =	 {Sun, Zhiqing and Li, Zhuohan and Wang, Haoqing and He, Di and
                  Lin, Zi and Deng, Zhihong},
  booktitle =	 {Advances in Neural Information Processing Systems},
  editor =	 {H. Wallach and H. Larochelle and A. Beygelzimer and
                  F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages =	 {3016--3026},
  publisher =	 {Curran Associates, Inc.},
  title =	 {Fast Structured Decoding for Sequence Models},
  url =
                  {https://proceedings.neurips.cc/paper/2019/file/74563ba21a90da13dacf2a73e3ddefa7-Paper.pdf},
  volume =	 {32},
  year =	 {2019}
}

@article{shao2020minimizing,
  title =	 {Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive
                  Neural Machine Translation},
  volume =	 {34},
  url =		 {https://ojs.aaai.org/index.php/AAAI/article/view/5351},
  DOI =		 {10.1609/aaai.v34i01.5351},
  abstractNote = {&lt;p&gt;Non-Autoregressive Neural Machine Translation (NAT)
                  achieves significant decoding speedup through generating
                  target words independently and simultaneously. However, in the
                  context of non-autoregressive translation, the word-level
                  cross-entropy loss cannot model the target-side sequential
                  dependency properly, leading to its weak correlation with the
                  translation quality. As a result, NAT tends to generate
                  influent translations with over-translation and
                  under-translation errors. In this paper, we propose to train
                  NAT to minimize the Bag-of-Ngrams (BoN) difference between the
                  model output and the reference sentence. The bag-of-ngrams
                  training objective is differentiable and can be efficiently
                  calculated, which encourages NAT to capture the target-side
                  sequential dependency and correlates well with the translation
                  quality. We validate our approach on three translation tasks
                  and show that our approach largely outperforms the NAT
                  baseline by about 5.0 BLEU scores on WMT14 En↔De and about 2.5
                  BLEU scores on WMT16 En↔Ro.&lt;/p&gt;},
  number =	 {01},
  journal =	 {Proceedings of the AAAI Conference on Artificial Intelligence},
  author =	 {Shao, Chenze and Zhang, Jinchao and Feng, Yang and Meng,
                  Fandong and Zhou, Jie},
  year =	 {2020},
  month =	 {Apr.},
  pages =	 {198-205}
}

@article{wang-etal-2019-nonautoregressive,
  title =	 {Non-Autoregressive Machine Translation with Auxiliary
                  Regularization},
  volume =	 {33},
  url =		 {https://ojs.aaai.org/index.php/AAAI/article/view/4476},
  DOI =		 {10.1609/aaai.v33i01.33015377},
  number =	 {01},
  journal =	 {Proceedings of the AAAI Conference on Artificial Intelligence},
  author =	 {Wang, Yiren and Tian, Fei and He, Di and Qin, Tao and Zhai,
                  ChengXiang and Liu, Tie-Yan},
  year =	 {2019},
  month =	 {Jul.},
  pages =	 {5377-5384}
}

@misc{huang-etal-2021-nonautoregressive,
      title={Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision},
      author={Chenyang Huang and Hao Zhou and Osmar R. Zaïane and Lili Mou and Lei Li},
      year={2021},
      eprint={2110.07515},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{kaiser2018fast,
  title =	 {Fast Decoding in Sequence Models Using Discrete Latent
                  Variables},
  author =	 {Kaiser, Lukasz and Bengio, Samy and Roy, Aurko and Vaswani,
                  Ashish and Parmar, Niki and Uszkoreit, Jakob and Shazeer,
                  Noam},
  booktitle =	 {Proceedings of the 35th International Conference on Machine
                  Learning},
  pages =	 {2390--2399},
  year =	 2018,
  editor =	 {Jennifer Dy and Andreas Krause},
  volume =	 80,
  series =	 {Proceedings of Machine Learning Research},
  address =	 {Stockholmsmässan, Stockholm Sweden},
  month =	 {10--15 Jul},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v80/kaiser18a/kaiser18a.pdf},
  url =		 {http://proceedings.mlr.press/v80/kaiser18a.html},
  abstract =	 {Autoregressive sequence models based on deep neural networks,
                  such as RNNs, Wavenet and Transformer are the state-of-the-art
                  on many tasks. However, they lack parallelism and are thus
                  slow for long sequences. RNNs lack parallelism both during
                  training and decoding, while architectures like WaveNet and
                  Transformer are much more parallel during training, but still
                  lack parallelism during decoding. We present a method to
                  extend sequence models using discrete latent variables that
                  makes decoding much more parallel. The main idea behind this
                  approach is to first autoencode the target sequence into a
                  shorter discrete latent sequence, which is generated
                  autoregressively, and finally decode the full sequence from
                  this shorter latent sequence in a parallel manner. To this
                  end, we introduce a new method for constructing discrete
                  latent variables and compare it with previously introduced
                  methods. Finally, we verify that our model works on the task
                  of neural machine translation, where our models are an order
                  of magnitude faster than comparable autoregressive models and,
                  while lower in BLEU than purely autoregressive models, better
                  than previously proposed non-autogregressive translation.}
}

@InProceedings{chan2020imputer,
  title =	 {Imputer: Sequence Modelling via Imputation and Dynamic
                  Programming},
  author =	 {Chan, William and Saharia, Chitwan and Hinton, Geoffrey and
                  Norouzi, Mohammad and Jaitly, Navdeep},
  booktitle =	 {Proceedings of the 37th International Conference on Machine
                  Learning},
  pages =	 {1403--1413},
  year =	 {2020},
  editor =	 {Hal Daumé III and Aarti Singh},
  volume =	 {119},
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--18 Jul},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v119/chan20b/chan20b.pdf},
  url =		 { http://proceedings.mlr.press/v119/chan20b.html },
  abstract =	 {This paper presents the Imputer, a neural sequence model that
                  generates output sequences iteratively via imputations. The
                  Imputer is an iterative generation model, requiring only a
                  constant number of generation steps independent of the number
                  of input or output tokens. The Imputer can be trained to
                  approximately marginalize over all possible alignments between
                  the input and output sequences, and all possible generation
                  orders. We present a tractable dynamic programming training
                  algorithm, which yields a lower bound on the log marginal
                  likelihood. When applied to end-to-end speech recognition, the
                  Imputer outperforms prior non-autoregressive models and
                  achieves competitive results to autoregressive models. On
                  LibriSpeech test-other, the Imputer achieves 11.1 WER,
                  outperforming CTC at 13.0 WER and seq2seq at 12.5 WER.}
}

@InProceedings{kasai2020nonautoregressive,
  title =	 {Non-autoregressive Machine Translation with Disentangled
                  Context Transformer},
  author =	 {Kasai, Jungo and Cross, James and Ghazvininejad, Marjan and
                  Gu, Jiatao},
  booktitle =	 {Proceedings of the 37th International Conference on Machine
                  Learning},
  pages =	 {5144--5155},
  year =	 {2020},
  editor =	 {Hal Daumé III and Aarti Singh},
  volume =	 {119},
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--18 Jul},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v119/kasai20a/kasai20a.pdf},
  url =		 { http://proceedings.mlr.press/v119/kasai20a.html },
  abstract =	 {State-of-the-art neural machine translation models generate a
                  translation from left to right and every step is conditioned
                  on the previously generated tokens. The sequential nature of
                  this generation process causes fundamental latency in
                  inference since we cannot generate multiple tokens in each
                  sentence in parallel. We propose an attention-masking based
                  model, called Disentangled Context (DisCo) transformer, that
                  simultaneously generates all tokens given different
                  contexts. The DisCo transformer is trained to predict every
                  output token given an arbitrary subset of the other reference
                  tokens. We also develop the parallel easy-first inference
                  algorithm, which iteratively refines every token in parallel
                  and reduces the number of required iterations. Our extensive
                  experiments on 7 translation directions with varying data
                  sizes demonstrate that our model achieves competitive, if not
                  better, performance compared to the state of the art in
                  non-autoregressive machine translation while significantly
                  reducing decoding time on average.}
}

@article{ghazvininejad-etal-2020-semiautoregressive,
  title =	 {Semi-Autoregressive Training Improves Mask-Predict Decoding},
  author =	 {Marjan Ghazvininejad and Omer Levy and Luke Zettlemoyer},
  journal =	 {ArXiv},
  year =	 {2020},
  volume =	 {abs/2001.08785}
}

@article{shu2020latent,
  title =	 {Latent-Variable Non-Autoregressive Neural Machine Translation
                  with Deterministic Inference Using a Delta Posterior},
  volume =	 {34},
  url =		 {https://ojs.aaai.org/index.php/AAAI/article/view/6413},
  DOI =		 {10.1609/aaai.v34i05.6413},
  abstractNote = {&lt;p&gt;Although neural machine translation models reached
                  high translation quality, the autoregressive nature makes
                  inference difficult to parallelize and leads to high
                  translation latency. Inspired by recent refinement-based
                  approaches, we propose LaNMT, a latent-variable
                  non-autoregressive model with continuous latent variables and
                  deterministic inference procedure. In contrast to existing
                  approaches, we use a deterministic inference algorithm to find
                  the target sequence that maximizes the lowerbound to the
                  log-probability. During inference, the length of translation
                  automatically adapts itself. Our experiments show that the
                  lowerbound can be greatly increased by running the inference
                  algorithm, resulting in significantly improved translation
                  quality. Our proposed model closes the performance gap between
                  non-autoregressive and autoregressive approaches on ASPEC
                  Ja-En dataset with 8.6x faster decoding. On WMT’14 En-De
                  dataset, our model narrows the gap with autoregressive
                  baseline to 2.0 BLEU points with 12.5x speedup. By decoding
                  multiple initial latent variables in parallel and rescore
                  using a teacher model, the proposed model further brings the
                  gap down to 1.0 BLEU point on WMT’14 En-De task with 6.8x
                  speedup.&lt;/p&gt;},
  number =	 {05},
  journal =	 {Proceedings of the AAAI Conference on Artificial Intelligence},
  author =	 {Shu, Raphael and Lee, Jason and Nakayama, Hideki and Cho,
                  Kyunghyun},
  year =	 {2020},
  month =	 {Apr.},
  pages =	 {8846-8853}
}

@article{chan-etal-2019-kermit,
  author =	 {William Chan and Nikita Kitaev and Kelvin Guu and Mitchell
                  Stern and Jakob Uszkoreit},
  title =	 {{KERMIT:} Generative Insertion-Based Modeling for Sequences},
  journal =	 {CoRR},
  volume =	 {abs/1906.01604},
  year =	 {2019},
  url =		 {http://arxiv.org/abs/1906.01604},
  archivePrefix ={arXiv},
  eprint =	 {1906.01604},
  timestamp =	 {Thu, 13 Jun 2019 13:36:00 +0200},
  biburl =	 {https://dblp.org/rec/journals/corr/abs-1906-01604.bib},
  bibsource =	 {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gu-etal-2019-levenshtein,
  author =	 {Gu, Jiatao and Wang, Changhan and Zhao, Junbo},
  booktitle =	 {Advances in Neural Information Processing Systems},
  editor =	 {H. Wallach and H. Larochelle and A. Beygelzimer and
                  F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages =	 {11181--11191},
  publisher =	 {Curran Associates, Inc.},
  title =	 {Levenshtein Transformer},
  url =
                  {https://proceedings.neurips.cc/paper/2019/file/675f9820626f5bc0afb47b57890b466e-Paper.pdf},
  volume =	 {32},
  year =	 {2019}
}



@inproceedings{stern2018blockwise,
  author =	 {Stern, Mitchell and Shazeer, Noam and Uszkoreit, Jakob},
  booktitle =	 {Advances in Neural Information Processing Systems},
  editor =	 {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and
                  N. Cesa-Bianchi and R. Garnett},
  pages =	 {10086--10095},
  publisher =	 {Curran Associates, Inc.},
  title =	 {Blockwise Parallel Decoding for Deep Autoregressive Models},
  url =
                  {https://proceedings.neurips.cc/paper/2018/file/c4127b9194fe8562c64dc0f5bf2c93bc-Paper.pdf},
  volume =	 {31},
  year =	 {2018}
}


@InProceedings{stern-etal-2019-insertion,
  title =	 {Insertion Transformer: Flexible Sequence Generation via
                  Insertion Operations},
  author =	 {Stern, Mitchell and Chan, William and Kiros, Jamie and
                  Uszkoreit, Jakob},
  booktitle =	 {Proceedings of the 36th International Conference on Machine
                  Learning},
  pages =	 {5976--5985},
  year =	 2019,
  editor =	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume =	 97,
  series =	 {Proceedings of Machine Learning Research},
  month =	 {09--15 Jun},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v97/stern19a/stern19a.pdf},
  url =		 { http://proceedings.mlr.press/v97/stern19a.html },
  abstract =	 {We present the Insertion Transformer, an iterative, partially
                  autoregressive model for sequence generation based on
                  insertion operations. Unlike typical autoregressive models
                  which rely on a fixed, often left-to-right ordering of the
                  output, our approach accommodates arbitrary orderings by
                  allowing for tokens to be inserted anywhere in the sequence
                  during decoding. This flexibility confers a number of
                  advantages: for instance, not only can our model be trained to
                  follow specific orderings such as left-to-right generation or
                  a binary tree traversal, but it can also be trained to
                  maximize entropy over all valid insertions for robustness. In
                  addition, our model seamlessly accommodates both fully
                  autoregressive generation (one insertion at a time) and
                  partially autoregressive generation (simultaneous insertions
                  at multiple locations). We validate our approach by analyzing
                  its performance on the WMT 2014 English-German machine
                  translation task under various settings for training and
                  decoding. We find that the Insertion Transformer outperforms
                  many prior non-autoregressive approaches to translation at
                  comparable or better levels of parallelism, and successfully
                  recovers the performance of the original Transformer while
                  requiring only logarithmically many iterations during
                  decoding.}
}

@inproceedings{gu2017nonautoregressive,
  author =	 {Jiatao Gu and James Bradbury and Caiming Xiong and Victor
                  O. K. Li and Richard Socher},
  title =	 {Non-Autoregressive Neural Machine Translation},
  booktitle =	 {6th International Conference on Learning Representations,
                  {ICLR} 2018},
  month =	 apr,
  address =	 {Vancouver, BC, Canada},
  year =	 2018,
  url =		 {https://openreview.net/forum?id=B1l8BtlCb},
  timestamp =	 {Thu, 25 Jul 2019 14:25:57 +0200},
}

@article{mansimov2019generalized,
  author =	 {Elman Mansimov and Alex Wang and Kyunghyun Cho},
  title =	 {A Generalized Framework of Sequence Generation with
                  Application to Undirected Sequence Models},
  journal =	 {CoRR},
  volume =	 {abs/1905.12790},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1905.12790},
  archivePrefix ={arXiv},
  eprint =	 {1905.12790},
  timestamp =	 {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl =	 {https://dblp.org/rec/journals/corr/abs-1905-12790.bib},
  bibsource =	 {dblp computer science bibliography, https://dblp.org}
}
@article{reiter2018structured,
  author =	 {Reiter, Ehud},
  title =	 {A Structured Review of the Validity of BLEU},
  journal =	 {Computational Linguistics},
  volume =	 44,
  number =	 3,
  pages =	 {393-401},
  year =	 2018,
  doi =		 {10.1162/coli\_a\_00322},
  URL =		 {https://doi.org/10.1162/coli_a_00322},
  eprint =	 {ttps://doi.org/10.1162/coli_a_00322},
  abstract =	 { The BLEU metric has been widely used in NLP for over 15 years
                  to evaluate NLP systems, especially in machine translation and
                  natural language generation. I present a structured review of
                  the evidence on whether BLEU is a valid evaluation
                  technique—in other words, whether BLEU scores correlate with
                  real-world utility and user-satisfaction of NLP systems; this
                  review covers 284 correlations reported in 34 papers. Overall,
                  the evidence supports using BLEU for diagnostic evaluation of
                  MT systems (which is what it was originally proposed for), but
                  does not support using BLEU outside of MT, for evaluation of
                  individual texts, or for scientific hypothesis testing. }
}

@inproceedings{mikolov-etal-2013-distributed,
  title =	 {Distributed representations of words and phrases and their
                  compositionality},
  author =	 {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado,
                  Greg S and Dean, Jeff},
  booktitle =	 {Advances in neural information processing systems},
  pages =	 {3111--3119},
  year =	 2013
}

@article{bengio2003neural,
  title =	 {A neural probabilistic language model},
  author =	 {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal
                  and Jauvin, Christian},
  journal =	 {Journal of machine learning research},
  volume =	 3,
  number =	 {Feb},
  pages =	 {1137--1155},
  year =	 2003
}

@article{ba2016layer,
  author =	 {Ba, Lei Jimmy and Kiros, Ryan and Hinton, Geoffrey E},
  title =	 {Layer Normalization},
  journal =	 {CoRR},
  archiveprefix ={arXiv},
  eprint =	 {1607.06450},
  issn =	 {2331-8422},
  volume =	 {abs/1607.06450},
  year =	 2016
}

@article{bahdanau2014neural,
  author =	 {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  title =	 {Neural Machine Translation by Jointly Learning to Align and
                  Translate},
  journal =	 {CoRR},
  issn =	 {2331-8422},
  volume =	 {abs/1409.0473},
  year =	 2014
}

@inproceedings{bengio2007greedy,
  author =	 {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and
                  Larochelle, Hugo},
  title =	 {Greedy layer-wise training of deep networks},
  booktitle =	 {Advances in Neural Information Processing Systems 20},
  address =	 {Vancouver, Canada},
  isbn =	 9780262256919,
  month =	 {December},
  pages =	 {153--160},
  publisher =	 {Curran Associates, Inc.},
  year =	 2007
}

@article{bengio2013representation,
  author =	 {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  title =	 {Representation Learning: A Review and New Perspectives},
  journal =	 {IEEE Trans. Pattern Anal. Mach. Intell.},
  address =	 {Washington, D.C., USA},
  issn =	 {0162-8828},
  month =	 {August},
  number =	 8,
  pages =	 {1798--1828},
  publisher =	 {IEEE Computer Society},
  volume =	 35,
  year =	 2013
}

@article{conneau2017supervised,
  author =	 {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and
                  Barrault, Lo\"{i}c and Bordes, Antoine},
  title =	 {Supervised learning of universal sentence representations from
                  natural language inference data},
  journal =	 {CoRR},
  issn =	 {2331-8422},
  volume =	 {abs/1705.02364},
  year =	 2017
}

@article{conneau2017word,
  author =	 {Conneau, Alexis and Lample, Guillaume and Ranzato,
                  Marc'Aurelio and Denoyer, Ludovic and J{\'{e}}gou,
                  Herv{\'{e}}},
  title =	 {Word Translation Without Parallel Data},
  journal =	 {CoRR},
  archiveprefix ={arXiv},
  eprint =	 {1710.04087},
  issn =	 {2331-8422},
  volume =	 {abs/1710.04087},
  year =	 2017
}

@inproceedings{conneau2018senteval,
  author =	 {Conneau, Alexis and Kiela, Douwe},
  title =	 {{SentEval}: An Evaluation Toolkit for Universal Sentence
                  Representations},
  booktitle =	 {Proceedings of the Eleventh International Conference on
                  Language Resources and Evaluation (LREC)},
  address =	 {Miyazaki, Japan},
  isbn =	 {979-10-95546-00-9},
  language =	 {english},
  month =	 {May},
  pages =	 {1699--1704},
  publisher =	 {European Language Resources Association (ELRA)},
  year =	 2018
}

@article{elman1990finding,
  author =	 {Elman, Jeffrey L},
  title =	 {Finding Structure in Time},
  journal =	 {Cognitive Science},
  address =	 {Victoria, Canada},
  issn =	 {1551-6709},
  number =	 2,
  pages =	 {179--211},
  publisher =	 {Cognitive Science Society},
  volume =	 14,
  year =	 1990
}

@inproceedings{gehring2017convolutional,
  author =	 {Gehring, Jonas and Auli, Michael and Grangier, David and
                  Yarats, Denis and Dauphin, Yann N},
  title =	 {Convolutional Sequence to Sequence Learning},
  booktitle =	 {International Conference on Machine Learning},
  address =	 {Sydney, Australia},
  month =	 {August},
  pages =	 {1243--1252},
  publisher =	 {PMLR},
  year =	 2017
}

@article{graves2005framewise,
  author =	 {Graves, Alex and Schmidhuber, J\"{u}rgen},
  title =	 {Framewise phoneme classification with bidirectional LSTM and
                  other neural network architectures},
  journal =	 {Neural Networks},
  address =	 {Amsterdam, Netherlands},
  issn =	 {0893-6080},
  month =	 {Jul},
  number =	 {5-6},
  pages =	 {602--610},
  publisher =	 {Elsevier},
  volume =	 18,
  year =	 2005
}

@inproceedings{graves2006connectionist,
  author =	 {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino
                  and Schmidhuber, J\"{u}rgen},
  title =	 {Connectionist temporal classification: labelling unsegmented
                  sequence data with recurrent neural networks},
  booktitle =	 {Proceedings of the 23rd International Conference on Machine
                  Learning},
  address =	 {Pittsburgh, PA, USA},
  month =	 {June},
  pages =	 {369--376},
  publisher =	 {JMLR.org},
  year =	 2006
}

@inproceedings{graves2009offline,
  author =	 {Graves, Alex and Schmidhuber, J\"{u}rgen},
  title =	 {Offline Handwriting Recognition with Multidimensional
                  Recurrent Neural Networks},
  booktitle =	 {Advances in Neural Information Processing Systems 21},
  address =	 {Vancouver, Canada},
  month =	 {December},
  pages =	 {545--552},
  publisher =	 {Curran Associates, Inc.},
  year =	 2009
}

@inproceedings{graves2013speech,
  author =	 {Graves, Alex and Mohamed, Abdel{-}rahman and Hinton, Geoffrey
                  E},
  title =	 {Speech recognition with deep recurrent neural networks},
  booktitle =	 {International Conference on Acoustics, Speech and Signal
                  Processing ({ICASSP})},
  address =	 {Vancouver, Canada},
  isbn =	 9781479903566,
  month =	 {May},
  pages =	 {6645--6649},
  publisher =	 {{IEEE} Computer Society},
  year =	 2013
}

@article{helcl-libovicky-2017-neural,
  author =	 {Helcl, Jind\v{r}ich and Libovick\'{y}, Jind\v{r}ich},
  title =	 {Neural Monkey: An Open-source Tool for Sequence Learning},
  journal =	 {The Prague Bulletin of Mathematical Linguistics},
  address =	 {Prague, Czech Republic},
  issn =	 {0032-6585},
  month =	 {Apr},
  number =	 1,
  pages =	 {5--17},
  publisher =	 {Charles University},
  volume =	 107,
  year =	 2017
}

@article{hochreiter1997long,
  author =	 {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
  title =	 {Long short-term memory},
  journal =	 {Neural Computation},
  address =	 {Cambridge, MA, USA},
  issn =	 {0899-7667},
  number =	 8,
  pages =	 {1735--1780},
  publisher =	 {MIT Press},
  volume =	 9,
  year =	 1997
}

@inproceedings{kingma2014semi,
  author =	 {Kingma, Diederik P and Mohamed, Shakir and Rezende, Danilo
                  Jimenez and Welling, Max},
  title =	 {Semi-supervised Learning with Deep Generative Models},
  booktitle =	 {Advances in Neural Information Processing Systems 27},
  address =	 {Montreal, Canada},
  month =	 {December},
  pages =	 {3581--3589},
  publisher =	 {Curran Associates, Inc.},
  year =	 2014
}

@article{kingma2014adam,
  author =	 {Kingma, Diederik P and Ba, Jimmy},
  title =	 {Adam: {A} Method for Stochastic Optimization},
  journal =	 {CoRR},
  issn =	 {2331-8422},
  volume =	 {abs/1412.6980},
  year =	 2014
}

@inproceedings{krizhevsky2012imagenet,
  author =	 {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  title =	 {Imagenet classification with deep convolutional neural
                  networks},
  booktitle =	 {Advances in Neural Information Processing Systems 25},
  address =	 {Red Hook, NY, USA},
  isbn =	 9781627480031,
  issn =	 {0001-0782},
  month =	 {May},
  number =	 6,
  pages =	 {1097--1105},
  publisher =	 {Curran Associates, Inc.},
  volume =	 60,
  year =	 2012
}

@inproceedings{kulkarni2011baby,
  author =	 {Kulkarni, Girish and Premraj, Visruth and Dhar, Sagnik and Li,
                  Siming and Choi, Yejin and Berg, Alexander C and Berg, Tamara
                  L},
  title =	 {Baby talk: Understanding and generating simple image
                  descriptions.},
  booktitle =	 {Conference on Computer Vision and Pattern Recognition
                  ({CVPR})},
  address =	 {Colorado Sprtings, CO, USA},
  isbn =	 {978-1-4577-0394-2},
  month =	 {June},
  pages =	 {1601-1608},
  publisher =	 {{IEEE} Computer Society},
  year =	 2011
}

@inproceedings{lafferty2001conditional,
  author =	 {Lafferty, John and McCallum, Andrew and Pereira, Fernando},
  title =	 {Conditional Random Fields: Probabilistic Models for Segmenting
                  and Labeling Sequence Data},
  booktitle =	 {Proceedings of the 18th International Conference on Machine
                  Learning},
  address =	 {Williamstown, MA, USA},
  isbn =	 {1-55860-778-1},
  month =	 {June},
  pages =	 {282--289},
  publisher =	 {Morgan Kaufmann},
  year =	 2001
}

@article{srivastava2014dropout,
  author =	 {Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex
                  and Sutskever, Ilya and Salakhutdinov, Ruslan},
  title =	 {Dropout: A simple way to prevent neural networks from
                  overfitting},
  journal =	 {The Journal of Machine Learning Research},
  address =	 {Brookline, MA, USA},
  issn =	 {1532-4435},
  number =	 1,
  pages =	 {1929--1958},
  publisher =	 {JMLR. org},
  volume =	 15,
  year =	 2014
}

@inproceedings{sutskever2014sequence,
  author =	 {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  title =	 {Sequence to Sequence Learning with Neural Networks},
  booktitle =	 {Advances in Neural Information Processing Systems 27},
  address =	 {Montreal, Canada},
  month =	 {December},
  pages =	 {3104--3112},
  publisher =	 {Curran Associates, Inc.},
  year =	 2014
}

@inproceedings{szegedy2015going,
  author =	 {Szegedy, Christian and Liu, Wei and Jia, Yangqing and
                  Sermanet, Pierre and Reed, Scott E and Anguelov, Dragomir and
                  Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title =	 {Going deeper with convolutions},
  booktitle =	 {Conference on Computer Vision and Pattern Recognition
                  ({CVPR})},
  address =	 {Boston, MA, USA},
  isbn =	 9781467369640,
  month =	 {June},
  pages =	 {1--9},
  publisher =	 {{IEEE} Computer Society},
  year =	 2015
}


@inproceedings{vaswani2017attention,
  author =	 {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                  Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                  Kaiser, {\L}ukasz and Polosukhin, Illia},
  title =	 {Attention is all you need},
  booktitle =	 {Advances in Neural Information Processing Systems 30},
  address =	 {Long Beach, CA, USA},
  month =	 {December},
  pages =	 {6000--6010},
  publisher =	 {Curran Associates, Inc.},
  year =	 2017
}

@inproceedings{schuster-nakajima-2012-japanese,
  author={Schuster, Mike and Nakajima, Kaisuke},
  booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal
                  Processing (ICASSP)},
  title={Japanese and Korean voice search},
  year={2012},
  volume={},
  number={},
  pages={5149-5152},
  doi={10.1109/ICASSP.2012.6289079}
}


@article{wu2016google,
  author =	 {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc
                  V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun,
                  Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and
                  Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu,
                  Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato,
                  Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith
                  and Kurian, George and Patil, Nishant and Wang, Wei and Young,
                  Cliff and Smith, Jason R and Riesa, Jason and Rudnick, Alex
                  and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and
                  Dean, Jeffrey},
  title =	 {Google's Neural Machine Translation System: Bridging the Gap
                  between Human and Machine Translation},
  journal =	 {CoRR},
  archiveprefix ={arXiv},
  eprint =	 {1609.08144},
  issn =	 {2331-8422},
  volume =	 {abs/1609.08144},
  year =	 2016
}
