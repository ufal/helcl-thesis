@misc{bogoychev2023opuscleaner,
      title={OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models},
      author={Nikolay Bogoychev and Jelmer van der Linde and Graeme Nail and Barry Haddow and Jaume Zaragoza-Bernabeu and Gema Ramírez-Sánchez and Lukas Weymann and Tudor Nicolae Mateiu and Jindřich Helcl and Mikko Aulamo},
      year={2023},
      eprint={2311.14838},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{helcl-libovicky-2023-cuni,
    title = "{CUNI} Submission to {MRL} 2023 Shared Task on Multi-lingual Multi-task Information Retrieval",
    author = "Helcl, Jind{\v{r}}ich  and
      Libovick{\'y}, Jind{\v{r}}ich",
    editor = "Ataman, Duygu",
    booktitle = "Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.mrl-1.23",
    doi = "10.18653/v1/2023.mrl-1.23",
    pages = "302--309",
}


@inproceedings{helcl-2022-cuni,
    title = "{CUNI} Non-Autoregressive System for the {WMT} 22 Efficient Translation Shared Task",
    author = "Helcl, Jind{\v{r}}ich",
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.64",
    pages = "668--670",
    abstract = "We present a non-autoregressive system submission to the WMT 22 Efficient Translation Shared Task. Our system was used by Helcl et al. (2022) in an attempt to provide fair comparison between non-autoregressive and autoregressive models. This submission is an effort to establish solid baselines along with sound evaluation methodology, particularly in terms of measuring the decoding speed. The model itself is a 12-layer Transformer model trained with connectionist temporal classification on knowledge-distilled dataset by a strong autoregressive teacher model.",
}


@inproceedings{popel-etal-2022-cuni,
    title = "{CUNI} Systems for the {WMT} 22 {C}zech-{U}krainian Translation Task",
    author = "Popel, Martin  and
      Libovick{\'y}, Jind{\v{r}}ich  and
      Helcl, Jind{\v{r}}ich",
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.30",
    pages = "352--357",
    abstract = "We present Charles University submissions to the WMT 22 GeneralTranslation Shared Task on Czech-Ukrainian and Ukrainian-Czech machine translation. We present two constrained submissions based on block back-translation and tagged back-translation and experiment with rule-basedromanization of Ukrainian. Our results show that the romanization onlyhas a minor effect on the translation quality. Further, we describe Charles Translator,a system that was developed in March 2022 as a response to the migrationfrom Ukraine to the Czech Republic. Compared to our constrained systems,it did not use the romanization and used some proprietary data sources.",
}


@article{haddow-etal-2022-survey,
    title = "Survey of Low-Resource Machine Translation",
    author = "Haddow, Barry  and
      Bawden, Rachel  and
      Miceli Barone, Antonio Valerio  and
      Helcl, Jind{\v{r}}ich  and
      Birch, Alexandra",
    journal = "Computational Linguistics",
    volume = "48",
    number = "3",
    month = sep,
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.cl-3.6",
    doi = "10.1162/coli_a_00446",
    pages = "673--732",
    abstract = "We present a survey covering the state of the art in low-resource machine translation (MT) research. There are currently around 7,000 languages spoken in the world and almost all language pairs lack significant resources for training machine translation models. There has been increasing interest in research addressing the challenge of producing useful translation models when very little translated training data is available. We present a summary of this topical research field and provide a description of the techniques evaluated by researchers in several recent shared tasks in low-resource MT.",
}


@inproceedings{helcl-etal-2022-non,
    title = "Non-Autoregressive Machine Translation: It{'}s Not as Fast as it Seems",
    author = "Helcl, Jind{\v{r}}ich  and
      Haddow, Barry  and
      Birch, Alexandra",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.129",
    doi = "10.18653/v1/2022.naacl-main.129",
    pages = "1780--1790",
    abstract = "Efficient machine translation models are commercially important as they can increase inference speeds, and reduce costs and carbon emissions. Recently, there has been much interest in non-autoregressive (NAR) models, which promise faster translation. In parallel to the research on NAR models, there have been successful attempts to create optimized autoregressive models as part of the WMT shared task on efficient translation. In this paper, we point out flaws in the evaluation methodology present in the literature on NAR models and we provide a fair comparison between a state-of-the-art NAR model and the autoregressive submissions to the shared task. We make the case for consistent evaluation of NAR models, and also for the importance of comparing NAR models with other widely used methods for improving efficiency. We run experiments with a connectionist-temporal-classification-based (CTC) NAR model implemented in C++ and compare it with AR models using wall clock times. Our results show that, although NAR models are faster on GPUs, with small batch sizes, they are almost always slower under more realistic usage conditions. We call for more realistic and extensive evaluation of NAR models in future work.",
}


@inproceedings{Behnke-wmt21-speed,
  author = {Maximiliana Behnke and Nikolay Bogoychev and Alham Fikri Aji and Kenneth Heafield and Graeme Nail and Qianqian Zhu and Svetlana Tchistiakova and Jelmer van der Linde and Pinzhen Chen and Sidharth Kashyap and Roman Grundkiewicz},
  title = {Efficient Machine Translation with Model Pruning and Quantization},
  year = {2021},
  month = nov,
  month_numeric = {11},
  booktitle = {Proceedings of the Conference on Machine Translation at the 2021 Conference on Empirical Methods in Natural Language Processing},
  address = {Punta Cana, Dominican Republic},
  url = {https://kheafield.com/papers/edinburgh/wmt21-speed.pdf}
}

@InProceedings{akhbardeh-etal-2021-findings,
  author    = {Akhbardeh, Farhad  and  Arkhangorodsky, Arkady  and  Biesialska, Magdalena  and  Bojar, Ondřej  and  Chatterjee, Rajen  and  Chaudhary, Vishrav  and  Costa-jussa, Marta R.  and  España-Bonet, Cristina  and  Fan, Angela  and  Federmann, Christian  and  Freitag, Markus  and  Graham, Yvette  and  Grundkiewicz, Roman  and  Haddow, Barry  and  Harter, Leonie  and  Heafield, Kenneth  and  Homan, Christopher  and  Huck, Matthias  and  Amponsah-Kaakyire, Kwabena  and  Kasai, Jungo  and  Khashabi, Daniel  and  Knight, Kevin  and  Kocmi, Tom  and  Koehn, Philipp  and  Lourie, Nicholas  and  Monz, Christof  and  Morishita, Makoto  and  Nagata, Masaaki  and  Nagesh, Ajay  and  Nakazawa, Toshiaki  and  Negri, Matteo  and  Pal, Santanu  and  Tapo, Allahsera Auguste  and  Turchi, Marco  and  Vydrin, Valentin  and  Zampieri, Marcos},
  title     = {Findings of the 2021 Conference on Machine Translation (WMT21)},
  booktitle      = {Proceedings of the Sixth Conference on Machine Translation},
  month          = {November},
  year           = {2021},
  address        = {Online},
  publisher      = {Association for Computational Linguistics},
  pages     = {1--93},
  url       = {https://aclanthology.org/2021.wmt-1.1}
}

@article{haddow2021survey,
title={Survey of Low-Resource Machine Translation},
  author={Haddow, Barry and Bawden, Rachel and Barone, Antonio Valerio Miceli and Helcl, Jind{\v{r}}ich and Birch, Alexandra},
  journal={arXiv preprint arXiv:2109.00486},
  year={2021}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}
@article{dostert1955georgetown,
  title={The Georgetown-{IBM} Experiment},
  author={Dostert, Leon E},
  journal={1955). Machine translation of languages. John Wiley \& Sons, New York},
  pages={124--135},
  year={1955}
}
@article{dupont2017cryptological,
author = {Dupont, Quinn},
year = {2017},
month = {01},
pages = {},
title = {The Cryptological Origins of Machine Translation, from al-Kindi to Weaver},
journal = {amodern}
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}
@article{nagao1984framework,
  title={A framework of a mechanical translation between Japanese and English by analogy principle},
  author={Nagao, Makoto},
  journal={Artificial and human intelligence},
  pages={351--354},
  year={1984}
}
@inproceedings{libovicky2018solving,
  title={Solving Three Czech NLP Tasks with End-to-end Neural Models.},
  author={Libovick{\`y}, Jindrich and Rosa, Rudolf and Helcl, Jindrich and Popel, Martin},
  booktitle={ITAT},
  pages={138--143},
  year={2018}
}
@article{bojar2016ufal,
  title={UFAL submissions to the IWSLT 2016 MT track},
  author={Bojar, Ondrej and Sudarikov, Roman and Kocmi, Tom and Helcl, Jindrich and C{\i}fka, Ondrej},
  journal={IWSLT. Seattle, WA},
  year={2016}
}

@article{kocmi2021ship,
  title={To ship or not to ship: An extensive evaluation of automatic metrics for machine translation},
  author={Kocmi, Tom and Federmann, Christian and Grundkiewicz, Roman and Junczys-Dowmunt, Marcin and Matsushita, Hitokazu and Menezes, Arul},
  journal={arXiv preprint arXiv:2107.10821},
  year={2021}
}

@article{daume2009search,
  title={Search-based structured prediction},
  author={Daum{\'e}, Hal and Langford, John and Marcu, Daniel},
  journal={Machine learning},
  volume={75},
  number={3},
  pages={297--325},
  year={2009},
  publisher={Springer}
}

@article{gers2000learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  journal={Neural computation},
  volume={12},
  number={10},
  pages={2451--2471},
  year={2000},
  publisher={MIT Press}
}

@inproceedings{heafield-etal-2021-findings,
  author = {Kenneth Heafield and Qianqian Zhu and Roman Grundkiewicz},
  title = {Findings of the {WMT} 2021 Shared Task on Efficient Translation},
  year = {2021},
  month = nov,
  month_numeric = {11},
  booktitle = {Proceedings of the Conference on Machine Translation at the 2021 Conference on Empirical Methods in Natural Language Processing},
  address = {Punta Cana, Dominican Republic},
  url = {https://kheafield.com/papers/edinburgh/wmt21-speedtask.pdf}
}

@misc{qian2021volctrans,
      title={The Volctrans GLAT System: Non-autoregressive Translation Meets WMT21},
      author={Lihua Qian and Yi Zhou and Zaixiang Zheng and Yaoming Zhu and Zehui Lin and Jiangtao Feng and Shanbo Cheng and Lei Li and Mingxuan Wang and Hao Zhou},
      year={2021},
      eprint={2109.11247},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{kuhn1955hungarian,
  title={The Hungarian method for the assignment problem},
  author={Kuhn, Harold W},
  journal={Naval research logistics quarterly},
  volume={2},
  number={1-2},
  pages={83--97},
  year={1955},
  publisher={Wiley Online Library}
}


@article{popel-bojar-2018-training,
 journal = {The Prague Bulletin of Mathematical Linguistics},
 title = {{Training Tips for the Transformer Model}},
 author = {Martin Popel and Ond{\v{r}}ej Bojar},
 year = {2018},
 month = {April},
 volume = {110},
 pages = {43--70},
 doi = {10.2478/pralin-2018-0002},
 issn = {0032-6585},
 url = {https://ufal.mff.cuni.cz/pbml/110/art-popel-bojar.pdf}
}

@inproceedings{zhou-etal-2020-understanding,
  author    = {Chunting Zhou and
               Jiatao Gu and
               Graham Neubig},
  title     = {Understanding Knowledge Distillation in Non-autoregressive Machine
               Translation},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=BygFVAEKDH},
  timestamp = {Thu, 07 May 2020 17:11:47 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ZhouGN20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{rezende2015variational,
  title={Variational Inference with Normalizing Flows},
  author={Rezende, Danilo and Mohamed, Shakir},
  booktitle={International conference on machine learning},
  pages={1530--1538},
  year={2015},
  organization={PMLR}
}

@misc{du2021orderagnostic,
      title={Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation},
      author={Cunxiao Du and Zhaopeng Tu and Jing Jiang},
      year={2021},
      eprint={2106.05093},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{turing1950computing,
    author = {Turing, A. M.},
    title = "{Computing Machinery and Intelligence}",
    journal = {Mind},
    volume = {LIX},
    number = {236},
    pages = {433-460},
    year = {1950},
    month = {10},
    issn = {0026-4423},
    doi = {10.1093/mind/LIX.236.433},
    url = {https://doi.org/10.1093/mind/LIX.236.433},
}

@inproceedings{chen-etal-2021-university,
  title = {The {U}niversity of {E}dinburgh’s {E}nglish-{G}erman and
                  {E}nglish-{H}ausa Submissions to the {WMT}21 News Translation
                  Task},
  author = {Pinzhen Chen and Jindřich Helcl and Ulrich Germann and Laurie
                  Burchell and Nikolay Bogoychev and Antonio Valerio Miceli
                  Barone and Jonas Waldendorf and Alexandra Birch and Kenneth
                  Heafield},
  year = 2021,
  publisher = "Association for Computational Linguistics",
  booktitle={Proceedings of the Sixth Conference on Machine Translation},
}


@book{koehn2009statistical,
    author        = {Koehn, Philipp},
    title         = {Statistical Machine Translation},
    address       = {Cambridge, UK},
    isbn          = {978-0521874151},
    publisher     = {Cambridge University Press},
    url           = {http://www.statmt.org/book/},
    year          = {2009}
}

@article{popel-etal-2020-transforming,
  journal = {Nature Communications},
  title = {Transforming Machine Translation: A Deep Learning System Reaches News Translation Quality Comparable to Human Professionals},
  author = {Martin Popel and Marketa Tomkova and Jakub Tomek and {\L}ukasz Kaiser and Jakob Uszkoreit and Ond{\v{r}}ej Bojar and Zden{\v{e}}k {\v{Z}}abokrtsk{\'{y}}},
  year = {2020},
  volume = {11},
  number = {4381},
  pages = {1--15},
  issn = {2041-1723},
  doi={10.1038/s41467-020-18073-9},
  url={https://www.nature.com/articles/s41467-020-18073-9},
}


@article{duchi2011adaptive,
  title={Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{zeiler2012adadelta,
  title={Adadelta: An Adaptive Learning Rate Method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}

@misc{tieleman2012lecture,
  title={{Lecture 6.5---RmsProp: Divide the Gradient by a Running Average of its Recent Magnitude}},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}

@inproceedings{amodei-etal-2016-deep,
  title={Deep Speech 2: End-to-end Speech Recognition in English and Mandarin},
  author={Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and
                  Bai, Jingliang and Battenberg, Eric and Case, Carl and
                  Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen,
                  Guoliang and others},
  booktitle={International conference on machine learning},
  pages={173--182},
  year={2016},
  organization={PMLR}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}


@article{conneau-lample-2019-cross,
  title={Cross-Lingual Language Model Pretraining},
  author={Conneau, Alexis and Lample, Guillaume},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={7059--7069},
  year={2019}
}


@article{joulin2016fasttext,
  title={FastText.zip: Compressing Text Classification Models},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze,
                  Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1612.03651},
  year={2016}
}


@InProceedings{glorot2010understanding,
  title = 	 {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html}
 }

@inproceedings{eyben2009speech,
  title={From Speech to Letters -- Using a Novel Neural Network Architecture
                  for Grapheme Based {ASR}},
  author={Eyben, Florian and W{\"o}llmer, Martin and Schuller, Bj{\"o}rn and Graves, Alex},
  booktitle={2009 IEEE Workshop on Automatic Speech Recognition \& Understanding},
  pages={376--380},
  year={2009},
  organization={IEEE}
}

@inproceedings{graves2014towards,
  title={Towards End-to-End Speech Recognition with Recurrent Neural Networks},
  author={Graves, Alex and Jaitly, Navdeep},
  booktitle={International conference on machine learning},
  pages={1764--1772},
  year={2014},
  organization={PMLR}
}

@inproceedings{liwicki2007novel,
  title={A Novel Approach to On-Line Handwriting Recognition Based on
                  Bidirectional Long Short-Term Memory Networks},
  author={Liwicki, Marcus and Graves, Alex and Fern{\`a}ndez, Santiago and
                  Bunke, Horst and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 9th International Conference on Document
                  Analysis and Recognition, ICDAR 2007},
  year={2007}
}

@inproceedings{ranzato2016sequence,
  author    = {Marc'Aurelio Ranzato and
               Sumit Chopra and
               Michael Auli and
               Wojciech Zaremba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Sequence Level Training with Recurrent Neural Networks},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.06732},
  timestamp = {Thu, 25 Jul 2019 14:25:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RanzatoCAZ15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{williams1992simple,
  title={Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{bengio2015scheduled,
  title={Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks},
  author={Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  booktitle={Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1},
  pages={1171--1179},
  year={2015}
}


@article{schwenk2019wikimatrix,
  title={Wikimatrix: Mining 135{M} Parallel Sentences in 1620 Language Pairs from Wikipedia},
  author={Schwenk, Holger and Chaudhary, Vishrav and Sun, Shuo and Gong, Hongyu and Guzm{\'a}n, Francisco},
  journal={arXiv preprint arXiv:1907.05791},
  year={2019}
}

@book{zipf1949human,
  author = {Zipf, George Kingsley},
  title = {Human Behavior and the Principle of Least Effort},
  publisher = {Addison-Wesley},
  year = 1949
}

@mastersthesis{kasner2020incorporating,
  title={Incorporating Language Models into Non-autoregressive Neural Machine
         Translation},
  author={Zdeněk Kasner},
  year={2020},
  school={Czech Technical University},
}

@misc{kasner2020improving,
      title={Improving Fluency of Non-Autoregressive Machine Translation},
      author={Zdeněk Kasner and Jindřich Libovický and Jindřich Helcl},
      year={2020},
      eprint={2004.03227},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{sun2020em,
  title =	 {An {EM} Approach to Non-Autoregressive Conditional Sequence
                  Generation},
  author =	 {Sun, Zhiqing and Yang, Yiming},
  booktitle =	 {Proceedings of the 37th International Conference on Machine
                  Learning},
  pages =	 {9249--9258},
  year =	 {2020},
  editor =	 {Hal Daumé III and Aarti Singh},
  volume =	 {119},
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--18 Jul},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v119/sun20c/sun20c.pdf},
  url =		 { http://proceedings.mlr.press/v119/sun20c.html },
  abstract =	 {Autoregressive (AR) models have been the dominating approach
                  to conditional sequence generation, but are suffering from the
                  issue of high inference latency. Non-autoregressive (NAR)
                  models have been recently proposed to reduce the latency by
                  generating all output tokens in parallel but could only
                  achieve inferior accuracy compared to their autoregressive
                  counterparts, primarily due to a difficulty in dealing with
                  the multi-modality in sequence generation. This paper proposes
                  a new approach that jointly optimizes both AR and NAR models
                  in a unified Expectation-Maximization (EM) framework. In the
                  E-step, an AR model learns to approximate the regularized
                  posterior of the NAR model. In the M-step, the NAR model is
                  updated on the new posterior and selects the training examples
                  for the next AR model. This iterative process can effectively
                  guide the system to remove the multi-modality in the output
                  sequences. To our knowledge, this is the first EM approach to
                  NAR sequence generation. We evaluate our method on the task of
                  machine translation. Experimental results on benchmark data
                  sets show that the proposed approach achieves competitive, if
                  not better, performance with existing NAR models and
                  significantly reduces the inference latency.}
}

@InProceedings{ghazvininejad2020aligned,
  title =	 {Aligned Cross Entropy for Non-Autoregressive Machine
                  Translation},
  author =	 {Ghazvininejad, Marjan and Karpukhin, Vladimir and Zettlemoyer,
                  Luke and Levy, Omer},
  booktitle =	 {Proceedings of the 37th International Conference on Machine
                  Learning},
  pages =	 {3515--3523},
  year =	 {2020},
  editor =	 {Hal Daumé III and Aarti Singh},
  volume =	 {119},
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--18 Jul},
  publisher =	 {PMLR},
  pdf =
                  {http://proceedings.mlr.press/v119/ghazvininejad20a/ghazvininejad20a.pdf},
  url =		 { http://proceedings.mlr.press/v119/ghazvininejad20a.html },
  abstract =	 {Non-autoregressive machine translation models significantly
                  speed up decoding by allowing for parallel prediction of the
                  entire target sequence. However, modeling word order is more
                  challenging due to the lack of autoregressive factors in the
                  model. This difficultly is compounded during training with
                  cross entropy loss, which can highly penalize small shifts in
                  word order. In this paper, we propose aligned cross entropy
                  (AXE) as an alternative loss function for training of
                  non-autoregressive models. AXE uses a differentiable dynamic
                  program to assign loss based on the best possible monotonic
                  alignment between target tokens and model
                  predictions. AXE-based training of conditional masked language
                  models (CMLMs) substantially improves performance on major WMT
                  benchmarks, while setting a new state of the art for
                  non-autoregressive models.}
}


@article{ran-etal-2021-guiding,
  title={Guiding Non-Autoregressive Neural Machine Translation Decoding with
                  Reordering Information},
  volume={35},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/17618},
  number={15},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Ran, Qiu and Lin, Yankai and Li, Peng and Zhou, Jie},
  year={2021},
  month={May},
  pages={13727-13735}}


@inproceedings{sun2019fast,
  author =	 {Sun, Zhiqing and Li, Zhuohan and Wang, Haoqing and He, Di and
                  Lin, Zi and Deng, Zhihong},
  booktitle =	 {Advances in Neural Information Processing Systems},
  editor =	 {H. Wallach and H. Larochelle and A. Beygelzimer and
                  F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages =	 {3016--3026},
  publisher =	 {Curran Associates, Inc.},
  title =	 {Fast Structured Decoding for Sequence Models},
  url =
                  {https://proceedings.neurips.cc/paper/2019/file/74563ba21a90da13dacf2a73e3ddefa7-Paper.pdf},
  volume =	 {32},
  year =	 {2019}
}

@article{shao2020minimizing,
  title =	 {Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive
                  Neural Machine Translation},
  volume =	 {34},
  url =		 {https://ojs.aaai.org/index.php/AAAI/article/view/5351},
  DOI =		 {10.1609/aaai.v34i01.5351},
  abstractNote = {&lt;p&gt;Non-Autoregressive Neural Machine Translation (NAT)
                  achieves significant decoding speedup through generating
                  target words independently and simultaneously. However, in the
                  context of non-autoregressive translation, the word-level
                  cross-entropy loss cannot model the target-side sequential
                  dependency properly, leading to its weak correlation with the
                  translation quality. As a result, NAT tends to generate
                  influent translations with over-translation and
                  under-translation errors. In this paper, we propose to train
                  NAT to minimize the Bag-of-Ngrams (BoN) difference between the
                  model output and the reference sentence. The bag-of-ngrams
                  training objective is differentiable and can be efficiently
                  calculated, which encourages NAT to capture the target-side
                  sequential dependency and correlates well with the translation
                  quality. We validate our approach on three translation tasks
                  and show that our approach largely outperforms the NAT
                  baseline by about 5.0 BLEU scores on WMT14 En↔De and about 2.5
                  BLEU scores on WMT16 En↔Ro.&lt;/p&gt;},
  number =	 {01},
  journal =	 {Proceedings of the AAAI Conference on Artificial Intelligence},
  author =	 {Shao, Chenze and Zhang, Jinchao and Feng, Yang and Meng,
                  Fandong and Zhou, Jie},
  year =	 {2020},
  month =	 {Apr.},
  pages =	 {198-205}
}

@article{wang-etal-2019-nonautoregressive,
  title =	 {Non-Autoregressive Machine Translation with Auxiliary
                  Regularization},
  volume =	 {33},
  url =		 {https://ojs.aaai.org/index.php/AAAI/article/view/4476},
  DOI =		 {10.1609/aaai.v33i01.33015377},
  number =	 {01},
  journal =	 {Proceedings of the AAAI Conference on Artificial Intelligence},
  author =	 {Wang, Yiren and Tian, Fei and He, Di and Qin, Tao and Zhai,
                  ChengXiang and Liu, Tie-Yan},
  year =	 {2019},
  month =	 {Jul.},
  pages =	 {5377-5384}
}

@misc{huang-etal-2021-nonautoregressive,
      title={Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision},
      author={Chenyang Huang and Hao Zhou and Osmar R. Zaïane and Lili Mou and Lei Li},
      year={2021},
      eprint={2110.07515},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{kaiser2018fast,
  title =	 {Fast Decoding in Sequence Models Using Discrete Latent
                  Variables},
  author =	 {Kaiser, Lukasz and Bengio, Samy and Roy, Aurko and Vaswani,
                  Ashish and Parmar, Niki and Uszkoreit, Jakob and Shazeer,
                  Noam},
  booktitle =	 {Proceedings of the 35th International Conference on Machine
                  Learning},
  pages =	 {2390--2399},
  year =	 2018,
  editor =	 {Jennifer Dy and Andreas Krause},
  volume =	 80,
  series =	 {Proceedings of Machine Learning Research},
  address =	 {Stockholmsmässan, Stockholm Sweden},
  month =	 {10--15 Jul},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v80/kaiser18a/kaiser18a.pdf},
  url =		 {http://proceedings.mlr.press/v80/kaiser18a.html},
  abstract =	 {Autoregressive sequence models based on deep neural networks,
                  such as RNNs, Wavenet and Transformer are the state-of-the-art
                  on many tasks. However, they lack parallelism and are thus
                  slow for long sequences. RNNs lack parallelism both during
                  training and decoding, while architectures like WaveNet and
                  Transformer are much more parallel during training, but still
                  lack parallelism during decoding. We present a method to
                  extend sequence models using discrete latent variables that
                  makes decoding much more parallel. The main idea behind this
                  approach is to first autoencode the target sequence into a
                  shorter discrete latent sequence, which is generated
                  autoregressively, and finally decode the full sequence from
                  this shorter latent sequence in a parallel manner. To this
                  end, we introduce a new method for constructing discrete
                  latent variables and compare it with previously introduced
                  methods. Finally, we verify that our model works on the task
                  of neural machine translation, where our models are an order
                  of magnitude faster than comparable autoregressive models and,
                  while lower in BLEU than purely autoregressive models, better
                  than previously proposed non-autogregressive translation.}
}

@InProceedings{chan2020imputer,
  title =	 {Imputer: Sequence Modelling via Imputation and Dynamic
                  Programming},
  author =	 {Chan, William and Saharia, Chitwan and Hinton, Geoffrey and
                  Norouzi, Mohammad and Jaitly, Navdeep},
  booktitle =	 {Proceedings of the 37th International Conference on Machine
                  Learning},
  pages =	 {1403--1413},
  year =	 {2020},
  editor =	 {Hal Daumé III and Aarti Singh},
  volume =	 {119},
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--18 Jul},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v119/chan20b/chan20b.pdf},
  url =		 { http://proceedings.mlr.press/v119/chan20b.html },
  abstract =	 {This paper presents the Imputer, a neural sequence model that
                  generates output sequences iteratively via imputations. The
                  Imputer is an iterative generation model, requiring only a
                  constant number of generation steps independent of the number
                  of input or output tokens. The Imputer can be trained to
                  approximately marginalize over all possible alignments between
                  the input and output sequences, and all possible generation
                  orders. We present a tractable dynamic programming training
                  algorithm, which yields a lower bound on the log marginal
                  likelihood. When applied to end-to-end speech recognition, the
                  Imputer outperforms prior non-autoregressive models and
                  achieves competitive results to autoregressive models. On
                  LibriSpeech test-other, the Imputer achieves 11.1 WER,
                  outperforming CTC at 13.0 WER and seq2seq at 12.5 WER.}
}

@InProceedings{kasai2020nonautoregressive,
  title =	 {Non-Autoregressive Machine Translation with Disentangled
                  Context Transformer},
  author =	 {Kasai, Jungo and Cross, James and Ghazvininejad, Marjan and
                  Gu, Jiatao},
  booktitle =	 {Proceedings of the 37th International Conference on Machine
                  Learning},
  pages =	 {5144--5155},
  year =	 {2020},
  editor =	 {Hal Daumé III and Aarti Singh},
  volume =	 {119},
  series =	 {Proceedings of Machine Learning Research},
  month =	 {13--18 Jul},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v119/kasai20a/kasai20a.pdf},
  url =		 { http://proceedings.mlr.press/v119/kasai20a.html },
  abstract =	 {State-of-the-art neural machine translation models generate a
                  translation from left to right and every step is conditioned
                  on the previously generated tokens. The sequential nature of
                  this generation process causes fundamental latency in
                  inference since we cannot generate multiple tokens in each
                  sentence in parallel. We propose an attention-masking based
                  model, called Disentangled Context (DisCo) transformer, that
                  simultaneously generates all tokens given different
                  contexts. The DisCo transformer is trained to predict every
                  output token given an arbitrary subset of the other reference
                  tokens. We also develop the parallel easy-first inference
                  algorithm, which iteratively refines every token in parallel
                  and reduces the number of required iterations. Our extensive
                  experiments on 7 translation directions with varying data
                  sizes demonstrate that our model achieves competitive, if not
                  better, performance compared to the state of the art in
                  non-autoregressive machine translation while significantly
                  reducing decoding time on average.}
}

@article{ghazvininejad-etal-2020-semiautoregressive,
  title =	 {Semi-Autoregressive Training Improves Mask-Predict Decoding},
  author =	 {Marjan Ghazvininejad and Omer Levy and Luke Zettlemoyer},
  journal =	 {ArXiv},
  year =	 {2020},
  volume =	 {abs/2001.08785}
}

@article{shu2020latent,
  title =	 {Latent-Variable Non-Autoregressive Neural Machine Translation
                  with Deterministic Inference Using a Delta Posterior},
  volume =	 {34},
  url =		 {https://ojs.aaai.org/index.php/AAAI/article/view/6413},
  DOI =		 {10.1609/aaai.v34i05.6413},
  abstractNote = {&lt;p&gt;Although neural machine translation models reached
                  high translation quality, the autoregressive nature makes
                  inference difficult to parallelize and leads to high
                  translation latency. Inspired by recent refinement-based
                  approaches, we propose LaNMT, a latent-variable
                  non-autoregressive model with continuous latent variables and
                  deterministic inference procedure. In contrast to existing
                  approaches, we use a deterministic inference algorithm to find
                  the target sequence that maximizes the lowerbound to the
                  log-probability. During inference, the length of translation
                  automatically adapts itself. Our experiments show that the
                  lowerbound can be greatly increased by running the inference
                  algorithm, resulting in significantly improved translation
                  quality. Our proposed model closes the performance gap between
                  non-autoregressive and autoregressive approaches on ASPEC
                  Ja-En dataset with 8.6x faster decoding. On WMT’14 En-De
                  dataset, our model narrows the gap with autoregressive
                  baseline to 2.0 BLEU points with 12.5x speedup. By decoding
                  multiple initial latent variables in parallel and rescore
                  using a teacher model, the proposed model further brings the
                  gap down to 1.0 BLEU point on WMT’14 En-De task with 6.8x
                  speedup.&lt;/p&gt;},
  number =	 {05},
  journal =	 {Proceedings of the AAAI Conference on Artificial Intelligence},
  author =	 {Shu, Raphael and Lee, Jason and Nakayama, Hideki and Cho,
                  Kyunghyun},
  year =	 {2020},
  month =	 {Apr.},
  pages =	 {8846-8853}
}

@article{chan-etal-2019-kermit,
  author =	 {William Chan and Nikita Kitaev and Kelvin Guu and Mitchell
                  Stern and Jakob Uszkoreit},
  title =	 {{KERMIT:} Generative Insertion-Based Modeling for Sequences},
  journal =	 {CoRR},
  volume =	 {abs/1906.01604},
  year =	 {2019},
  url =		 {http://arxiv.org/abs/1906.01604},
  archivePrefix ={arXiv},
  eprint =	 {1906.01604},
  timestamp =	 {Thu, 13 Jun 2019 13:36:00 +0200},
  biburl =	 {https://dblp.org/rec/journals/corr/abs-1906-01604.bib},
  bibsource =	 {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gu-etal-2019-levenshtein,
  author =	 {Gu, Jiatao and Wang, Changhan and Zhao, Junbo},
  booktitle =	 {Advances in Neural Information Processing Systems},
  editor =	 {H. Wallach and H. Larochelle and A. Beygelzimer and
                  F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages =	 {11181--11191},
  publisher =	 {Curran Associates, Inc.},
  title =	 {Levenshtein Transformer},
  url =
                  {https://proceedings.neurips.cc/paper/2019/file/675f9820626f5bc0afb47b57890b466e-Paper.pdf},
  volume =	 {32},
  year =	 {2019}
}



@inproceedings{stern2018blockwise,
  author =	 {Stern, Mitchell and Shazeer, Noam and Uszkoreit, Jakob},
  booktitle =	 {Advances in Neural Information Processing Systems},
  editor =	 {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and
                  N. Cesa-Bianchi and R. Garnett},
  pages =	 {10086--10095},
  publisher =	 {Curran Associates, Inc.},
  title =	 {Blockwise Parallel Decoding for Deep Autoregressive Models},
  url =
                  {https://proceedings.neurips.cc/paper/2018/file/c4127b9194fe8562c64dc0f5bf2c93bc-Paper.pdf},
  volume =	 {31},
  year =	 {2018}
}


@InProceedings{stern-etal-2019-insertion,
  title =	 {Insertion Transformer: Flexible Sequence Generation via
                  Insertion Operations},
  author =	 {Stern, Mitchell and Chan, William and Kiros, Jamie and
                  Uszkoreit, Jakob},
  booktitle =	 {Proceedings of the 36th International Conference on Machine
                  Learning},
  pages =	 {5976--5985},
  year =	 2019,
  editor =	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume =	 97,
  series =	 {Proceedings of Machine Learning Research},
  month =	 {09--15 Jun},
  publisher =	 {PMLR},
  pdf =		 {http://proceedings.mlr.press/v97/stern19a/stern19a.pdf},
  url =		 { http://proceedings.mlr.press/v97/stern19a.html },
  abstract =	 {We present the Insertion Transformer, an iterative, partially
                  autoregressive model for sequence generation based on
                  insertion operations. Unlike typical autoregressive models
                  which rely on a fixed, often left-to-right ordering of the
                  output, our approach accommodates arbitrary orderings by
                  allowing for tokens to be inserted anywhere in the sequence
                  during decoding. This flexibility confers a number of
                  advantages: for instance, not only can our model be trained to
                  follow specific orderings such as left-to-right generation or
                  a binary tree traversal, but it can also be trained to
                  maximize entropy over all valid insertions for robustness. In
                  addition, our model seamlessly accommodates both fully
                  autoregressive generation (one insertion at a time) and
                  partially autoregressive generation (simultaneous insertions
                  at multiple locations). We validate our approach by analyzing
                  its performance on the WMT 2014 English-German machine
                  translation task under various settings for training and
                  decoding. We find that the Insertion Transformer outperforms
                  many prior non-autoregressive approaches to translation at
                  comparable or better levels of parallelism, and successfully
                  recovers the performance of the original Transformer while
                  requiring only logarithmically many iterations during
                  decoding.}
}

@inproceedings{gu2017nonautoregressive,
  author =	 {Jiatao Gu and James Bradbury and Caiming Xiong and Victor
                  O. K. Li and Richard Socher},
  title =	 {Non-Autoregressive Neural Machine Translation},
  booktitle =	 {6th International Conference on Learning Representations,
                  {ICLR} 2018},
  month =	 apr,
  address =	 {Vancouver, BC, Canada},
  year =	 2018,
  url =		 {https://openreview.net/forum?id=B1l8BtlCb},
  timestamp =	 {Thu, 25 Jul 2019 14:25:57 +0200},
}

@article{mansimov2019generalized,
  author =	 {Elman Mansimov and Alex Wang and Kyunghyun Cho},
  title =	 {A Generalized Framework of Sequence Generation with
                  Application to Undirected Sequence Models},
  journal =	 {CoRR},
  volume =	 {abs/1905.12790},
  year =	 2019,
  url =		 {http://arxiv.org/abs/1905.12790},
  archivePrefix ={arXiv},
  eprint =	 {1905.12790},
  timestamp =	 {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl =	 {https://dblp.org/rec/journals/corr/abs-1905-12790.bib},
  bibsource =	 {dblp computer science bibliography, https://dblp.org}
}
@article{reiter2018structured,
  author =	 {Reiter, Ehud},
  title =	 {A Structured Review of the Validity of BLEU},
  journal =	 {Computational Linguistics},
  volume =	 44,
  number =	 3,
  pages =	 {393-401},
  year =	 2018,
  doi =		 {10.1162/coli\_a\_00322},
  URL =		 {https://doi.org/10.1162/coli_a_00322},
  eprint =	 {ttps://doi.org/10.1162/coli_a_00322},
  abstract =	 { The BLEU metric has been widely used in NLP for over 15 years
                  to evaluate NLP systems, especially in machine translation and
                  natural language generation. I present a structured review of
                  the evidence on whether BLEU is a valid evaluation
                  technique—in other words, whether BLEU scores correlate with
                  real-world utility and user-satisfaction of NLP systems; this
                  review covers 284 correlations reported in 34 papers. Overall,
                  the evidence supports using BLEU for diagnostic evaluation of
                  MT systems (which is what it was originally proposed for), but
                  does not support using BLEU outside of MT, for evaluation of
                  individual texts, or for scientific hypothesis testing. }
}

@inproceedings{mikolov-etal-2013-distributed,
  title =	 {Distributed Representations of Words and Phrases and Their
                  Compositionality},
  author =	 {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado,
                  Greg S and Dean, Jeff},
  booktitle =	 {Advances in neural information processing systems},
  pages =	 {3111--3119},
  year =	 2013
}

@article{bengio2003neural,
  title =	 {A Neural Probabilistic Language Model},
  author =	 {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal
                  and Jauvin, Christian},
  journal =	 {Journal of machine learning research},
  volume =	 3,
  number =	 {Feb},
  pages =	 {1137--1155},
  year =	 2003
}

@article{ba2016layer,
  author =	 {Ba, Lei Jimmy and Kiros, Ryan and Hinton, Geoffrey E},
  title =	 {Layer Normalization},
  journal =	 {CoRR},
  archiveprefix ={arXiv},
  eprint =	 {1607.06450},
  issn =	 {2331-8422},
  volume =	 {abs/1607.06450},
  year =	 2016
}

@article{bahdanau2014neural,
  author =	 {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  title =	 {Neural Machine Translation by Jointly Learning to Align and
                  Translate},
  journal =	 {CoRR},
  issn =	 {2331-8422},
  volume =	 {abs/1409.0473},
  year =	 2014
}

@inproceedings{bengio2007greedy,
  author =	 {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and
                  Larochelle, Hugo},
  title =	 {Greedy Layer-Wise Training of Deep Networks},
  booktitle =	 {Advances in Neural Information Processing Systems 20},
  address =	 {Vancouver, Canada},
  isbn =	 9780262256919,
  month =	 {December},
  pages =	 {153--160},
  publisher =	 {Curran Associates, Inc.},
  year =	 2007
}

@article{bengio2013representation,
  author =	 {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  title =	 {Representation Learning: A Review and New Perspectives},
  journal =	 {IEEE Trans. Pattern Anal. Mach. Intell.},
  address =	 {Washington, D.C., USA},
  issn =	 {0162-8828},
  month =	 {August},
  number =	 8,
  pages =	 {1798--1828},
  publisher =	 {IEEE Computer Society},
  volume =	 35,
  year =	 2013
}

@article{conneau2017supervised,
  author =	 {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and
                  Barrault, Lo\"{i}c and Bordes, Antoine},
  title =	 {Supervised Learning of Universal Sentence Representations from
                  Natural Language Inference Data},
  journal =	 {CoRR},
  issn =	 {2331-8422},
  volume =	 {abs/1705.02364},
  year =	 2017
}

@article{conneau2017word,
  author =	 {Conneau, Alexis and Lample, Guillaume and Ranzato,
                  Marc'Aurelio and Denoyer, Ludovic and J{\'{e}}gou,
                  Herv{\'{e}}},
  title =	 {Word Translation Without Parallel Data},
  journal =	 {CoRR},
  archiveprefix ={arXiv},
  eprint =	 {1710.04087},
  issn =	 {2331-8422},
  volume =	 {abs/1710.04087},
  year =	 2017
}

@inproceedings{conneau2018senteval,
  author =	 {Conneau, Alexis and Kiela, Douwe},
  title =	 {{SentEval}: An Evaluation Toolkit for Universal Sentence
                  Representations},
  booktitle =	 {Proceedings of the Eleventh International Conference on
                  Language Resources and Evaluation (LREC)},
  address =	 {Miyazaki, Japan},
  isbn =	 {979-10-95546-00-9},
  language =	 {english},
  month =	 {May},
  pages =	 {1699--1704},
  publisher =	 {European Language Resources Association (ELRA)},
  year =	 2018
}

@article{elman1990finding,
  author =	 {Elman, Jeffrey L},
  title =	 {Finding Structure in Time},
  journal =	 {Cognitive Science},
  address =	 {Victoria, Canada},
  issn =	 {1551-6709},
  number =	 2,
  pages =	 {179--211},
  publisher =	 {Cognitive Science Society},
  volume =	 14,
  year =	 1990
}

@inproceedings{gehring2017convolutional,
  author =	 {Gehring, Jonas and Auli, Michael and Grangier, David and
                  Yarats, Denis and Dauphin, Yann N},
  title =	 {Convolutional Sequence to Sequence Learning},
  booktitle =	 {International Conference on Machine Learning},
  address =	 {Sydney, Australia},
  month =	 {August},
  pages =	 {1243--1252},
  publisher =	 {PMLR},
  year =	 2017
}

@article{graves2005framewise,
  author =	 {Graves, Alex and Schmidhuber, J\"{u}rgen},
  title =	 {Framewise Phoneme Classification with Bidirectional {LSTM} and
                  Other Neural Network Architectures},
  journal =	 {Neural Networks},
  address =	 {Amsterdam, Netherlands},
  issn =	 {0893-6080},
  month =	 {Jul},
  number =	 {5-6},
  pages =	 {602--610},
  publisher =	 {Elsevier},
  volume =	 18,
  year =	 2005
}

@inproceedings{graves2006connectionist,
  author =	 {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino
                  and Schmidhuber, J\"{u}rgen},
  title =	 {Connectionist Temporal Classification: Labelling Unsegmented
                  Sequence Data with Recurrent Neural Networks},
  booktitle =	 {Proceedings of the 23rd International Conference on Machine
                  Learning},
  address =	 {Pittsburgh, PA, USA},
  month =	 {June},
  pages =	 {369--376},
  publisher =	 {JMLR.org},
  year =	 2006
}

@inproceedings{graves2009offline,
  author =	 {Graves, Alex and Schmidhuber, J\"{u}rgen},
  title =	 {Offline Handwriting Recognition with Multidimensional
                  Recurrent Neural Networks},
  booktitle =	 {Advances in Neural Information Processing Systems 21},
  address =	 {Vancouver, Canada},
  month =	 {December},
  pages =	 {545--552},
  publisher =	 {Curran Associates, Inc.},
  year =	 2009
}

@inproceedings{graves2013speech,
  author =	 {Graves, Alex and Mohamed, Abdel{-}rahman and Hinton, Geoffrey
                  E},
  title =	 {Speech recognition with deep recurrent neural networks},
  booktitle =	 {International Conference on Acoustics, Speech and Signal
                  Processing ({ICASSP})},
  address =	 {Vancouver, Canada},
  isbn =	 9781479903566,
  month =	 {May},
  pages =	 {6645--6649},
  publisher =	 {{IEEE} Computer Society},
  year =	 2013
}

@article{helcl-libovicky-2017-neural,
  author =	 {Helcl, Jind\v{r}ich and Libovick\'{y}, Jind\v{r}ich},
  title =	 {Neural Monkey: An Open-source Tool for Sequence Learning},
  journal =	 {The Prague Bulletin of Mathematical Linguistics},
  address =	 {Prague, Czech Republic},
  issn =	 {0032-6585},
  month =	 {Apr},
  number =	 1,
  pages =	 {5--17},
  publisher =	 {Charles University},
  volume =	 107,
  year =	 2017
}

@article{hochreiter1997long,
  author =	 {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
  title =	 {Long Short-Term Memory},
  journal =	 {Neural Computation},
  address =	 {Cambridge, MA, USA},
  issn =	 {0899-7667},
  number =	 8,
  pages =	 {1735--1780},
  publisher =	 {MIT Press},
  volume =	 9,
  year =	 1997
}

@inproceedings{kingma2014semi,
  author =	 {Kingma, Diederik P and Mohamed, Shakir and Rezende, Danilo
                  Jimenez and Welling, Max},
  title =	 {Semi-supervised Learning with Deep Generative Models},
  booktitle =	 {Advances in Neural Information Processing Systems 27},
  address =	 {Montreal, Canada},
  month =	 {December},
  pages =	 {3581--3589},
  publisher =	 {Curran Associates, Inc.},
  year =	 2014
}

@article{kingma2014adam,
  author =	 {Kingma, Diederik P and Ba, Jimmy},
  title =	 {Adam: {A} Method for Stochastic Optimization},
  journal =	 {CoRR},
  issn =	 {2331-8422},
  volume =	 {abs/1412.6980},
  year =	 2014
}

@inproceedings{lafferty2001conditional,
  author =	 {Lafferty, John and McCallum, Andrew and Pereira, Fernando},
  title =	 {Conditional Random Fields: Probabilistic Models for Segmenting
                  and Labeling Sequence Data},
  booktitle =	 {Proceedings of the 18th International Conference on Machine
                  Learning},
  address =	 {Williamstown, MA, USA},
  isbn =	 {1-55860-778-1},
  month =	 {June},
  pages =	 {282--289},
  publisher =	 {Morgan Kaufmann},
  year =	 2001
}

@article{srivastava2014dropout,
  author =	 {Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex
                  and Sutskever, Ilya and Salakhutdinov, Ruslan},
  title =	 {Dropout: A Simple Way to Prevent Neural Networks from
                  Overfitting},
  journal =	 {The Journal of Machine Learning Research},
  address =	 {Brookline, MA, USA},
  issn =	 {1532-4435},
  number =	 1,
  pages =	 {1929--1958},
  publisher =	 {JMLR. org},
  volume =	 15,
  year =	 2014
}

@inproceedings{sutskever2014sequence,
  author =	 {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  title =	 {Sequence to Sequence Learning with Neural Networks},
  booktitle =	 {Advances in Neural Information Processing Systems 27},
  address =	 {Montreal, Canada},
  month =	 {December},
  pages =	 {3104--3112},
  publisher =	 {Curran Associates, Inc.},
  year =	 2014
}

@inproceedings{vaswani2017attention,
  author =	 {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                  Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                  Kaiser, {\L}ukasz and Polosukhin, Illia},
  title =	 {Attention is All You Need},
  booktitle =	 {Advances in Neural Information Processing Systems 30},
  address =	 {Long Beach, CA, USA},
  month =	 {December},
  pages =	 {6000--6010},
  publisher =	 {Curran Associates, Inc.},
  year =	 2017
}

@inproceedings{schuster-nakajima-2012-japanese,
  author={Schuster, Mike and Nakajima, Kaisuke},
  booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal
                  Processing (ICASSP)},
  title={Japanese and Korean Voice Search},
  year={2012},
  volume={},
  number={},
  pages={5149-5152},
  doi={10.1109/ICASSP.2012.6289079}
}


@article{wu2016google,
  author =	 {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc
                  V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun,
                  Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and
                  Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu,
                  Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato,
                  Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith
                  and Kurian, George and Patil, Nishant and Wang, Wei and Young,
                  Cliff and Smith, Jason R and Riesa, Jason and Rudnick, Alex
                  and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and
                  Dean, Jeffrey},
  title =	 {Google's Neural Machine Translation System: Bridging the Gap
                  between Human and Machine Translation},
  journal =	 {CoRR},
  archiveprefix ={arXiv},
  eprint =	 {1609.08144},
  issn =	 {2331-8422},
  volume =	 {abs/1609.08144},
  year =	 2016
}
