% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Non-Autoregressive NMT}
\label{chap:nat}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\JH{this chapter needs reorganization}

The efficiency of \ac{mt} models is often crucial in real-world applications.
Most commercial \ac{nmt} models are available through a cloud-based service,
such as Microsoft Translator\footnote{\url{https://microsoft.com/translator/}}
or Google Translate\footnote{\url{https://translate.google.com/}}. Scaling
cloud-based solutions for large user bases is simple but costly. But even with
a large pool of computational resources, it is worthwhile to implement
optimizations which decrease the model latency and improve the user experience.

% lokální modely maj tu výhodu, že to běží u mně, takže to jde pouyžít offline
% a taky se nemusím bát o svoje soukromý data. ale jsou pomalý.

Locally-deployed \ac{nmt} models provide a number of advantages over
cloud-based solutions. First, the service does not rely on internet
connection. Second, the data are not being sent to a 3rd party server, and
therefore it is suitable for translating private or confidential data.
However, without optimization, running state-of-the-art translation models
locally often requires specialized hardware, such as one or more GPU
cards. Otherwise, the time for translating a single sentence can easily reach
more than a second on a standard CPU.

Higher decoding speeds can be achieved by model optimization. In their 2019
submission to the \ac{wngt} shared task on efficiency,
\citet{kim-etal-2019-research} successfully employed knowledge distillation,
quantization, shortlisting \citep{jean-etal-2015-using} and a simpler recurrent
unit design to bring the throughput of the translation model up to 3,600 words
per second on a CPU, for a modest drop in the BLEU score. Following this
submission, \citet{bogoychev-etal-2020-edinburghs} reported further
improvements with attention head pruning \citep{voita-etal-2019-analyzing}. The
work has been a part of the Bergamot research project, which aims to bring
offline translation models into a browser.\footnote{https://browser.mt/}

% vedle toho tu jsou neautoregresivní modely který se na to snaží jít jinak

\Ac{nar} models present an alternative approach to model optimization, using a
different architecture and decoding algorithm which has lower time complexity.
In \ac{nmt}, a non-autoregressive decoding algorithm does not access the
previously decoded outputs, imposing conditional independence on the output
token probability distributions. This assumption allows parallelization of the
decoding which can significantly reduce the latency of the translation
system. On the other hand, it also presents a challenge to the language model,
which usually leads to poorer translation quality.

The research presented in this thesis is focused on the usefulness of \ac{nar}
models for translation (\acs{nat}\glsunset{nat}) in the quest for faster
translation models. We first analyze the \ac{nat} models alone and assess the
necessary techniques needed to match the quality of \ac{ar} models.  Then, we
adapt the optimizations successfully used in \ac{ar} models for \ac{nar} models
and evaluate the additional speed gains.

In this chapter, we present an overview of the current state of the research
and the key concepts in non-autoregressive \ac{nmt}.  In Section
\ref{sec:nat:principles}, we first describe the principles that most of the
related literature has in common. Then, we provide a survey of notable
approaches to \ac{nat}. We categorize the studied methods into alignment-based
methods (Section \ref{sec:nat:alignment}), iterative (or semi-autoregressive)
methods (Section \ref{sec:nat:semi}), methods using auxiliary objectives
(Section \ref{sec:nat:aux}). \JH{Then there is the Misc category ((Section
  \ref{sec:nat:misc}).} We conclude this chapter with a discussion of the
limitations of the presented literature and \JH{and what?}
% Naturally, some methods belong to more than one category, in which case we
% point that out to the reader.

\JH{state difference between semi-autoregressive and iterative}

% ------------------------------------------------------------------------------
\section{Non-Autoregressive Models}%
\label{sec:nat:principles}
% ------------------------------------------------------------------------------

This section summarizes the main features of \ac{nar} models and some of the
basic concepts and the research problems in this field. The \ac{nar} variant of
the Transformer model was first described by \citet{gu2017nonautoregressive}
and \citet{lee-etal-2018-deterministic}. Each of these two foundational papers
provides different (complementary) grounds for further research discussed later
in this chapter -- using latent variables, and iterative decoding.

% ------------------------------------------------------------------------------
\paragraph{Conditional Independence.} The defining feature of a
non-autoregressive model is the conditional independence of the output
distributions across time steps. Recall Equation \ref{eq:output-distribution}
which defines the output distribution for autoregressive models:
%
\begin{equation*}
  p(y|x) = \prod_{t=1}^{T_y}p(y_t|y_{<t},x,\theta).
  \tag{\ref{eq:output-distribution}}
\end{equation*}
%
Unlike Equation \ref{eq:output-distribution}, \Ac{nat} models do not condition
the output token probabilities on the previously decoded outputs $y_{<t}$.  The
probability of an output sentence $y$ given an input sequence $x$ can then be
modeled as:
%
\begin{equation}
  p(y|x) = \prod_{t=1}^{T_y}p(y_t|x,\theta).
  \label{eq:nat-output-distribution}
\end{equation}

Although technically possible, making the outputs in \acs{rnn}-based models
conditionally independent does not reduce the decoding time because in
\acsp{rnn}, each state depends on the value of the preceding state. However, in
the Transformer model, the hidden states in one layer depend only on the states
from the previous layer. This allows for parallelization on the layer level.

Since the outputs are conditionally independent, the Transformer decoder cannot
work with the previously decoded outputs. In the following paragraphs, we
discuss the necessary alterations to the Transformer architecture.  First, the
decoder needs to know the sentence length. Second, we need to supply the
decoder inputs. Third, the causal mask in the decoder self-attention is now
unnecessary.

% ------------------------------------------------------------------------------
\paragraph{Target Length Estimation.} In a standard NMT model such as the
Transformer or an \acs{rnn}-based model with attention, the length of the
output sentence is modeled implicitly by using the special end-of-sentence
(\eos{}) token. Equations \ref{eq:output-distribution} and
\ref{eq:nat-output-distribution} work with the sentence length implicitly,
assuming that the last token $y_{T_y}$ is the \eos{} token and that \eos{} does
not appear among the previous tokens $y_{<T_y}$.

The probability distribution in Equation \ref{eq:nat-output-distribution} can
be factorized to express the estimation of the target sentence length
explicitly, leaving out the \eos{} symbol from the mathematical model:
\begin{equation}
  p(y|x) = p_L(T_y|x, \theta) \cdot \prod_{t=1}^{T_y}p(y_t|x,\theta).
  \label{eq:explicit-length}
\end{equation}


% When using Equation \ref{eq:output-distribution} to model probability
% distribution over the set of all possible sentences, the probabilities assigned
% to longer sentences are considerably lower than the probabilities of sentences
% which are short. This follows from the multiplication of probabilities, which
% are numbers between zero and one. \JH{další věc je že je ten search space
%   exponenciálně roste -- This follows from the fact that the search space
%   (i.e. the number of all possible sentences of length $T_y$) grows
%   exponentially with $T_y$.}  \JH{odkazat spis na argmax nez na output
%   distribution.} To diminish the negative influence of this property,
% \citet{wu2016google} introduce length normalization to the beam search algorithm
% (\JH{odkazat na eq. v intro do MT}). The normalization acts as a prior imposed
% on the target sentence length distribution.

% It is, however, possible to estimate the target length explicitly.
% \JH{...}

% ------------------------------------------------------------------------------
\paragraph{Multimodality Problem.} In one of the first applications of a
non-autoregressive model to \ac{nmt}, \citet{gu2017nonautoregressive} describe
the \emph{multimodality problem} which arise when the outputs are conditionally
independent.

When estimating the probability of a word on a given position, there may be
multiple words which get a high probability. These words are the so called
\emph{modes} of the distribution. In autoregressive models, once a word gets
selected, other modes are ignored in the following time steps. However, a
non-autoregressive model does not base the decisions on the preceding ones, so
when multiple positions have multiple modes, the model has no way of knowing
which modes have been selected on other positions.

% This issue is illustrated in Figure \ref{fig:multimodality-problem}.
% \begin{figure}
%   \centering
%   \begin{minipage}{\textwidth}
%     source: thank you

%     \begin{center}
%     \begin{tabular}{cccc}
%       \toprule
%       $y_1$ & $p(y_1|x)$ & $y_2$ & $p(y_2|x)$ \\
%       \midrule
%       vielen & 0.4 & dank & 0.4 \\
%       danke  & 0.3 & schön & 0.3 \\
%       \vdots & & \vdots & & \\
%       \bottomrule
%     \end{tabular}
%     \end{center}

%   \end{minipage}
%   \caption{Illustration of the multimodality problem.}
%   \label{fig:multimodality-problem}
% \end{figure}

A well-known example of the multimodality problem is the translation of the
sentence ``thank you'' into German, which has two equally likely translations:
``vielen dank'' and ``danke schön.'' In this case, the pair of German tokens
``danke'' and ``vielen'' create the two modes on the first position, and the
tokens ``dank'' and ``schön'' are modes on the second position. If an
autoregressive model chooses to generate ``danke'' on the first position, the
token ``dank'' on the second position will no longer be likely. However, when a
non-autoregressive model assigns high probabilities to the correct
translations, it also has to assign high probabilities to the other (incorrect)
two combinations, ``danke dank'' and ``vielen schön''.

% ------------------------------------------------------------------------------
\paragraph{\Ac{nat} with Fertility Model.} Non-autoregressive Transformer
decoder does not receive the previously decoded tokens on the input. A solution
proposed by \citet{gu2017nonautoregressive} is to use a simple fertility model,
which also serves as the explicit target length estimator.

Compared to the autoregressive Transformer model, the model has the following
modifications. First, the inputs to the decoder are made up of the sequence of
encoder inputs, either uniformly stretched to the predicted target sentence
length, or copied using a fertility model. Second, the decoder self-attention
does not use the causal mask since all states can now attend to all other
states in both directions. Third, \emph{positional attention} is added to every
decoder layer where the positional encoding is used as queries and keys, and
the decoder states as values.

In \citet{gu2017nonautoregressive}, the multimodal problem is addressed by
introducing a latent fertility variables $f_1, \ldots, f_{T_x}$ sampled from a
prior distribution. Each $f_i \in \mathbb{N}_0$ denotes the number of times to
copy $x_i$ to the decoder input. The output probability is then conditioned on
the value of the latent variable.
% \begin{equation}
%   p(y|x) = p_L(T_y|x, \theta) \cdot \prod_{t=1}^{T_y}p(y_t|x, z, \theta).
% \end{equation}

The training is done by combining two loss functions -- translation loss and
fertility loss:
\begin{align}
  \mathcal{L}(\theta)  & = \log p(y|x, \theta) \\
                       & = log \sum_{F \in \mathcal{F}} p(F| x, \theta ) p(y | x, F, \theta) \\
                       & \geq \mathbb{E}_{\bar{F} \sim q}
                         \left(
                         \sum_{t=1}^{T_y} \log p(y_t | x, \bar{F}, \theta)
                         + \sum_{t=1}^{T_x} \log p(f_t | x, \theta)
                         \right)
                         + \mathcal{H}(q)
\end{align}
%
where \JH{... describe the equation, or comment it out. Also, what is $\mathcal{H}$?}
%
The fertility model depends on an external module and is not trained end to
end. The authors fine-tune the trained model using reinforcement learning
\citep{williams1992simple}.

During decoding, the marginalizing over all possible fertility values is
intractable. Therefore, the authors experiment with three approximation methods
-- argmax and average decoding, and \ac{npd}.

% ------------------------------------------------------------------------------
\paragraph{Autoregressive Rescoring.} A usual technique in the \ac{nat}
literature is to use some form of rescoring of the outputs of a \acl{nar} model
by an \acl{ar} model. The nature of the Transformer model allows to score a
sentence in a single step (as opposed to generating one), which means that the
additional rescoring computation increases the decoding complexity only
linearly with respect to the number of the candidate hypoteheses for rescoring.

The choice of the rescoring candidates varies. \citet{gu2017nonautoregressive}
use \ac{npd} which first samples the decoder input sequences from the fertility
distribution, compute the best candidate for each input, and then rescore the
results using the \ac{ar} teacher model. A closely related method is \ac{lpd}
which generates and scores sentences of different lengths.

% ------------------------------------------------------------------------------
\paragraph{Knowledge Distillation.} To tackle the multimodality problem from
another angle, \citet{gu2017nonautoregressive} propose to use sequence-level
knowledge distillation to create artificial training data
\citep{kim-rush-2016-sequence}. The main idea is that outputs of a teacher
model will have a simpler structure than natural language, limiting the number
of the distribution modes.

According to an ablation study published by \citet{gu-kong-2021-fully}, using
knowledge distillation is a crucial element in training non-autoregressive
translation models, regardless the actual method used.

% ------------------------------------------------------------------------------
\paragraph{Iterative Refinement.} Another attempt to introduce latent variables
in a non-autoregressive model was proposed by
\citet{lee-etal-2018-deterministic}.  Instead of modeling fertility, the
authors introduce $L$ discrete sequential latent variables interpreted as
stages of refinement:
%
\begin{align}
  \begin{split}
    p(y|x) & = \sum_{y^L}
      \left( \prod_{t=1}^{T_y} p(y_t|y^L, x) \right) p(y^L|x) \\
    p(y^L|x) & = \sum_{y^{L-1}}
      \left( \prod_{t=1}^{T_y} p(y_t^L | y^{L-1}, x) \right)
      p(y^{L-1}|x) \\
    \vdots \\
    p(y^0|x) & = \prod_{t=1}^{T_y} p(y_t^0|x)
  \end{split}
\end{align}
%
Each summation in the equations above is computed over the whole space of
possible sequences, and thus, it is intractable for the probablility $p(y|x)$
to be caluculated exactly. To overcome this problem, the authors approximate
the sums with only the element corresponding to the sequence with the largest
probability, $\hat{y}_t = \argmax_{y_t}p(y_t|x)$.\footnote{Note that in
  non-autoregressive model where the probability does not depend on previously
  decoded outputs $y_{<t}$, getting the most probable tokens will also yield
  the most probable sequence.}  Putting it together with the equations above
and moving to the logarithmic domain, we get:
\begin{align}
  \begin{split}
    \log p(y|x) \geq
    & \sum_{t=1}^{T_y} \log p(y_t| \hat{y}^L, x) + \\
    & + \sum_{t=1}^{T_y} \log p(\hat{y}_t^{L}| \hat{y}^{L-1}, x) + \ldots \\
    & + \sum_{t=1}^{T_y} \log p(\hat{y}_t^0 | x). \label{eq:refinement-lowerbound}
    % = & \sum_{l=0}^{L} \sum_{t=1}^{T_Y} \log p(\hat{y}_t^{l+1} | \hat{Y}^l, X)
  \end{split}
\end{align}
%where $Y^0 = X$ and $\hat{y}_t^{L+1} = y_t$.

The initial sequence $\hat{y}_t^0$ is set to $x_{t'}$ where
$t' = (T_x / T_y) \cdot t$, i.e. the source sentence is either squished or
stretched by copying or omitting some of the source words in order to fit the
target sentence length. During training, the length of the reference sentence
is known. During decoding, the authors use a separate model $p(T_y|x)$ for
target length prediction. All probability distributions in the above equations
are modeled with neural networks with shared parameters. In this way, the
number of intermediate refinement steps remains flexible during the decoding.

The latent variable model is trained by minimizing the log-likelihood of the
eference sentence $y^*$ in each of the refinement steps:
\begin{equation}
  \mathcal{L}_{\text{LVM}}(\theta) = - \sum_{l=1}^{L+1} \left(
    \sum_{t=1}^{T_{y^*}} \log p(y_t^* | \hat{y}^{l-1}, x, \theta)
  \right) \label{eq:refinement-lvm-loss}
\end{equation}

In their iterative approach, \citet{lee-etal-2018-deterministic} also discuss
the training of the refinement process from a denoising perspective. Formally,
they introduce an additional denoising autoencoder loss to the model:
%
\begin{equation}
  \mathcal{L}_{\text{DAE}}(\theta) = - \sum_{t=1}^{T_y} \log p(y_t^* | \bar{y}, x, \theta)
\end{equation}
where $\bar{y}$ is a corrupted version of the reference translation $y^*$. The
corruption process is performed on each token with a probability $\beta$. Each
corrupted token is either replaced with a random word from the vocabulary,
copied to its neighbor, or swapped with the neighbor.

During training, the two loss functions are stochastically mixed using a
hyperparameter $\alpha$, sampled from a Bernoulli distribution.

% ------------------------------------------------------------------------------
\section{Alignment-Based Methods}%
\label{sec:nat:alignment}
% ------------------------------------------------------------------------------

\Acl{ar} models are trained using the cross-entropy loss (recall Equation
\ref{eq:loss}). When we assume a single sentence pair $(x,y)$, the negative
log-likelihood is the sum of negative log-probabilities
$\log p(y_i|x, y_{<i}, \theta)$ for $i=1, \ldots, T_y$, as estimated by the
model with parameters $\theta$.  Notice how the ground-truth sequence
$y_1, \ldots, y_{T_y}$ corresponds to the sequence of the probabilities
$p(y_i|\ldots)$: we require that the $i$-th reference token is decoded at the
$i$-th position of the output state sequence. The methods described in this
section relax this requirement.

We define an alignment as a function $a$ which maps the positions in the
ground-truth sentence to the positions in the output. In autoregressive
models, $a(i) = i$ for every position $i$ in the ground truth. % When we
% generalize the cross-entropy loss for sentence pair $(x, y)$ to account for the
% alignment, we get
Methods that consider different alignments allow to work with output and label
sequences with different lengths, which is a useful feature in the context of
\ac{nat} models.

To the best of our knowledge, we were the first to propose an alignment-based
method for \ac{nat} \citep{libovicky-helcl-2018-end}. Our method is based on
\acf{ctc}\glsunset{ctc}, which computes the cross-entropy losses over all
possible alignments between the ground truth and a longer output state
seqeunce. We describe this method in detail in Chapter \ref{chap:nar-nmt-ctc}.
In the following paragraphs, we present a number of related methods based on
modeling the alignment between the sequences of outputs and ground-truth
labels.

% -----------------------------------------------------------------------------
\paragraph{Aligned Cross Entropy for \ac{nat}} One problem with using
cross-entropy objective for training \ac{nat} models is that it heavily
penalizes misaligned target words. If a correct word is generated at an
incorrect position in the target sentence, the loss is the same as when the
model generates a completely unrelated word. In autoregressive models, this
problem is avoided with teacher forcing -- the model is provided with the
preceding words from the reference sentence. Without teacher forcing, the
alignment between the predicted distributions and the positions in the
reference sentence is too rigid. For example, consider the reference sentence
``Thank you for your cooperation.'' In a non-autoregressive model, another
translation ``Thanks for your cooperation'' receives the same training signal
as a completely wrong translation. The difference is that on the second
position, the autoregressive model knows that ``Thank'' was expected on the
first position, so the next word ``for'' does not receive high probability.
One way to address this problem is to consider the alignment as a latent
variable.

\Acl{axe} \glsunset{axe} (\acs{axe}; \citealp{ghazvininejad2020aligned}) is an
objective function that takes this problem into account. It uses a dynamic
programming algorithm to find the alignment with the minimum cross-entropy
loss. In the example from the previous paragraph, the alignment with the lowest
cross-entropy is the one that aligns the positions 3--5 from the label sequence
(representing the suffix ``for your cooperation'') to output positions 2--4 in
the output. Either of the first two positions in the label sequence can be
aligned to the first position in the output sequence, while the other label
position remains unaligned.

It is possible for the alignment to either skip a target position (as in the
example above), or keep an unaligned position in the prediction sequence. In
case of skipping the predictions, an empty token is aligned with the skipped
position (so that $-\log p(y_t = \epsilon)$ is added to the loss). In case of
skipping a target position, the negative log-probability of the skipped target
word is added to the loss, weighted by a hyperparameter $\delta$ which controls
the cost of the skip-target operation.

Formally, ... \JH{doplnit}

In our experiments, we use the \ac{ctc} loss, which is similar to this
approach. Instead of considering the alignment that yields the minimum
cross-entropy loss, the \ac{ctc} algorithm computes the sum of all the possible
alignments. Unlike \ac{axe}, we only consider skipping the predictions, while
not allowing to skip the target positions. We present the \ac{ctc}-based method
in detail in Chapter \ref{chap:nar-nmt-ctc}.


% -----------------------------------------------------------------------------
% AXE, but extended to non-monotonic
\paragraph{Order-Agnostic Cross-Entropy.} The alignment-based methods discussed
in this section so far are considering only monotonic alignments, i.e. for
every alignment $a$ the condition $a(i) \leq a(i + 1)$
holds. \citet{du2021orderagnostic} design \ac{oaxe}, a new training objective
which also permits non-monotonic alignments (see illustration in Figure
\ref{fig:oaxe-example}).  \JH{continue}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{img/oaxe.png}

  \caption{An illustration of (a) cross-entropy, (b) aligned cross-entropy, and
    (c) order-agnostic cross-entropy on a toy example. We use the figure from
    \citet{du2021orderagnostic}, Figure 1}%
  \label{fig:oaxe-example}
\end{figure}

% -----------------------------------------------------------------------------
\JH{relate the axe approach to bag-of-ngrams who also deal with misalignment}
\paragraph{\JH{Batch of ngrams.}} \citet{shao2020minimizing} also address the
misalignment issue. Instead of finding an appropriate alignment, they introduce
a new bag-of-ngrams objective, which minimizes the difference between the
\ac{nat} model output and the target sentence. \JH{tell me more!}


% ----------------------------------------------------------------------------
\paragraph{Imputer.} Antoher variant of the alignment-based approach was
described by \citet{saharia-etal-2020-non}. In the paper, they experiment with
a \acs{ctc}-based model similar to the model we use in this thesis, and an
iterative variant called \emph{Imputer}.

The model architecture is a stack of self-attentive layers without the causal
mask. The \acs{ctc}-based model takes upsampled source sentence embeddings (in
order to be able to create translations which are longer than the source) and
processes them with the Transformer layer stack.  The Imputer model proceeds in
a fixed number $L$ of iterations. In each iteration, the model generates
(``imputes'') $\ceil*{\frac{T_y}{L}}$ on some positions where $T_y$ is the
output length. This procedure is similar to decoding from \aclp{mlm} described
below \JH{check ref}. Additionally to the \ac{ctc} model, the input to the
Imputer model is the embedding of the partially decoded sentence from the
previous iteration summed with the (upsampled) source sentence embeddings.

The training of the Imputer model takes into account every partial output
potentially already decoded and maximizes a log-likelihood lower bound, which
can be comptued with a dynamic programming algorithm. \JH{more needed? or
  less?}



% ------------------------------------------------------------------------------
\section{Auxiliary Objective Methods}
\label{sec:nat:aux}
% ------------------------------------------------------------------------------



% ------------------------------------------------------------------------------
\paragraph{\Ac{nat} with auxiliary regularization.}
\citet{wang-etal-2019-nonautoregressive} use regularization terms to penalize
similarity of hidden states corresponding to dissimilar words(which battles
repeated translations), and to express a reconstruction loss using an external
translation model (in order to battle incomplete translation
problem). \JH{NAT-REG}

% ------------------------------------------------------------------------------
\paragraph{Hint-NAT.} \citet{li-etal-2019-hint} propose to regularize the
training of the student \ac{nat} model using two types of signals (hints) from
the \ac{ar} teacher. First, to bring the values of the decoder hidden states
closer together the authors consider tying the teacher and student states with
$L_1$ or $L_2$ distance, but argue that this straightforward approach
destabilizes the student training process and fails. Instead, they use a weaker
signal from the teacher model and define an auxiliary loss based on the cosine
distances within the decoder hidden states. Second, they use the information
from the teacher model cross-attention to guide the alignment learning in the
student model. Again, this information is incorporated into the training as an
auxiliary loss, based on the KL divergence between the teacher and student
attention distributions.

\JH{Add equations for the auxiliary losses? Ich dont think so..} \JH{Unify
  losses, it's either $J$ or $\mathcal{L}$}



% -----------------------------------------------------------------------------
\paragraph{ReorderNAT.} \citet{ran-etal-2021-guiding} propose a modification to
the orignal \ac{nat} model \citep{gu2017nonautoregressive}. They introduce an
intermediate Transformer block to model the sentence in the source language
after reordering for translation to the target language.

Formally, the translation model $p(y|x)$ is altered by including a latent
variable $z$ which is marginalized out, similarly to the refinement process:
%
\begin{equation}
  p(y|x) = \sum_{z} p(z|x) p(y|z,x)
\end{equation}
%
where $p(z|x)$ is the reordering module, and $p(y|z,x)$ is a \ac{nar}
Transformer decoder (i.e. without the causal mask) which receives the embedded
sequence $z$ on the input. The authors experiment with both \ac{ar} and
\ac{nar} variants of the reordering module, concluding that using a
single-layer autoregressive Transformer decoder performs best while still
retaining a speed improvement.

The reordering module is trained in supervised fashion using the
\texttt{fast\_align} alignment tool \citep{dyer-etal-2013-simple}. The model is
trained end to end by minimizing the sum of the reordering and decoder module
losses.


% ------------------------------------------------------------------------------
\section{Semi-Autoregressive and Iterative Methods}%
\label{sec:nat:semi}
% ------------------------------------------------------------------------------

\JH{some text here, about how there is a constant time of the transformer model
  executions, and when the constant is one, some people call it fully NAT, and
  when it is more than one (but constant wrt target length), then we call it
  semi-autoregressive, or iterative, and that there is not much difference
  between these two terms. Then a paragraph about the methods presented in this
  section, that most of them are actually based on the ideas from
  \citet{lee-etal-2018-deterministic}, and in turn on
  \citet{ghazvininejad-etal-2019-mask}.}

\paragraph{Iterative Decoding from Masked Language Models.} Recently,
pre-trained \acp{mlm} such as BERT \citep{devlin-etal-2019-bert} have attracted
attention from the translation research
community. \citet{ghazvininejad-etal-2019-mask} introduce \acp{cmlm}, an
extension to \acp{xlm} for multilingual applications, including \ac{mt}
\citep{conneau-lample-2019-cross}.

Conventional \acp{lm} estimate the probability of a word given the preceding
words.  In contrast, \acp{mlm} are trained on sentences where a number of
tokens has been masked-out, and the objective is to predict what the masked
words were, given the rest of the sentence. \Acp{xlm} extend this idea to a
pair of sentences in two (or more) languages. That is, tokens from either
source, or target sentence, or both, are masked and the model tries to predict
them.  To facilitate for alignment modeling and language identification,
\ac{xlm} architectures include a language embedding and reset the positional
encodings for each sentence. See Figure \ref{fig:mlm-xlm-example} for an
illustration of the difference between \acp{mlm} and \acp{xlm}.

\begin{figure}
  \centering

  \includegraphics[width=\textwidth]{img/mlm-xlm.png}

  \caption{A comparison of masked language model and translation
    (cross-lingual) language model architectures. We use the image of
    \citet{conneau-lample-2019-cross}, Figure 1.}%
  \label{fig:mlm-xlm-example}
\end{figure}

\Aclp{cmlm} originate in the ideas of
\citet{conneau-lample-2019-cross}. However, there are a few differences. First,
\acp{cmlm} use an encoder-decoder architecture, rather than a decoder-only
model that works on the concatenation of the source and target
sentences. Second, only the target words are masked and subject to the training
and prediction. \Acp{cmlm} are also intended for use in the end-task, rather
than for cross-lingual \ac{lm} pretraining, as is the case of \acp{xlm}.

The decoding process starts with the estimation of the target length. For this
purpose, \citet{ghazvininejad-etal-2019-mask} include a special \texttt{LENGTH}
token in the decoder input and predict the target length as the output on this
position. This way the length predictor can be trained jointly with the
translation model using the cross entropy training signal. The authors show
that it is useful to sample a small number of different length candidates,
decode a translation for each length, and then pick the highest scoring one.

Given a target length, the generation begins with a sequence of mask symbols.
In each iteration, the tokens on all masked positions are predicted in parallel
(independently, and thus, non-autoregressively). Then, a number of the tokens
that receive the lowest probability by the model is selected and masked-out
again, prepared to be re-predicted in the next iteration. The number of the
masked tokens is decreasing with each iteration, so that a constant number of
iterations is needed to generate the whole translation.

In their generalized framework for sequence generation,
\citet{mansimov2019generalized} experiment with other masking strategies, such
as left-to-right, easy-first, or uniform.

\paragraph{SMART.} In their follow-up experiments,
\citet{ghazvininejad-etal-2020-semiautoregressive} address the exposure bias
issue in context of \acp{cmlm} (although they use the terms autoregressive and
non-autoregressive for the distinction between using the model predictions
during training and teacher forcing).

As discussed in Section \ref{sec:training}, the exposure bias problem arises
when the model is trained on ground-truth data, but during decoding, it relies
on its own (often incorrect) decisions instead. \Acp{cmlm} are trained to
predict masked tokens given the ground-truth context, but during decoding, the
refinement is done using generated context instead.

To tackle this issue, \citet{ghazvininejad-etal-2020-semiautoregressive}
propose to mix the ground-truth tokens with the model predictions. This
approach is somewhat similar to scheduled sampling for \aclp{rnn}
\citep{bengio2015scheduled}.

\paragraph{DisCO.} \textbf{iterative, change to the attention and objective}
\citep{kasai2020nonautoregressive} -- the authors extend the \ac{cmlm} model
\citep{ghazvininejad-etal-2019-mask}. Instead of predicting only the tokens
which are masked-out, the authors propose an architecture that is able for each
position in the target sequence to predict a subset of the other tokens. They
achieve this goal by adapting the attention module. First the self-attention
does not attend to the same position in the previous layer. Second, to prevent
information leakage between the same positions on different layers, the
self-attention keys and values are decontextualized, meaning that instead of
the hidden states from the previous layer, the decoder input embeddings are
used.

\paragraph{JM-NAT} \textbf{iterative, auxiliary objective}
\citep{guo-etal-2020-jointly} They mask some words in the encoder (as they do
it in BERT) in each step to achieve a more robust representation in the
encoder. They introduce an auxiliary loss for the encoder, for predicting the
masked tokens. They do the same thing for the decoder, but they mask random
n-grams instead of random tokens. Unlike encoder, they first randomly select
the number n-grams to mask (to allow for iterative unmasking during
decoding). They also use an auxiliary n-gram loss function which is tailored to
help with the problem of repetitive tokens in the translation. They use
mask-predict \citep{ghazvininejad-etal-2019-mask} for decoding.


% ------------------------------------------------------------------------------
\paragraph{Semi-Autoregressive \acs{nmt}} One of the first attempts to bring
together the best of the worlds of \ac{ar} and \ac{nar} models was proposed by
\citet{wang-etal-2018-semi}. In their work, the authors use a
semi-autoregressive Transformer model which predicts groups of $K$ consecutive
tokens at a time. The tokens in each group are predicted independently and in
parallel, but the conditional dependency is retained between the groups in
autoregressive fashion.

As expected, the authors find that increasing $K$ leads to degraded translation
quality, but brings improvements in terms of decoding speed.


\paragraph{Blockwise Parallel Decoding for Deep Autoregressive Models.}
\citet{stern2018blockwise} propose a semi-autoregressive approach where chunks
of the target sentence are generated in parallel.
%
They start with a greedy decoding from an autoregressive model, $p_1$, and
introduce additional ``look-ahead'' models $p_2, \ldots p_k$. In time step $t$,
each model $p_i$ predicts the $(t + i)$-th word in the target sequence given the
same prefix of $t$ previously decoded words.

The decoding process has three stages. First, the block of predictions using the
models $p_1, \ldots, p_k$ is computed. Second, model $p_1$ is used to verify the
$(k-1)$ candidates (which is done in parallel in the Transformer model) and
finds the largest $\hat{k}$ such that decoded words from models $p_i$,
$1 \leq i \leq \hat{k}$ are all considered best by $p_1$. Third, the accepted
$\hat{k}$ words are generated and the decoding process jumps to time step
$t + \hat{k}$.

\paragraph{Insertion Transformer.} The Insertion Transformer
\citep{stern-etal-2019-insertion} is an extension of the Transformer model
which handles sequence generation in an arbitrary order. Instead of predicting
tokens left-to-right, the model predicts a sequence of insertion operations in
each step. An insertion operation specifies what tokens to insert, and where to
insert them. In each steps, the insertion operations are generated in parallel
and independently, thus we can include this approach in the semi-autoregressive
category. The authors experiment with decoding in a balanced binary-tree order,
which can generate the output sequence in a logarithmic number of steps.

An extension to the Insertion Transformer is
KERMIT. \citep{chan-etal-2019-kermit} \JH{more about kermit}


\paragraph{Levenshtein Transformer.} An interesting combination of the ideas
introduced in Insertion Transformer and \acp{cmlm} is the \acl{levt}
(\acs{levt}\glsunset{levt}; \citealp{gu-etal-2019-levenshtein}). This model
operates in an iterative fashion, much like the aforementioned methods. At the
beginning, \ac{levt} takes either an empty sentence or a crude, intermediate
sentence for refinement. Each iteration consists of applying three
policies. First, tokens to be deleted are identified and removed from the
sequence. Second, each slot (a position between two tokens) is assigned with a
number, and this number of placeholder symbols is inserted into the slot.
Third, for each placeholder, a token from the vocabulary is selected and the
placeholder symbols are replaced with the selected tokens, resulting in a
refined version of the input sentence.  Each of the three stages of a
refinement iteration is performed non-autoregressively.



% ------------------------------------------------------------------------------


% ------------------------------------------------------------------------------
\section{Fully NAT}%
\label{sec:nat:misc}
% ------------------------------------------------------------------------------

% \paragraph{LaNMT} \citep{shu2020latent} -- latent-variable NAR NMT with
% deterministic inference using a delta posterior \JH{to bych nechal na potom
% nebo vyhodil}

\paragraph{DCRF} \textbf{CRF decoding on top of vanilla NAT, not really NAT
  (similar to CTC+beam)} \citet{sun2019fast} propose to use \ac{crf} instead of
the argmax decoding from the \ac{nat} model, which allows the modeling of
causal relations within the output sequence. Although this process is not
\acl{ar} given the nature of the \ac{crf} decoding, the Transformer model needs
to be run only once and does not need to take into account the tokens as they
are being generated.

\paragraph{Flowseq} \textbf{latent variables} \citep{ma-etal-2019-flowseq} --
NAR conditional sequence generator with generative flow. The authors use an
intermediate latent variable $z$, but they argue that the prior distribution
$p(z|x)$ needs to be complex to model all of the dependency relations. They
propose to use generative flows \citep{rezende2015variational} for deriving the
complex distribution from a simple prior using a series of invertible
transformations. \JH{megacomplex paper}

\paragraph{EM+ODD} \textbf{iterative knowledge distillation, fully NAT}
\citep{sun2020em} -- they start from the multimodality problem, and the common
way of addressing it -- knowledge distillation using an autoregressive
teacher. They iterate the knowledge distillation step. They alternate the
training of the \ac{ar} teacher model and the \ac{nar} student model, where the
sellection of samples for the \ac{ar} training dataset is informed by the log
likelihood as modeled by the \ac{nar} model.

Additionally, the authors propose \acf{odd} which tackles the issue with
repetitive oputput tokens while preserving the predicted output length and
finds the optimal sequence without repetition.


\paragraph{GLAT} \citep{qian2020glancing} -- \textbf{auxiliary objective, fully
  NAT} The authors propose \acf{glat}, a technique for training the \ac{nat}
model similarly to \acp{cmlm}, but enabling decoding in a single pass instead
of an iterative process.

During training, the model first generates an intermediate output
non-autoregressively. Then, a number of positions is masked according to a
glancing sampling strategy, and the model is trained to predict the masked
tokens on the selected positions. \JH{equations?} The sampling strategy
randomly selects $S$ positions, where $S$ is proportional to the number of
errors in the intermediate prediction.

The decoding is similar to the process proposed by
\citet{gu2017nonautoregressive}, but the target length prediction is done as in
the \ac{cmlm} approach -- an artificial source token \texttt{LENGTH} is
prepended to the source sentence, and the encoder representation at the first
position is used to predict the target length.


\paragraph{Layer-Wise Prediction with Deep Supervision.}  \textbf{fully NAT}
\JH{although looks like iterative with many shallow decoder steps} A recent
approach by \citet{huang-etal-2021-nonautoregressive} uses grounding of the
Transformer hidden states in all layers to the target sentence.  In every
decoder layer, the model generates the intermediate sequence from the hidden
states using a linear projection and a softmax layer. The embeddings of the
decoded tokens are added to the input to the next layer along with the hidden
state values.

The deep supervision grounds the states of all layers, not just the last one,
to the target sequence. The objective function is extended to include the cross
entropies of the layer-wise softmax predictions on all Transformer layers in
the decoder stack.


\paragraph{Latent Transformer.} \citet{kaiser2018fast} use discrete latent
variables in a model called \acf{lt}\glsunset{lt}. The model has three
components. First, the \emph{autoencoder} takes a sentence pair $(x, y)$ and
encode the target sentence $y$ into a sequence of discrete latent variables
$l$. Second, an \acl{ar} \emph{latent prediction} model, which predicts the
sequence $l$ based on a source sentence $x$. Third, an \acs{nar}
\emph{decoder}, used to generate $y$ given the source sentence $x$ and the
hidden sequence $l$.

The autoencoder is used for training in order to condense the longer target
sequence $y$ into a shorter (usually 8 times) sequence $l$, which is used as an
itermediate representation of $y$. Unlike perhaps more common autoencoders,
this autoencoder uses discrete latent variables. The authors propose two
methods (decomposed vector quantization and latent semantic hashing) for the
discretization. A stack of convolutions with residucl connections is selected
as the autoencoder architecture. The latent predictor is a stack of Transformer
layers, and the decoder consists of up-convolutions.

% ------------------------------------------------------------------------------
\section{Discussion}%
\label{sec:nat:discussion}
% ------------------------------------------------------------------------------

In the conclusion to this chapter, we take a high-level view on the approaches
we have presented here. We point out a few aspects that most of the literature
has in common, as well as some issues regarding evaluation and comparison to
meaningful baselines.

\paragraph{Results.} Table \ref{tab:related-models} shows selected results of
the described approaches measured on the \acs{wmt}~14 test set. We can see that
rescoring using an \acl{ar} teacher consistently helps the translation
performance. However, the more rescoring steps is taken, the less decoding
speed is gained. \JH{There should be a table with the decoding speeds, and
  there should be a figure showing the pareto-frontier.}

\paragraph{Capturing Reordering.} \JH{Many papers claim that reordering is an
  issue. While this might be true, the culprit is the multimodality problem, as
  the reordering can be modeled within the self-attentive layers. Also, the
  papers that propose methods to model the reordering better, evaluate their
  method only on the end task (translation) and do not assess the actual
  reordering ability. However, modeling reordering helps reducing the
  multimodality. Anyway, if they claim it, they should measuring I guess.}


\paragraph{Evaluation Methodology.} \JH{This paragraph will discuss the
  evalutaion in general. Reporting relative speed-up OK, but citing
  autoregressive latency by others and then measure speed up relative to that
  is not OK, as well as citing and comparing to relative speed-ups of others.
  Most papers report GPU latency, but it is not a rule. The speed should be
  reported under multiple settings, best would be to report it as they do in
  WNGT, i.e. latency and throughput (batching and no-batching), and both on CPU
  and on GPU. Also, they should always say how many cores they used.}

\JH{We could also have a table with autoregressive baselines reported by each
  study, and get some details on that.}


\paragraph{Decoding Speed.} \JH{this is regarding the baseline decoding speeds
  reported by NAT literature. This paragraph should look into WNGT 20 or 21 and
  check the speeds of best AR results and compare to the autoregressive speeds
  reported in the NAT literature. What do we find is, that the AR baselines in
  NAT models are actually much slower. Often, which is another issue, the
  decoding speeds are not reported at all.  }

\paragraph{Translation Quality.} \JH{Using BLEU is fine, but we should not be
  stuck on WMT14 with results of \citet{vaswani2017attention}, since they are
  not really up to date. }

\JH{perhaps add something about time complexity and how it is not constant?}

\begin{table}
  \centering

  \begin{tabular}{lrcc}
    \toprule
    Method & Steps & En $\rightarrow$ De & De $\rightarrow$ En \\
    \midrule
    \citet{gu2017nonautoregressive} & & & \\
    \quad \acs{nat} + FT & 1 & 17.69 & 21.47 \\
    \quad \acs{nat} + FT + top-100 \acs{npd} rescoring & 101 & 19.17 & 23.20 \\

    \citet{lee-etal-2018-deterministic} & & \\
    \quad 1 iteration & 1 & 13.91 & 16.77 \\
    \quad 10 iterations & 10 & 21.61 & 25.48 \\

    \citet{kaiser2018fast} & & \\
    \quad \acs{lt}, no rescoring & 1 & 19.80 & -- \\
    \quad \acs{lt}, top-100 rescoring & 101 & 22.50 & -- \\

    \citet{ghazvininejad-etal-2019-mask} & & \\
    \quad Base \acs{cmlm} with Mask-Predict, 1 iteration & 1 & 18.05 & 21.83 \\
    \quad Base \acs{cmlm} with Mask-Predict, 10 iterations & 10 & 27.03 & 30.53 \\

    \citet{sun2019fast} & & & \\
    \quad \acs{nat} + \acs{dcrf}, no rescoring & 1 & 23.44 & 27.22 \\
    \quad \acs{nat} + \acs{dcrf}, top-19 rescoring & 20 & 26.80 & 30.04 \\

    \citet{wang-etal-2019-nonautoregressive} & & & \\
    \quad \acs{natreg}, no rescoring & 1 &  20.65 & 24.77 \\
    \quad \acs{natreg}, top-9 rescoring & 10 & 20.65 & 24.77 \\

    \citet{li-etal-2019-hint} & & & \\
    \quad \acs{hintnat}, no rescoring & 1 & 21.11  & 25.24 \\
    \quad \acs{hintnat}, top-9 rescoring & 10 & 25.20  & 29.52 \\

    \citet{ma-etal-2019-flowseq} & & & \\
    \quad FlowSeq-large & 1 & 23.72 & 28.39 \\
    \quad FlowSeq-large + top-30 \acs{npd} rescoring & 31 & 25.31 & 30.68 \\

    \citet{ghazvininejad2020aligned} \acs{axe} \acs{cmlm} & 1 & 23.53 & 27.90 \\

    \citet{guo-etal-2020-jointly} & & & \\
    \quad \acs{jmnat}, 4 iterations & 4 & 27.05 & 31.51 \\
    \quad \acs{jmnat}, 10 iterations & 10 & 27.69 & 32.24 \\

    \citet{saharia-etal-2020-non} & & & \\
    \quad \acs{ctc} & 1 & 25.70 & 28.10 \\
    \quad Imputer &  \JH{??} & 25.80 & 28.40 \\

    \citet{kasai2020nonautoregressive} -- DisCo + Easy-First
           & \JH{??} & 27.34 & 31.31 \\

    \citet{sun2020em} -- \acs{emodd} & \JH{??} & 24.54 & 27.93 \\

    \citet{qian2020glancing} & & & \\
    \quad \acs{glat} + \acs{ctc} & 1 & 26.39 & 29.54 \\
    \quad \acs{glat} + \acs{ctc} + top-5 \acs{npd} rescoring & 6 & 26.55 & 31.02 \\

    \citet{ran-etal-2021-guiding} & & &  \\
    \quad \acs{ar} reordering & N & 26.49 & 31.13  \\
    \quad \acs{nar} reordering & 1\footnotemark\JH{..} & 22.79 & 27.28 \\

    \citet{gu-kong-2021-fully} \acs{nat} + \acs{ctc} + \acs{glat}
           & 1\JH{?} & 27.20 & 31.39 \\

    \citet{du2021orderagnostic} -- \acs{cmlm} + \acs{oaxe}
           & \JH{1?} & 26.10 & 30.20 \\

    \bottomrule
  \end{tabular}
  \caption{The results (in terms of \acs{bleu}) of the described models
    measured on the \acs{wmt}~14 test set, as reported by the authors.}%
  \label{tab:related:wmt14}
\end{table}


\begin{table}
  \centering

  \begin{tabular}{ll}
    \toprule

    \midrule

    \bottomrule
  \end{tabular}

  \caption{An overview of the \ac{nat} models in the literature.}%
  \label{tab:related-models}
\end{table}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
