% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{chap:intro}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\JH{Write the introduction.}

This thesis is structured as follows.
%
In Chapter \ref{chap:nmt}, we introduce the field of \ac{nmt}. We show how
models for \ac{nmt} process textual data, and how they deal with the large
number of different words they need to understand. We introduce the concept of
encoder-decoder architectures, which are the backbone of nearly every \ac{nmt}
system there is, and their two types, \acs{rnn}-based models and the
Transformer. We give an overview of how to train these models, including a
number of practical aspects of training, such as what methods do we use for
data cleaning, or what training algorithms do we use and how to configure
them. We briefly talk about methods for decoding applied once the model has
been trained. We conclude the chapter by discussing the evaluation, which tells
us how good or how bad our translation models are.

Chapter \ref{chap:nat} walks us through the research on \ac{nat}. We explain
the principal differences between \acl{ar} and \acl{nar} models, what issues
arise when using the latter, and how we can solve them. We provide a
comprehensive survey of the literature, owing to the fact that the whole domain
of \ac{nat} resarch is fairly new. We describe efforts to keep the most
important feature of \ac{nat} models -- the high decoding speed -- intact by
introducing constraints on the training process, or other efforts that
partially sacrifice the higher decoding speed for better translation quality by
iterative refinement. At the end of the chapter, we identify weaknesses
prevalent in a large part of the literature on this topic. One of the main
problems lies in the evaluation methodology adopted by the \ac{nat} research
community.

We present a method for training \acl{nar} models for \ac{nmt} with \ac{ctc} in
Chapter \ref{chap:nar-nmt-ctc}. We first describe the \ac{ctc} algorithm for
computing loss between unaligned label and prediction sequences. Having such
loss function relaxes the requirement for target length prediction, which is
one of the obstacles in \ac{nat}. We show the changes we need to apply to the
model architecture so we can use the \ac{ctc} loss, and we report on our
experiments and their results, both in terms of speed and translation quality.
Our analysis shows that \ac{nat} models suffer from reduced fluency of the
translations, which is in line with the conclusions of other \ac{nat} studies.
We briefly mention our attempt to improve the translation fluency using a
\acl{lm} rescoring.

In Chapter \ref{chap:experiments}, we document our recent efforts to address
the flaws in the evaluation methodology discussed in Chapter \ref{chap:nat}
with our \acs{ctc}-based model from Chapter \ref{chap:nar-nmt-ctc}. We compare
our results to methods published in the context of \ac{nat}, as well as methods
that focus directly on improving the efficiency of translation systems.  We
find that despite achieving a high performance among \ac{nat} models, using
efficient \acl{ar} translation models leads to better results, both in terms of
speed and translation quality. We argue that for better estimation of the
benefits of \acl{nar} methods, the future research should take into account the
methods employed in production-ready systems.

We conclude the thesis in Chapter \ref{chap:conclusions}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
