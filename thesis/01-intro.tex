% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{chap:intro}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This thesis is structured as follows.
%
In Chapter \ref{chap:nmt}, we introduce the field of \ac{nmt}. We show how
models for \ac{nmt} process textual data, and how they deal with the large
number of different words they need to understand. We introduce the concept of
encoder-decoder architectures, which are the backbone of nearly every \ac{nmt}
system there is, and their two types, \acs{rnn}-based models and the
Transformer. We give an overview of how to train these models, including a
number of practical aspects of training, such as what methods do we use for
data cleaning, or what training algorithms do we use and how to configure
them. We briefly talk about methods for decoding applied once the model has
been trained. We conclude the chapter by discussing the evaluation, which tells
us how good or how bad our translation models are.

Chapter \ref{chap:nat} presents \ac{nat}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
